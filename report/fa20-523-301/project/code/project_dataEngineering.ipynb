{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_dataEngineering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to1YX14hA5NK"
      },
      "source": [
        "#Data Engineering Notebook\n",
        "\n",
        "The report for this final project can be found at this [link](https://cybertraining-dsc.github.io/report/fa20-523-301/project/project/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neKgmN6QBFJa"
      },
      "source": [
        "## Part 1 Importing the functions\n",
        "\n",
        "This file requires that we import Numpy, Matplotlib, Pylab, Keras, and Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU7SDylhA3Qe",
        "outputId": "9c7bf544-37e7-41c1-b679-3d84cdb34a41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install utils\n",
        "import numpy as np\n",
        "import pylab\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import warnings\n",
        "import utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9h_pp2vBV55"
      },
      "source": [
        "Now that the funtions have been imported the team can focus on the download coding. The following cells will set up an install for Kaggle files and prompt for an upload of the kaggle.json file for credentials. \n",
        "\n",
        "The mkdir function creates a directory for the Kaggle data. This cell will allow the team to verify that the kaggle.json file appropriately uploaded to the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTVOSTQjBYiN",
        "outputId": "7d7ef176-6b7c-4432-a46b-40d64b362664",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "##import the kaggle.json from local to drive\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "##when it asks you to choose a file select the kaggle.json located within the 'project' folder from the github repo\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8beb9837-6e2d-4774-82b7-6ab4cd62501e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8beb9837-6e2d-4774-82b7-6ab4cd62501e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"chelseagorius\",\"key\":\"0a34819ed937ff55d31f4288ab40cf19\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJZE3PjtBeQR",
        "outputId": "d27ee05c-573a-424e-b98c-94978b197c1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##make a kaggle and a data folder\n",
        "!mkdir ~/.kaggle\n",
        "!mkdir data\n",
        "##copy the kaggle.json to the .kaggle folder then grant permissions\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "#test to see if kaggle is working, should print list of datasets\n",
        "!kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
            "ref                                                     title                                         size  lastUpdated          downloadCount  \n",
            "------------------------------------------------------  -------------------------------------------  -----  -------------------  -------------  \n",
            "manchunhui/us-election-2020-tweets                      US Election 2020 Tweets                      353MB  2020-11-09 18:51:59           1224  \n",
            "unanimad/us-election-2020                               US Election 2020                             417KB  2020-11-09 13:52:09           1280  \n",
            "headsortails/us-election-2020-presidential-debates      US Election 2020 - Presidential Debates      199MB  2020-10-23 16:56:10            297  \n",
            "antgoldbloom/covid19-data-from-john-hopkins-university  COVID-19 data from John Hopkins University     2MB  2020-11-09 06:07:13             31  \n",
            "etsc9287/2020-general-election-polls                    2020 General Election Polls                  109KB  2020-02-09 08:20:59            440  \n",
            "radustoicescu/2020-united-states-presidential-election  2020 United States presidential election      11MB  2019-07-04 15:00:45            620  \n",
            "shivamb/netflix-shows                                   Netflix Movies and TV Shows                  971KB  2020-01-20 07:33:56          56173  \n",
            "terenceshin/covid19s-impact-on-airport-traffic          COVID-19's Impact on Airport Traffic         106KB  2020-10-19 12:40:17           3241  \n",
            "sootersaalu/amazon-top-50-bestselling-books-2009-2019   Amazon Top 50 Bestselling Books 2009 - 2019   15KB  2020-10-13 09:39:21           3152  \n",
            "nehaprabhavalkar/indian-food-101                        Indian Food 101                                7KB  2020-09-30 06:23:43           6633  \n",
            "karangadiya/fifa19                                      FIFA 19 complete player dataset                2MB  2018-12-21 03:52:59         102981  \n",
            "omarhanyy/500-greatest-songs-of-all-time                500 Greatest Songs of All Time                33KB  2020-10-26 13:36:09           1002  \n",
            "heeraldedhia/groceries-dataset                          Groceries dataset                            257KB  2020-09-17 04:36:08           7038  \n",
            "andrewmvd/trip-advisor-hotel-reviews                    Trip Advisor Hotel Reviews                     5MB  2020-09-30 08:31:20           4733  \n",
            "docstein/brics-world-bank-indicators                    BRICS World Bank Indicators                    4MB  2020-10-22 12:18:40            825  \n",
            "google/tinyquickdraw                                    QuickDraw Sketches                            11GB  2018-04-18 19:38:04           2433  \n",
            "datasnaek/youtube-new                                   Trending YouTube Video Statistics            201MB  2019-06-03 00:56:47         114128  \n",
            "uciml/mushroom-classification                           Mushroom Classification                       34KB  2016-12-01 23:08:00          53698  \n",
            "anikannal/solar-power-generation-data                   Solar Power Generation Data                    2MB  2020-08-18 15:52:03           9290  \n",
            "zynicide/wine-reviews                                   Wine Reviews                                  51MB  2017-11-27 17:08:04         118247  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn6gsxIJBzoe"
      },
      "source": [
        "Now, the team must download all of the datasets for the class. The three datasets are focused on the NBA. \n",
        "\n",
        "The first dataset is for injuries. Each injury will be used to set up players, timeframes, and severity of injuries. \n",
        "\n",
        "The other two datasets are for the player performance. By cross referencing this data to the previous list, the team will be able to see which players are limited from the injury and how performance is hampered by time in rehab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rmkpN1YByux",
        "outputId": "c268319c-1413-4c27-e7da-2db476e1c0a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##downloading all the datasets\n",
        "!kaggle datasets download -d ghopkins/nba-injuries-2010-2018\n",
        "!kaggle datasets download -d nathanlauga/nba-games\n",
        "!kaggle datasets download -d justinas/nba-players-data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading nba-injuries-2010-2018.zip to /content\n",
            "\r  0% 0.00/226k [00:00<?, ?B/s]\n",
            "100% 226k/226k [00:00<00:00, 32.5MB/s]\n",
            "Downloading nba-games.zip to /content\n",
            " 50% 9.00M/18.1M [00:01<00:01, 7.70MB/s]\n",
            "100% 18.1M/18.1M [00:01<00:00, 11.7MB/s]\n",
            "Downloading nba-players-data.zip to /content\n",
            "  0% 0.00/490k [00:00<?, ?B/s]\n",
            "100% 490k/490k [00:00<00:00, 31.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qopGnNNOB84J",
        "outputId": "df20f00a-dfda-4104-c42c-3cf3d80b64c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##unzipping to the data folder\n",
        "!unzip nba-injuries-2010-2018.zip -d data\n",
        "!unzip nba-games.zip -d data\n",
        "!unzip nba-players-data.zip -d data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  nba-injuries-2010-2018.zip\n",
            "  inflating: data/injuries_2010-2020.csv  \n",
            "Archive:  nba-games.zip\n",
            "  inflating: data/games.csv          \n",
            "  inflating: data/games_details.csv  \n",
            "  inflating: data/players.csv        \n",
            "  inflating: data/ranking.csv        \n",
            "  inflating: data/teams.csv          \n",
            "Archive:  nba-players-data.zip\n",
            "  inflating: data/all_seasons.csv    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VyMM4w4CB6A"
      },
      "source": [
        "The team must now use these downloads to create dataframes. Pandas dataframes will be easier to manage the data. The team will be able to use Pandas to process the data and allow the team to make correlations for feature engineering to create the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNthqUMxB_N5"
      },
      "source": [
        "#import csv files as dataframes and save to respective list, injury set first\n",
        "df_Injuries = pd.read_csv('data/injuries_2010-2020.csv')\n",
        "df_Injury_Start = df_Injuries[df_Injuries.Acquired.isnull()]\n",
        "df_Injury_End = df_Injuries[df_Injuries.Relinquished.isnull()]\n",
        "\n",
        "#nba games dataset\n",
        "df_Games_games = pd.read_csv('data/games.csv')\n",
        "df_Games_gamesDetails = pd.read_csv('data/games_details.csv')\n",
        "df_Games_players = pd.read_csv('data/players.csv')\n",
        "df_Games_ranking = pd.read_csv('data/ranking.csv')\n",
        "df_Games_teams = pd.read_csv('data/teams.csv')\n",
        "\n",
        "#nba player season stats dataset\n",
        "df_Season_stats = pd.read_csv('data/all_seasons.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEmzUVhYCWoR"
      },
      "source": [
        "Preparing data tables to have the appropriate columns in order to calculate time and player specific metrics for each injury."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQQ2M60LCRc5"
      },
      "source": [
        "#\n",
        "##Create a dataframe of distinct player and player IDs\n",
        "df_distinct_playerID = df_Games_players[[\"PLAYER_NAME\", \"PLAYER_ID\"]].drop_duplicates()\n",
        "df_distinct_playerID.astype({'PLAYER_ID':'object'}).dtypes\n",
        "#\n",
        "##Create a dataframe of distinct gameID and game dates\n",
        "df_Games_games['GAME_DATE_EST'] = pd.to_datetime(df_Games_games['GAME_DATE_EST'])\n",
        "df_distinct_gameId_date = df_Games_games[[\"GAME_ID\", \"GAME_DATE_EST\", \"SEASON\"]].drop_duplicates()\n",
        "#\n",
        "##Join the distinct player df by player name and join the gameID information\n",
        "df_Injury_Start = df_Injury_Start.join(df_distinct_playerID.astype('object').set_index('PLAYER_NAME'), on='Relinquished')\n",
        "df_Injury_Start = df_Injury_Start.merge(df_Games_teams[[\"TEAM_ID\", \"NICKNAME\"]], left_on=\"Team\", right_on=\"NICKNAME\")\n",
        "df_Injury_Start.drop(['NICKNAME'], axis=1)#.apply(lambda x: x.date())\n",
        "df_Injury_Start['Date']= pd.to_datetime(df_Injury_Start['Date'])\n",
        "#\n",
        "##Do the same for the Injury End database\n",
        "df_Injury_End = df_Injury_End.join(df_distinct_playerID.astype('object').set_index('PLAYER_NAME'), on='Acquired')\n",
        "df_Injury_End = df_Injury_End.merge(df_Games_teams[[\"TEAM_ID\", \"NICKNAME\"]], left_on=\"Team\", right_on=\"NICKNAME\")\n",
        "df_Injury_End.drop(['NICKNAME'], axis=1)#.apply(lambda x: x.date())\n",
        "df_Injury_End['Date']= pd.to_datetime(df_Injury_End['Date'])\n",
        "df_Games_gamesDetails = df_Games_gamesDetails.merge(df_distinct_gameId_date, on=\"GAME_ID\")\n",
        "#\n",
        "##calculating injury length could be hard\n",
        "df_Injury_length = df_Injury_Start.copy()\n",
        "df_Injury_length = df_Injury_length.rename(columns={\"Date\":\"DateInjured\", \"Relinquished\":\"Player\", \"Notes\":\"InjuryNotes\"})\n",
        "df_Injury_length = df_Injury_length[[\"TEAM_ID\", \"Team\", \"PLAYER_ID\", \"Player\", \"DateInjured\", \"InjuryNotes\"]]\n",
        "#\n",
        "##Decreasing the size of the Season stats database, reforamting season column\n",
        "df_Season_stats['Season_year'] = df_Season_stats['season'].str.slice(0,4)\n",
        "df_Season_stats = df_Season_stats[df_Season_stats['Season_year'].astype('int64') >= 2010].copy()\n",
        "df_Season_stats = df_Season_stats.drop(columns=['college', 'country', 'draft_round', 'draft_number']).copy()\n",
        "df_Season_stats['Season_year'] = df_Season_stats['Season_year'].astype('int64')\n",
        "df_Season_stats.to_csv('df_Season_stats.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWBgkuoLCzsn"
      },
      "source": [
        "Transforming the minutes column to a numeric value that can be used to create calculated metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlNyxrIDCqlA"
      },
      "source": [
        "for index, row in df_Games_gamesDetails.iterrows():\n",
        "  try:\n",
        "    m, s = str(row.MIN).split(':')\n",
        "  except (SyntaxError, ValueError) as e:\n",
        "    m = (row.MIN)\n",
        "    s = 0\n",
        "  df_Games_gamesDetails.loc[index,'MIN'] = pd.to_numeric(m) + pd.to_numeric(s)/60\n",
        "df_Games_gamesDetails['GAME_DATE_EST'] = pd.to_datetime(df_Games_gamesDetails['GAME_DATE_EST'])\n",
        "df_Games_gamesDetails.to_csv('df_Games_gamesDetails.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPFMNYvMfq_g"
      },
      "source": [
        "df_Games_gamesDetails = pd.read_csv('df_Games_gamesDetails.csv')\n",
        "df_Games_gamesDetails['GAME_DATE_EST'] = pd.to_datetime(df_Games_gamesDetails['GAME_DATE_EST'])\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlDThd6PXjKz"
      },
      "source": [
        "Creating db that contains the injury start and end date for each injury listed in the original db."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnTbGFVaRlif"
      },
      "source": [
        "df_Injury_length = df_Injury_length[df_Injury_length['Player'] != np.nan]\n",
        "inj_count = df_Injury_length['Player'].value_counts()\n",
        "#First find the date recovered\n",
        "for index, row in df_Injury_length.iterrows():\n",
        "  #get rows with same player ID\n",
        "  temp = df_Injury_End.loc[df_Injury_End['PLAYER_ID'] == row.PLAYER_ID]\n",
        "  #get rows after the injury date\n",
        "  temp2 = temp.loc[(temp['Date'] > row.DateInjured)]\n",
        "  #get the row with the oldest (smallest) date\n",
        "  recover = temp2.nsmallest(1, 'Date')\n",
        "  try:\n",
        "    df_Injury_length.at[index, 'DateRecovered'] = pd.Series(recover[['Date']].Date).values[0]\n",
        "    df_Injury_length.at[index, 'RecoverNotes'] = pd.Series(recover[['Notes']].Notes).values[0]\n",
        "  except (IndexError) as e:\n",
        "    df_Injury_length.at[index, 'DateRecovered'] = np.nan\n",
        "    df_Injury_length.at[index, 'RecoverNotes'] = np.nan\n",
        "  #Get number of injuries\n",
        "  count_name = row['Player']\n",
        "  try:\n",
        "   df_Injury_length.at[index,'NumberInjuries'] = inj_count[count_name]\n",
        "  except (KeyError) as e:\n",
        "   pass#print(inj_count[count_name])\n",
        "#Group by player Id and date recovered to avoid miscount of injuries\n",
        "group = df_Injury_length.groupby(['PLAYER_ID','DateRecovered'])['DateInjured'].min().reset_index()\n",
        "df_Injury_length = pd.merge(group, df_Injury_length,  how='left', left_on=['PLAYER_ID','DateRecovered', 'DateInjured'], right_on = ['PLAYER_ID','DateRecovered', 'DateInjured'])\n",
        "#Calculating injury length in days\n",
        "df_Injury_length['InjuryLengthDays'] = df_Injury_length['DateRecovered'] - df_Injury_length['DateInjured']\n",
        "for index, row in df_Injury_length.iterrows():\n",
        "  df_Injury_length.at[index, 'InjuryLengthDays'] = row.InjuryLengthDays.days\n",
        "  #Calculating the season of the injury and recovery games\n",
        "  inj_year = int(df_Injury_length.at[index, 'DateInjured'][0:4])\n",
        "  inj_month = int(df_Injury_length.at[index, 'DateInjured'][5:7])\n",
        "  if inj_month >= 8:\n",
        "    df_Injury_length.at[index, 'SeasonInjured'] = inj_year\n",
        "  else:\n",
        "    df_Injury_length.at[index, 'SeasonInjured'] = inj_year - 1\n",
        "  rev_year = int(df_Injury_length.at[index, 'DateRecovered'][0:4])\n",
        "  rev_month = int(df_Injury_length.at[index, 'DateRecovered'][5:7])\n",
        "  if rev_month >= 8:\n",
        "    df_Injury_length.at[index, 'SeasonRecovered'] = rev_year\n",
        "  else:\n",
        "    df_Injury_length.at[index, 'SeasonRecovered'] = rev_year-1\n",
        "#Saving to a .csv file\n",
        "df_Injury_length.to_csv('df_Injury_length.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgI5tzu5DX_g"
      },
      "source": [
        "Creating the metrics for player performance metrics during the injury game and summarized for the 5 games prior to the injury.\n",
        "Creating the metrics for player performance metrics in the first game back from injury and summarized for the 5 games after thedf_Injury_stats.to_csv('df_Injury_stat.csv')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lfiek9Poz8Y"
      },
      "source": [
        "df_Injury_stats = df_Injury_length.copy()\n",
        "for index, row in df_Injury_stats.iterrows():\n",
        "  #games of just that player\n",
        "  plyr = df_Games_gamesDetails.loc[df_Games_gamesDetails['PLAYER_ID'] == row.PLAYER_ID]\n",
        "  #games before and inlucding injury date#5 games prior and the game of injury, for some reason we need to have 4 different variabels, did not work with resetting the variable 'game_set' to itself\n",
        "  temp = plyr.loc[(plyr['GAME_DATE_EST'] <= row.DateInjured)]\n",
        "  inj_gameset = temp.nlargest(6, 'GAME_DATE_EST')\n",
        "  #games after and inlucding recover date\n",
        "  temp2 = plyr.loc[(plyr['GAME_DATE_EST'] >= row.DateRecovered)]\n",
        "  #5 games after and the game of injury, for some reason we need to have 3 different variabels, did not work with resetting the variable 'game_set' to itself\n",
        "  rev_gameset = temp.nsmallest(6, 'GAME_DATE_EST')\n",
        "  ##\n",
        "  ##Start calculated columns for injury games and prior\n",
        "  if len(inj_gameset) > 0:\n",
        "    #injury game\n",
        "    inj_game = inj_gameset.iloc[0]\n",
        "    #5 games prior to injury\n",
        "    prior5 = inj_gameset.iloc[1:]\n",
        "    #storing game data from injury game\n",
        "    df_Injury_stats.at[index, 'inj_MIN'] = inj_game[['MIN']].MIN\n",
        "    df_Injury_stats.at[index,'inj_FGA'] = inj_game[['FGA']].FGA\n",
        "    df_Injury_stats.at[index,'inj_FG_PCT'] = inj_game[['FG_PCT']].FG_PCT\n",
        "    df_Injury_stats.at[index,'inj_FG3A'] = inj_game[['FG3A']].FG3A\n",
        "    df_Injury_stats.at[index,'inj_FG3_PCT'] = inj_game[['FG3_PCT']].FG3_PCT\n",
        "    df_Injury_stats.loc[index,'inj_FTA'] = inj_game[['FTA']].FTA\n",
        "    df_Injury_stats.loc[index,'inj_FT_PCT'] = inj_game[['FT_PCT']].FT_PCT\n",
        "    df_Injury_stats.loc[index,'inj_REB'] = inj_game[['REB']].REB\n",
        "    df_Injury_stats.loc[index,'inj_AST'] = inj_game[['AST']].AST\n",
        "    df_Injury_stats.loc[index,'inj_STL'] = inj_game[['STL']].STL\n",
        "    df_Injury_stats.loc[index,'inj_BLK'] = inj_game[['BLK']].BLK\n",
        "    df_Injury_stats.loc[index,'inj_TO'] = inj_game[['TO']].TO\n",
        "    df_Injury_stats.loc[index,'inj_PF'] = inj_game[['PF']].PF\n",
        "    df_Injury_stats.loc[index,'inj_PTS'] = inj_game[['PTS']].PTS\n",
        "    df_Injury_stats.loc[index,'inj_PLUS_MINUS'] = inj_game[['PLUS_MINUS']].PLUS_MINUS\n",
        "#storing game data from prior 5 games\n",
        "    df_Injury_stats.at[index,'pr5_MIN'] = prior5[['MIN']].MIN.mean()\n",
        "    df_Injury_stats.at[index,'pr5_FGA'] = prior5[['FGA']].FGA.mean()\n",
        "    df_Injury_stats.at[index,'pr5_FG_PCT'] = prior5[['FG_PCT']].FG_PCT.mean()\n",
        "    df_Injury_stats.at[index,'pr5_FG3A'] = prior5[['FG3A']].FG3A.mean()\n",
        "    df_Injury_stats.at[index,'pr5_FG3_PCT'] = prior5[['FG3_PCT']].FG3_PCT.mean()\n",
        "    df_Injury_stats.at[index,'pr5_FTA'] = prior5[['FTA']].FTA.mean()\n",
        "    df_Injury_stats.at[index,'pr5_FT_PCT'] = prior5[['FT_PCT']].FT_PCT.mean()\n",
        "    df_Injury_stats.at[index,'pr5_REB'] = prior5[['REB']].REB.mean()\n",
        "    df_Injury_stats.at[index,'pr5_AST'] = prior5[['AST']].AST.mean()\n",
        "    df_Injury_stats.at[index,'pr5_STL'] = prior5[['STL']].STL.mean()\n",
        "    df_Injury_stats.at[index,'pr5_BLK'] = prior5[['BLK']].BLK.mean()\n",
        "    df_Injury_stats.at[index,'pr5_TO'] = prior5[['TO']].TO.mean()\n",
        "    df_Injury_stats.at[index,'pr5_PF'] = prior5[['PF']].PF.mean()\n",
        "    df_Injury_stats.at[index,'pr5_PTS'] = prior5[['PTS']].PTS.mean()\n",
        "    df_Injury_stats.at[index,'pr5_PLUS_MINUS'] = prior5[['PLUS_MINUS']].PLUS_MINUS.mean()\n",
        "  ##\n",
        "  ##Star calculated column for recovery game and after\n",
        "  if len(rev_gameset) > 0:\n",
        "    #injury game\n",
        "    rev_game = rev_gameset.iloc[0]\n",
        "    #5 games post injury\n",
        "    post5 = rev_gameset.iloc[1:]\n",
        "    #storing game data from injury game\n",
        "    df_Injury_stats.at[index, 'rev_MIN'] = rev_game[['MIN']].MIN\n",
        "    df_Injury_stats.at[index,'rev_FGA'] = rev_game[['FGA']].FGA\n",
        "    df_Injury_stats.at[index,'rev_FG_PCT'] = rev_game[['FG_PCT']].FG_PCT\n",
        "    df_Injury_stats.at[index,'rev_FG3A'] = rev_game[['FG3A']].FG3A\n",
        "    df_Injury_stats.at[index,'rev_FG3_PCT'] = rev_game[['FG3_PCT']].FG3_PCT\n",
        "    df_Injury_stats.loc[index,'rev_FTA'] = rev_game[['FTA']].FTA\n",
        "    df_Injury_stats.loc[index,'rev_FT_PCT'] = rev_game[['FT_PCT']].FT_PCT\n",
        "    df_Injury_stats.loc[index,'rev_REB'] = rev_game[['REB']].REB\n",
        "    df_Injury_stats.loc[index,'rev_AST'] = rev_game[['AST']].AST\n",
        "    df_Injury_stats.loc[index,'rev_STL'] = rev_game[['STL']].STL\n",
        "    df_Injury_stats.loc[index,'rev_BLK'] = rev_game[['BLK']].BLK\n",
        "    df_Injury_stats.loc[index,'rev_TO'] = rev_game[['TO']].TO\n",
        "    df_Injury_stats.loc[index,'rev_PF'] = rev_game[['PF']].PF\n",
        "    df_Injury_stats.loc[index,'rev_PTS'] = rev_game[['PTS']].PTS\n",
        "    df_Injury_stats.loc[index,'rev_PLUS_MINUS'] = rev_game[['PLUS_MINUS']].PLUS_MINUS\n",
        "    #storing game data from prior 5 games\n",
        "    df_Injury_stats.at[index,'po5_MIN'] = post5[['MIN']].MIN.mean()\n",
        "    df_Injury_stats.at[index,'po5_FGA'] = post5[['FGA']].FGA.mean()\n",
        "    df_Injury_stats.at[index,'po5_FG_PCT'] = post5[['FG_PCT']].FG_PCT.mean()\n",
        "    df_Injury_stats.at[index,'po5_FG3A'] = post5[['FG3A']].FG3A.mean()\n",
        "    df_Injury_stats.at[index,'po5_FG3_PCT'] = post5[['FG3_PCT']].FG3_PCT.mean()\n",
        "    df_Injury_stats.at[index,'po5_FTA'] = post5[['FTA']].FTA.mean()\n",
        "    df_Injury_stats.at[index,'po5_FT_PCT'] = post5[['FT_PCT']].FT_PCT.mean()\n",
        "    df_Injury_stats.at[index,'po5_REB'] = post5[['REB']].REB.mean()\n",
        "    df_Injury_stats.at[index,'po5_AST'] = post5[['AST']].AST.mean()\n",
        "    df_Injury_stats.at[index,'po5_STL'] = post5[['STL']].STL.mean()\n",
        "    df_Injury_stats.at[index,'po5_BLK'] = post5[['BLK']].BLK.mean()\n",
        "    df_Injury_stats.at[index,'po5_TO'] = post5[['TO']].TO.mean()\n",
        "    df_Injury_stats.at[index,'po5_PF'] = post5[['PF']].PF.mean()\n",
        "    df_Injury_stats.at[index,'po5_PTS'] = post5[['PTS']].PTS.mean()\n",
        "    df_Injury_stats.at[index,'po5_PLUS_MINUS'] = post5[['PLUS_MINUS']].PLUS_MINUS.mean()\n",
        "        #print(inj_game)\n",
        "df_Injury_stats = pd.merge(df_Injury_stats, df_Season_stats, how='left', left_on=['Player', 'SeasonInjured'], right_on=['player_name', 'Season_year'])\n",
        "df_Injury_stats = df_Injury_stats.drop(['Unnamed: 0_x', 'Unnamed: 0_y', 'Unnamed: 0.1', 'player_name', 'team_abbreviation', 'Season_year'], axis=1)\n",
        "df_Injury_stats.to_csv('df_Injury_stats.csv')\n"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}