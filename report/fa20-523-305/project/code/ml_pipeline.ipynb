{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os as os\n",
    "from time import time, strftime\n",
    "from datetime import datetime\n",
    "from cloudmesh.common.StopWatch import StopWatch\n",
    "from cloudmesh.common.Benchmark import Benchmark\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, explained_variance_score, r2_score\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.4\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+------------------+--------------------------------------------------------------------+\n",
      "| Attribute        | Value                                                              |\n",
      "|------------------+--------------------------------------------------------------------|\n",
      "| cpu_count        | 12                                                                 |\n",
      "| mem.available    | 21.6 GiB                                                           |\n",
      "| mem.free         | 21.6 GiB                                                           |\n",
      "| mem.percent      | 55.1 %                                                             |\n",
      "| mem.total        | 47.9 GiB                                                           |\n",
      "| mem.used         | 26.4 GiB                                                           |\n",
      "| platform.version | ('10', '10.0.18362', 'SP0', '')                                    |\n",
      "| python           | 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] |\n",
      "| python.pip       | 20.0.2                                                             |\n",
      "| python.version   | 3.7.4                                                              |\n",
      "| sys.platform     | win32                                                              |\n",
      "| uname.machine    | AMD64                                                              |\n",
      "| uname.node       | DESKTOP-PDVG430                                                    |\n",
      "| uname.processor  | Intel64 Family 6 Model 158 Stepping 10, GenuineIntel               |\n",
      "| uname.release    | 10                                                                 |\n",
      "| uname.system     | Windows                                                            |\n",
      "| uname.version    | 10.0.18362                                                         |\n",
      "| user             | codyh                                                              |\n",
      "+------------------+--------------------------------------------------------------------+\n",
      "\n",
      "+--------+----------+--------+-------+---------------------+-------+-----------------+--------+---------+---------------------------------+\n",
      "| Name   | Status   |   Time |   Sum | Start               | tag   | Node            | User   | OS      | Version                         |\n",
      "|--------+----------+--------+-------+---------------------+-------+-----------------+--------+---------+---------------------------------|\n",
      "| a      | ok       |  3.001 | 3.001 | 2020-11-16 22:05:00 |       | DESKTOP-PDVG430 | codyh  | Windows | ('10', '10.0.18362', 'SP0', '') |\n",
      "+--------+----------+--------+-------+---------------------+-------+-----------------+--------+---------+---------------------------------+\n",
      "\n",
      "# csv,timer,status,time,sum,start,tag,uname.node,user,uname.system,platform.version\n",
      "# csv,a,ok,3.001,3.001,2020-11-16 22:05:00,,DESKTOP-PDVG430,codyh,Windows,('10', '10.0.18362', 'SP0', '')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "StopWatch.start(\"a\")\n",
    "time.sleep(3)\n",
    "StopWatch.stop(\"a\")\n",
    "StopWatch.status(\"a\", True)\n",
    "StopWatch.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b():\n",
    "  Benchmark.Start()\n",
    "  print (\"b\")\n",
    "  import time\n",
    "  time.sleep(3)\n",
    "  Benchmark.Stop()\n",
    "\n",
    "def c():\n",
    "  Benchmark.Start()\n",
    "  print (\"c\")\n",
    "  import time\n",
    "  time.sleep(1)\n",
    "  Benchmark.Stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "b()\n",
    "c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+------------------+--------------------------------------------------------------------+\n",
      "| Attribute        | Value                                                              |\n",
      "|------------------+--------------------------------------------------------------------|\n",
      "| cpu_count        | 12                                                                 |\n",
      "| mem.available    | 21.6 GiB                                                           |\n",
      "| mem.free         | 21.6 GiB                                                           |\n",
      "| mem.percent      | 55.0 %                                                             |\n",
      "| mem.total        | 47.9 GiB                                                           |\n",
      "| mem.used         | 26.4 GiB                                                           |\n",
      "| platform.version | ('10', '10.0.18362', 'SP0', '')                                    |\n",
      "| python           | 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] |\n",
      "| python.pip       | 20.0.2                                                             |\n",
      "| python.version   | 3.7.4                                                              |\n",
      "| sys.platform     | win32                                                              |\n",
      "| uname.machine    | AMD64                                                              |\n",
      "| uname.node       | DESKTOP-PDVG430                                                    |\n",
      "| uname.processor  | Intel64 Family 6 Model 158 Stepping 10, GenuineIntel               |\n",
      "| uname.release    | 10                                                                 |\n",
      "| uname.system     | Windows                                                            |\n",
      "| uname.version    | 10.0.18362                                                         |\n",
      "| user             | codyh                                                              |\n",
      "+------------------+--------------------------------------------------------------------+\n",
      "\n",
      "+-----------------------------------+----------+--------+-------+---------------------+-------+-----------------+--------+---------+---------------------------------+\n",
      "| Name                              | Status   |   Time |   Sum | Start               | tag   | Node            | User   | OS      | Version                         |\n",
      "|-----------------------------------+----------+--------+-------+---------------------+-------+-----------------+--------+---------+---------------------------------|\n",
      "| a                                 | ok       |  3.001 | 3.001 | 2020-11-16 22:05:00 |       | DESKTOP-PDVG430 | codyh  | Windows | ('10', '10.0.18362', 'SP0', '') |\n",
      "| <ipython-input-57-124396bcabb3>/b | ok       |  3.045 | 3.045 | 2020-11-16 22:05:03 |       | DESKTOP-PDVG430 | codyh  | Windows | ('10', '10.0.18362', 'SP0', '') |\n",
      "| <ipython-input-57-124396bcabb3>/c | ok       |  1.041 | 1.041 | 2020-11-16 22:05:06 |       | DESKTOP-PDVG430 | codyh  | Windows | ('10', '10.0.18362', 'SP0', '') |\n",
      "+-----------------------------------+----------+--------+-------+---------------------+-------+-----------------+--------+---------+---------------------------------+\n",
      "\n",
      "# csv,timer,status,time,sum,start,tag,uname.node,user,uname.system,platform.version\n",
      "# csv,a,ok,3.001,3.001,2020-11-16 22:05:00,,DESKTOP-PDVG430,codyh,Windows,('10', '10.0.18362', 'SP0', '')\n",
      "# csv,<ipython-input-57-124396bcabb3>/b,ok,3.045,3.045,2020-11-16 22:05:03,,DESKTOP-PDVG430,codyh,Windows,('10', '10.0.18362', 'SP0', '')\n",
      "# csv,<ipython-input-57-124396bcabb3>/c,ok,1.041,1.041,2020-11-16 22:05:06,,DESKTOP-PDVG430,codyh,Windows,('10', '10.0.18362', 'SP0', '')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Benchmark.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load_Data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "        self.weather_dir = ''\n",
    "        self.soil_dir = ''\n",
    "        self.drop_columns = ['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'AWND_ATTRIBUTES', 'PGTM_ATTRIBUTES', \n",
    "                             'PSUN', 'PSUN_ATTRIBUTES', 'SNOW', 'SNOW_ATTRIBUTES', 'SNWD', 'SNWD_ATTRIBUTES', 'TAVG',\n",
    "                             'TAVG_ATTRIBUTES', 'TMAX_ATTRIBUTES', 'TMIN_ATTRIBUTES', 'TSUN', 'TSUN_ATTRIBUTES', 'WDF2_ATTRIBUTES', \n",
    "                             'WDF5_ATTRIBUTES', 'WSF2_ATTRIBUTES','WSF5_ATTRIBUTES', 'WT01_ATTRIBUTES', 'WT02_ATTRIBUTES', \n",
    "                             'WT03_ATTRIBUTES', 'WT06_ATTRIBUTES', 'WT08_ATTRIBUTES', 'PRCP_ATTRIBUTES']\n",
    "        \n",
    "    def fit(self, w_dir, s_dir):\n",
    "        self.weather_dir = w_dir\n",
    "        self.soil_dir = s_dir\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #Aggregate all 43 files into one file\n",
    "        file_list = os.listdir(self.soil_dir)\n",
    "        agg_data = pd.DataFrame()\n",
    "        for file in file_list:\n",
    "            path = self.soil_dir + file\n",
    "            curr_data = pd.read_csv(path, sep='\\t')\n",
    "            agg_data = agg_data.append(curr_data)\n",
    "        \n",
    "        #Drop rows with only NAs for measurement values\n",
    "        soil = agg_data.dropna(thresh=10)\n",
    "        \n",
    "        #Import weather files and drop unnessecary fields\n",
    "        weather = pd.read_csv(self.weather_dir)\n",
    "        drop_cols = list(set(weather.columns).intersection(self.drop_columns))\n",
    "        weather = weather.drop(columns = self.drop_columns)\n",
    "        \n",
    "        #Convert both files to use same datetime\n",
    "        soil['Date'] = pd.to_datetime(soil['Date'])\n",
    "        weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "        \n",
    "        #Join previous 16 days weather to moisture readings\n",
    "        for i in range(0, 17):\n",
    "            weather_new = weather.add_suffix('_' + str(i))\n",
    "            soil = soil.merge(weather_new, how = 'left', left_on = 'Date', right_on = weather['DATE'] - pd.DateOffset(i * -1))\n",
    "            \n",
    "        #Store the month of the reading as a feature\n",
    "        soil['Month'] = pd.DatetimeIndex(soil['Date']).month\n",
    "        \n",
    "        date_attribs = ['Date', 'DATE_0', 'DATE_1', 'DATE_2', 'DATE_3', 'DATE_4','DATE_5', 'DATE_6', 'DATE_7', 'DATE_8', 'DATE_9', 'DATE_10', \\\n",
    "                        'DATE_11', 'DATE_12', 'DATE_13', 'DATE_14', 'DATE_15', 'DATE_16']\n",
    "        \n",
    "        if 'DATE_0' in list(soil.columns):\n",
    "            soil.drop(columns = date_attribs, inplace = True)\n",
    "        soil['Location'] = soil['Location'].astype('object')\n",
    "            \n",
    "        return soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Engineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #Add categorical feature that simply stores if it rained that day or not\n",
    "        for i in range(17):\n",
    "            col_name = 'PRCP_' + str(i)\n",
    "            rain_y_n_name = 'RAIN_Y_N_' + str(i)\n",
    "            X[rain_y_n_name] = np.nan\n",
    "            X[rain_y_n_name].loc[X[col_name] > 0] = 1\n",
    "            X[rain_y_n_name].loc[X[col_name] == 0] = 0\n",
    "            X[rain_y_n_name] = X[rain_y_n_name].astype('object')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print(X)\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert_Date(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names = None):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X['Date'] = pd.to_timedelta(X['Date']).dt.total_seconds().astype(int)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "soil_file_dir = '../data/soil/'\n",
    "weather_file_dir = '../data/weather/weather_data.csv'\n",
    "x = 0\n",
    "\n",
    "pre_work_pipeline = Pipeline([\n",
    "    ('prework', Load_Data()),\n",
    "    ('features', Feature_Engineer())\n",
    "])\n",
    "\n",
    "pre_work_pipeline.fit(weather_file_dir, soil_file_dir)\n",
    "prework_df = pre_work_pipeline.transform(x)\n",
    "#Save to CSV so that we do not need to import and clean data everytime\n",
    "prework_df.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Classifier Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = ['VW_30cm', 'VW_60cm', 'VW_90cm', 'VW_120cm', 'VW_150cm']\n",
    "\n",
    "for cols in y_cols:\n",
    "    name = cols[3:] + '_class'\n",
    "    prework_df[name] = ''\n",
    "    prework_df[name].loc[(prework_df[cols] <= 0.1)] = '0.1'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.1) & (prework_df[cols] <= 0.2)] = '0.2'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.2) & (prework_df[cols] <= 0.3)] = '0.3'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.3) & (prework_df[cols] <= 0.4)] = '0.4'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.4) & (prework_df[cols] <= 0.5)] = '0.5'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.5) & (prework_df[cols] <= 0.6)] = '0.6'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.6) & (prework_df[cols] <= 0.7)] = '0.7'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.7) & (prework_df[cols] <= 0.8)] = '0.8'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.8)] = '0.9'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Frames for Each Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moisture data is taken at various depths. We want to build models seperately for different depths. So we need to make a dataframe for each depth so that we can elminate entire rows where the predictor is NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split out y values\n",
    "all_y_cols = ['VW_30cm', 'VW_60cm', 'VW_90cm', 'VW_120cm', 'VW_150cm', '30cm_class', '60cm_class', '90cm_class', '120cm_class', '150cm_class']\n",
    "X_sets = {}\n",
    "y_sets = {}\n",
    "x_cols = [col for col in prework_df.columns if col not in y_cols]\n",
    "X = prework_df.loc[:, x_cols]\n",
    "#y = prework_df.loc[:, y_cols]\n",
    "\n",
    "for cols in all_y_cols:\n",
    "    if cols[:1] == 'V':\n",
    "        dataset_name = cols[3:]\n",
    "    else:\n",
    "        dataset_name = cols\n",
    "    holder = prework_df.dropna(subset = [cols])\n",
    "    X_sets[dataset_name] = holder[x_cols].fillna(0)\n",
    "    y_sets[dataset_name] = holder[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test data\n",
    "# 80-20 ratio\n",
    "# Trying to keep same ratios for each location using stratify\n",
    "# Could have done this in the cell above, but wanted a seperate step for this\n",
    "X_train_set = {}\n",
    "X_test_set = {}\n",
    "y_train_set = {}\n",
    "y_test_set = {}\n",
    "\n",
    "for cols in all_y_cols:\n",
    "    if cols[:1] == 'V':\n",
    "        dataset_name = cols[3:]\n",
    "    else:\n",
    "        dataset_name = cols \n",
    "    X_train_set[dataset_name], X_test_set[dataset_name], y_train_set[dataset_name], y_test_set[dataset_name] = train_test_split(X_sets[dataset_name], y_sets[dataset_name], \\\n",
    "                                                                                                                                test_size=0.2, stratify = X_sets[dataset_name]['Location'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = X_train_set['60cm'].select_dtypes(exclude=['object', 'category']).columns\n",
    "cat_attribs = X_train_set['60cm'].select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = 0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = '')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_attribs),\n",
    "        ('cat', categorical_transformer, cat_attribs)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Experiment  Depth  Fit_Time  Pred_Time      r2_score  \\\n",
      "0  First Linear Reg   30cm  2.029387   0.169824  9.162373e-01   \n",
      "1  First Linear Reg   60cm  2.002373   0.173770 -1.418205e+15   \n",
      "2  First Linear Reg   90cm  2.080393   0.162992  9.487957e-01   \n",
      "3  First Linear Reg  120cm  2.299457   0.180560  9.460123e-01   \n",
      "4  First Linear Reg  150cm  2.573193   0.186042  9.432325e-01   \n",
      "\n",
      "              datetime  \n",
      "0  2020-11-16 15:41:56  \n",
      "1  2020-11-16 15:41:58  \n",
      "2  2020-11-16 15:42:01  \n",
      "3  2020-11-16 15:42:03  \n",
      "4  2020-11-16 15:42:06  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', LinearRegression())])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    log = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "    \n",
    "for cols in data_cols:\n",
    "    t0 = time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log.loc[len(log)] = ['First Linear Reg', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great Scores, but oddly 60 cm has a very small r2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Experiment  Depth  Fit_Time  Pred_Time      r2_score  \\\n",
      "0       First Linear Reg   30cm  2.029387   0.169824  9.162373e-01   \n",
      "1       First Linear Reg   60cm  2.002373   0.173770 -1.418205e+15   \n",
      "2       First Linear Reg   90cm  2.080393   0.162992  9.487957e-01   \n",
      "3       First Linear Reg  120cm  2.299457   0.180560  9.460123e-01   \n",
      "4       First Linear Reg  150cm  2.573193   0.186042  9.432325e-01   \n",
      "5  Ridge Reg - Alpha = 1   30cm  1.321553   0.173714  9.162112e-01   \n",
      "6  Ridge Reg - Alpha = 1   60cm  1.291670   0.187392  9.427566e-01   \n",
      "7  Ridge Reg - Alpha = 1   90cm  1.393526   0.197152  9.487904e-01   \n",
      "8  Ridge Reg - Alpha = 1  120cm  1.307926   0.176656  9.460320e-01   \n",
      "9  Ridge Reg - Alpha = 1  150cm  1.337380   0.179585  9.433202e-01   \n",
      "\n",
      "              datetime  \n",
      "0  2020-11-16 15:41:56  \n",
      "1  2020-11-16 15:41:58  \n",
      "2  2020-11-16 15:42:01  \n",
      "3  2020-11-16 15:42:03  \n",
      "4  2020-11-16 15:42:06  \n",
      "5  2020-11-16 15:45:58  \n",
      "6  2020-11-16 15:45:59  \n",
      "7  2020-11-16 15:46:01  \n",
      "8  2020-11-16 15:46:02  \n",
      "9  2020-11-16 15:46:04  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', Ridge(alpha = 1))])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    log = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "    \n",
    "for cols in data_cols:\n",
    "    t0 = time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log.loc[len(log)] = ['Ridge Reg - Alpha = 1', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results are better! Let's try Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Experiment  Depth  Fit_Time  Pred_Time      r2_score  \\\n",
      "0        First Linear Reg   30cm  2.029387   0.169824  9.162373e-01   \n",
      "1        First Linear Reg   60cm  2.002373   0.173770 -1.418205e+15   \n",
      "2        First Linear Reg   90cm  2.080393   0.162992  9.487957e-01   \n",
      "3        First Linear Reg  120cm  2.299457   0.180560  9.460123e-01   \n",
      "4        First Linear Reg  150cm  2.573193   0.186042  9.432325e-01   \n",
      "5   Ridge Reg - Alpha = 1   30cm  1.321553   0.173714  9.162112e-01   \n",
      "6   Ridge Reg - Alpha = 1   60cm  1.291670   0.187392  9.427566e-01   \n",
      "7   Ridge Reg - Alpha = 1   90cm  1.393526   0.197152  9.487904e-01   \n",
      "8   Ridge Reg - Alpha = 1  120cm  1.307926   0.176656  9.460320e-01   \n",
      "9   Ridge Reg - Alpha = 1  150cm  1.337380   0.179585  9.433202e-01   \n",
      "10  Lasso Reg - Alpha = 1   30cm  1.451020   0.170752 -1.832157e-04   \n",
      "11  Lasso Reg - Alpha = 1   60cm  1.419546   0.174177 -4.613909e-05   \n",
      "12  Lasso Reg - Alpha = 1   90cm  1.463200   0.176657 -5.673799e-06   \n",
      "13  Lasso Reg - Alpha = 1  120cm  1.553091   0.182349 -1.131381e-06   \n",
      "14  Lasso Reg - Alpha = 1  150cm  1.437419   0.163967 -1.814059e-04   \n",
      "\n",
      "               datetime  \n",
      "0   2020-11-16 15:41:56  \n",
      "1   2020-11-16 15:41:58  \n",
      "2   2020-11-16 15:42:01  \n",
      "3   2020-11-16 15:42:03  \n",
      "4   2020-11-16 15:42:06  \n",
      "5   2020-11-16 15:45:58  \n",
      "6   2020-11-16 15:45:59  \n",
      "7   2020-11-16 15:46:01  \n",
      "8   2020-11-16 15:46:02  \n",
      "9   2020-11-16 15:46:04  \n",
      "10  2020-11-16 15:48:38  \n",
      "11  2020-11-16 15:48:40  \n",
      "12  2020-11-16 15:48:41  \n",
      "13  2020-11-16 15:48:43  \n",
      "14  2020-11-16 15:48:45  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', Lasso(alpha = 1))])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    log = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "    \n",
    "for cols in data_cols:\n",
    "    t0 = time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log.loc[len(log)] = ['Lasso Reg - Alpha = 1', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At least with with these parameters, Lasso Fits Poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge with a built in gridsearch cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Experiment  Depth  Fit_Time  Pred_Time      r2_score  \\\n",
      "0        First Linear Reg   30cm  2.029387   0.169824  9.162373e-01   \n",
      "1        First Linear Reg   60cm  2.002373   0.173770 -1.418205e+15   \n",
      "2        First Linear Reg   90cm  2.080393   0.162992  9.487957e-01   \n",
      "3        First Linear Reg  120cm  2.299457   0.180560  9.460123e-01   \n",
      "4        First Linear Reg  150cm  2.573193   0.186042  9.432325e-01   \n",
      "5   Ridge Reg - Alpha = 1   30cm  1.321553   0.173714  9.162112e-01   \n",
      "6   Ridge Reg - Alpha = 1   60cm  1.291670   0.187392  9.427566e-01   \n",
      "7   Ridge Reg - Alpha = 1   90cm  1.393526   0.197152  9.487904e-01   \n",
      "8   Ridge Reg - Alpha = 1  120cm  1.307926   0.176656  9.460320e-01   \n",
      "9   Ridge Reg - Alpha = 1  150cm  1.337380   0.179585  9.433202e-01   \n",
      "10  Lasso Reg - Alpha = 1   30cm  1.451020   0.170752 -1.832157e-04   \n",
      "11  Lasso Reg - Alpha = 1   60cm  1.419546   0.174177 -4.613909e-05   \n",
      "12  Lasso Reg - Alpha = 1   90cm  1.463200   0.176657 -5.673799e-06   \n",
      "13  Lasso Reg - Alpha = 1  120cm  1.553091   0.182349 -1.131381e-06   \n",
      "14  Lasso Reg - Alpha = 1  150cm  1.437419   0.163967 -1.814059e-04   \n",
      "15       Ridge Reg - GSCV   30cm  3.914718   0.203007  9.162351e-01   \n",
      "16       Ridge Reg - GSCV   60cm  3.726651   0.172752  9.427570e-01   \n",
      "17       Ridge Reg - GSCV   90cm  4.135154   0.200589  9.487957e-01   \n",
      "18       Ridge Reg - GSCV  120cm  4.032030   0.193512  9.460322e-01   \n",
      "19       Ridge Reg - GSCV  150cm  4.361977   0.191296  9.433280e-01   \n",
      "\n",
      "               datetime  \n",
      "0   2020-11-16 15:41:56  \n",
      "1   2020-11-16 15:41:58  \n",
      "2   2020-11-16 15:42:01  \n",
      "3   2020-11-16 15:42:03  \n",
      "4   2020-11-16 15:42:06  \n",
      "5   2020-11-16 15:45:58  \n",
      "6   2020-11-16 15:45:59  \n",
      "7   2020-11-16 15:46:01  \n",
      "8   2020-11-16 15:46:02  \n",
      "9   2020-11-16 15:46:04  \n",
      "10  2020-11-16 15:48:38  \n",
      "11  2020-11-16 15:48:40  \n",
      "12  2020-11-16 15:48:41  \n",
      "13  2020-11-16 15:48:43  \n",
      "14  2020-11-16 15:48:45  \n",
      "15  2020-11-16 15:53:47  \n",
      "16  2020-11-16 15:53:51  \n",
      "17  2020-11-16 15:53:55  \n",
      "18  2020-11-16 15:54:00  \n",
      "19  2020-11-16 15:54:04  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', RidgeCV(alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]))])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    log = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "    \n",
    "for cols in data_cols:\n",
    "    t0 = time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log.loc[len(log)] = ['Ridge Reg - GSCV', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch found alpha = 1 to be the best parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Regressor Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now Ridge Regression with an alpha of 1 is winning as the best model so far. Let's see if we can beat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Experiment  Depth   Fit_Time  Pred_Time  r2_score  \\\n",
      "0  Random Forest - Default   30cm  60.069519   0.250543  0.977118   \n",
      "1  Random Forest - Default   60cm  62.174345   0.216641  0.989113   \n",
      "2  Random Forest - Default   90cm  62.294753   0.243051  0.991580   \n",
      "3  Random Forest - Default  120cm  64.482274   0.256666  0.991274   \n",
      "4  Random Forest - Default  150cm  68.470005   0.240149  0.991748   \n",
      "\n",
      "              datetime  \n",
      "0  2020-11-16 16:17:40  \n",
      "1  2020-11-16 16:18:43  \n",
      "2  2020-11-16 16:19:45  \n",
      "3  2020-11-16 16:20:50  \n",
      "4  2020-11-16 16:21:59  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', RandomForestRegressor())])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log_other\n",
    "except NameError:\n",
    "    log_other = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "for cols in data_cols:\n",
    "    t0 = time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_other.loc[len(log_other)] = ['Random Forest - Default', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing results! Although it takes considerably longer to train, the default does rather well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a litmus test, lets just try a few more models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Experiment  Depth    Fit_Time  Pred_Time  r2_score  \\\n",
      "0  Random Forest - Default   30cm   60.069519   0.250543  0.977118   \n",
      "1  Random Forest - Default   60cm   62.174345   0.216641  0.989113   \n",
      "2  Random Forest - Default   90cm   62.294753   0.243051  0.991580   \n",
      "3  Random Forest - Default  120cm   64.482274   0.256666  0.991274   \n",
      "4  Random Forest - Default  150cm   68.470005   0.240149  0.991748   \n",
      "5            SVM - Default   30cm   38.838219   5.712513  0.676934   \n",
      "6            SVM - Default   60cm  106.281603   7.897556  0.766008   \n",
      "7            SVM - Default   90cm  102.943794   7.763206  0.788833   \n",
      "8            SVM - Default  120cm   79.764755   6.985236  0.760895   \n",
      "9            SVM - Default  150cm   96.463517   7.548365  0.760936   \n",
      "\n",
      "              datetime  \n",
      "0  2020-11-16 16:17:40  \n",
      "1  2020-11-16 16:18:43  \n",
      "2  2020-11-16 16:19:45  \n",
      "3  2020-11-16 16:20:50  \n",
      "4  2020-11-16 16:21:59  \n",
      "5  2020-11-16 16:30:39  \n",
      "6  2020-11-16 16:32:33  \n",
      "7  2020-11-16 16:34:24  \n",
      "8  2020-11-16 16:35:50  \n",
      "9  2020-11-16 16:37:34  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', SVR())])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log_other\n",
    "except NameError:\n",
    "    log_other = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "for cols in data_cols:\n",
    "    t0 = time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_other.loc[len(log_other)] = ['SVM - Default', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just with the default values, SVM, did not perform well, but this could just mean that default parameters are not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Experiment  Depth    Fit_Time  Pred_Time  r2_score  \\\n",
      "0   Random Forest - Default   30cm   60.069519   0.250543  0.977118   \n",
      "1   Random Forest - Default   60cm   62.174345   0.216641  0.989113   \n",
      "2   Random Forest - Default   90cm   62.294753   0.243051  0.991580   \n",
      "3   Random Forest - Default  120cm   64.482274   0.256666  0.991274   \n",
      "4   Random Forest - Default  150cm   68.470005   0.240149  0.991748   \n",
      "5             SVM - Default   30cm   38.838219   5.712513  0.676934   \n",
      "6             SVM - Default   60cm  106.281603   7.897556  0.766008   \n",
      "7             SVM - Default   90cm  102.943794   7.763206  0.788833   \n",
      "8             SVM - Default  120cm   79.764755   6.985236  0.760895   \n",
      "9             SVM - Default  150cm   96.463517   7.548365  0.760936   \n",
      "10            SGD - Default   30cm    1.382992   0.171777  0.890190   \n",
      "11            SGD - Default   60cm    1.392753   0.151280  0.931394   \n",
      "12            SGD - Default   90cm    1.399587   0.142493  0.941092   \n",
      "13            SGD - Default  120cm    1.438626   0.150302  0.936692   \n",
      "14            SGD - Default  150cm    1.403488   0.149330  0.929570   \n",
      "\n",
      "               datetime  \n",
      "0   2020-11-16 16:17:40  \n",
      "1   2020-11-16 16:18:43  \n",
      "2   2020-11-16 16:19:45  \n",
      "3   2020-11-16 16:20:50  \n",
      "4   2020-11-16 16:21:59  \n",
      "5   2020-11-16 16:30:39  \n",
      "6   2020-11-16 16:32:33  \n",
      "7   2020-11-16 16:34:24  \n",
      "8   2020-11-16 16:35:50  \n",
      "9   2020-11-16 16:37:34  \n",
      "10  2020-11-16 17:06:05  \n",
      "11  2020-11-16 17:06:07  \n",
      "12  2020-11-16 17:06:08  \n",
      "13  2020-11-16 17:06:10  \n",
      "14  2020-11-16 17:06:11  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', SGDRegressor())])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log_other\n",
    "except NameError:\n",
    "    log_other = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "for cols in data_cols:\n",
    "    t0 = time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_other.loc[len(log_other)] = ['SGD - Default', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following, will take a considerable amount of time to run. Run with caution!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment is not included in the final report, but shows an extension of trying to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Param grid comes from the following site:\n",
    "## https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', RandomForestRegressor())])\n",
    "\n",
    "param_grid = {'classifier__bootstrap': [True, False],\n",
    "              'classifier__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "              'classifier__max_features': ['auto', 'sqrt'],\n",
    "              'classifier__min_samples_leaf': [1, 2, 4],\n",
    "              'classifier__min_samples_split': [2, 5, 10],\n",
    "              'classifier__n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "cv_res = {}\n",
    "try:\n",
    "    log_rf\n",
    "except NameError:\n",
    "    log_rf = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'best_params' 'datetime'])\n",
    "for cols in data_cols:\n",
    "    t0 = time()\n",
    "    random_search = RandomizedSearchCV(estimator = pipe_with_estimator, param_distributions = param_grid, n_iter = 10, cv = 3, verbose=10, random_state=42, n_jobs = -1)\n",
    "    random_search.fit(X_train_set[cols], y_train_set[cols])\n",
    "    best = random_search.best_params_\n",
    "    t1 = time()\n",
    "    preds = random_search.predict(X_test_set[cols])\n",
    "    t2 = time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_rf.loc[len(log_rf)] = ['RF - random search', cols, t1-t0, t2-t1, r2sc, best, now]\n",
    "    cv_res[cols] = random_search.cv_results_\n",
    "    print(log_rf)\n",
    "    \n",
    "print(log_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
