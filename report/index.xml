<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cybertraining – Reports</title><link>/report/</link><description>Recent content in Reports on Cybertraining</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Tue, 29 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="/report/index.xml" rel="self" type="application/rss+xml"/><item><title>Report: Tutorial on Getting PyCharm Professional for Free</title><link>/report/su21-reu-361/tutorials/pycharm/</link><pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-361/tutorials/pycharm/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This tutorial teaches how to get PyCharm Professional for free on Windows 10 using a university email address.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#steps">Steps&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> pycharm&lt;/p>
&lt;h2 id="steps">Steps&lt;/h2>
&lt;p>Click the following image to be redirected to a YouTube video tutorial for installing PyCharm Professional.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/QPESX-VBnEU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Please ensure that you have a university or college email before proceeding.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Open up a web browser and search &lt;code>pycharm&lt;/code>. Look under the link from &lt;code>jetbrains.com&lt;/code> and click &lt;code>Download Pycharm&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click the blue button that reads &lt;code>Download&lt;/code> under Professional. Wait for the download to complete.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Open the completely downloaded file and click &lt;code>Yes&lt;/code> on the UAC prompt.&lt;/p>
&lt;ol>
&lt;li>If you have a school computer, please refer to the note under step 5 in the Python tutorial found here:
&lt;a href="https://cybertraining-dsc.github.io/report/su21-reu-361/tutorials/python/">https://cybertraining-dsc.github.io/report/su21-reu-361/tutorials/python/&lt;/a>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Next&lt;/code>, click &lt;code>Next&lt;/code> again, and check the box that reads &lt;code>Add launchers dir to the PATH&lt;/code>. You can also
create a Desktop Shortcut and create the &lt;code>.py&lt;/code> association, if you would like. The association changes which program,
by default, opens &lt;code>.py&lt;/code> files on your computer.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Next&lt;/code> and then click &lt;code>Install&lt;/code>. Wait for the green progress bar to complete. Then, you must restart your
computer after making sure all of your programs are saved and closed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Open PyCharm either by clicking on the Desktop shortcut you might have made, or hit the Windows key and type
&lt;code>PyCharm&lt;/code> and choose the program from the search results.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check the box that says &lt;code>I confirm that I have read and accept the terms...&lt;/code> after reading through each and every
word and fully committing every character on your screen to memory. Only if you want to!&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Continue&lt;/code>. You can choose to send anonymous statistics, if you want to; click the option you want.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click the hyperlink that says &lt;code>Buy license&lt;/code> in the top right of the window. Do not worry— you will not be spending
a cent.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click the person icon in the top right of the page (if you cannot find this person icon, then click this link
and hopefully it still works: &lt;a href="https://account.jetbrains.com/login)">https://account.jetbrains.com/login)&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create a JetBrains account by entering your university email address. Click &lt;code>Sign Up&lt;/code> after entering your email;
then, you have to go on your email and confirm your account in the automated email sent to you. Click &lt;code>Confirm your account&lt;/code> in the email.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Complete the registration form and click &lt;code>Submit&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Apply for a free student or teacher license&lt;/code>. Scroll down and click the blue button that reads &lt;code>Apply now&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Fill out the form, using your university email address and real name. Check the boxes if they apply to you. Then
click &lt;code>APPLY FOR FREE PRODUCTS&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>JetBrains should send you an automated email, ideally informing you that your information has been confirmed and
you have been granted a free license. If it does not immediately arrive, wait a few minutes. Go back to PyCharm and
sign in with your JetBrains account after receiving this email. Click &lt;code>Activate&lt;/code>. Congratulations!&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Report: Tutorial on Installing Python</title><link>/report/su21-reu-361/tutorials/python/</link><pubDate>Thu, 24 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-361/tutorials/python/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This tutorial teaches how to install Python on Windows 10.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#windows">Windows&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mac">Mac&lt;/a>&lt;/li>
&lt;li>&lt;a href="#linux">Linux&lt;/a>&lt;/li>
&lt;li>&lt;a href="#troubleshooting">Troubleshooting&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#incorrect-python-version-on-command-prompt">Incorrect Python Version on Command Prompt&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> python&lt;/p>
&lt;h2 id="windows">Windows&lt;/h2>
&lt;p>Click the following image to be redirected to a 2-minute YouTube walkthrough.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/T6UYyu5XVMc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>First, open up any web browser. This tutorial utilizes Google Chrome, but any other browser should work as long as it is not a 1990s version of Netscape. (Do not worry— you probably don&amp;rsquo;t have this.) The browser of choice can be Microsoft Edge, Firefox, Opera— as long as it can perform a search on a search engine, access a webpage, and download a file.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Open your browser by clicking the search box in the bottom left of your screen, where it says &amp;ldquo;Type here to search&amp;rdquo;. Then, type &amp;ldquo;google chrome&amp;rdquo; (or whatever is the name of the browser you use) and click it once it appears.&lt;/p>
&lt;ol>
&lt;li>The &amp;ldquo;Type here to search&amp;rdquo; box could be missing if you have customized your taskbar (the taskbar is the long box typically located on the bottom of your screen which has icons). In this case, just click the Windows logo in the bottom left and type your browser name.&lt;/li>
&lt;li>This is just one way to open your browser. You can even click a shortcut to your web browser on your taskbar, on your Desktop, or your Start Menu. In computing, there is typically many ways to accomplish the same end objective.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Once your browser has loaded, search for &amp;ldquo;python&amp;rdquo; on Google or any search engine. Click the result that reads &amp;ldquo;Downloads&amp;rdquo; from the website &amp;ldquo;python.org&amp;rdquo;.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>As of June 2021, the latest version of Python is &lt;code>3.9.5&lt;/code>. You may see a different number. As long as you click the button under &amp;ldquo;Download the latest version for Windows&amp;rdquo;, this will work. Try it now.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the download has completed, open the file by clicking on it in your Downloads pane.&lt;/p>
&lt;ol>
&lt;li>If you are utilizing a school-issued computer, you may be prevented from opening this .exe file because you are not the administrator. Please email or otherwise get in contact with your instructor, professor, or head of IT to discuss installing Python.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Be sure to check the box that reads &amp;ldquo;Add Python x.x to PATH&amp;rdquo;. This will allow you to run commands from the terminal/command prompt.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &amp;ldquo;Install Now&amp;rdquo;. The default options that entail this selection are appropriate for this experiment&amp;rsquo;s intents and purposes; choosing &amp;ldquo;Customize installation&amp;rdquo; may create reproducibility issues down the road, so please select &amp;ldquo;Install Now&amp;rdquo; instead.&lt;/p>
&lt;ol>
&lt;li>The UAC prompt will pop up. UAC stands for &amp;ldquo;User Account Control&amp;rdquo; and exists so that the computer will not have unauthorized changes performed on it. Click &amp;ldquo;Yes&amp;rdquo; because Python is safe. School-issued computers may ask for an administrator password, so refer to step 5&amp;rsquo;s sidenote.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Sit back and watch the green progress bar, whose speed will depend on the power of the computer.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>If the setup was successful, then it will say so. Click &amp;ldquo;Close&amp;rdquo;.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click the &amp;ldquo;Type here to search&amp;rdquo; box in the bottom-left of the screen, type &amp;ldquo;cmd&amp;rdquo;, and press Enter.&lt;/p>
&lt;ol>
&lt;li>An alternative method is to press the Windows key and the &amp;ldquo;R&amp;rdquo; key at the same time, type &amp;ldquo;cmd&amp;rdquo;, and press Enter. This is convenient for those who like to use the keyboard.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Type &lt;code>python --version&lt;/code> and the output should read &amp;ldquo;Python x.x.x&amp;rdquo;; as long as it is the latest version from the website, congratulations. Python is installed on the computer.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="mac">Mac&lt;/h2>
&lt;p>Click the following image to be redirected to a 5-minute YouTube walkthrough. (Yes, Mac&amp;rsquo;s video is a little longer, but do not fret!
You can skip to the 1:00 minute mark if you are in a hurry.)&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/TttmzM-EDmk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>Open a web browser that is able to search and download a file. This tutorial uses Google Chrome for Mac.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type in &lt;code>python&lt;/code> in the address bar and press enter. It should perform a search on your default search engine.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Look for the result that is from &lt;code>python.org&lt;/code>. Click on the subresult that says &lt;code>Downloads&lt;/code>.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Underneath &lt;code>Download the latest version for Mac OS X&lt;/code>, there should be a yellow button that reads &lt;code>Download Python x.x.x&lt;/code>. Click on it, and the download should commence.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the download finishes, open it by clicking on it. The installer will open. Click &lt;code>Continue&lt;/code>, click &lt;code>Continue&lt;/code> again, click &lt;code>Continue&lt;/code> again, oh my goodness!
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Agree&lt;/code>.&lt;/p>
&lt;ol>
&lt;li>If you want to check how much free storage you have on your computer, click the Apple icon in the top left of your computer. Click
&lt;code>About This Mac&lt;/code> and then click on &lt;code>Storage&lt;/code>. As of July 2021, Python takes ~120 MB of space. Remember that 1 GB = 1000 MB.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Install&lt;/code>. Enter your password and press Enter. Watch the blue progress bar crawl like a turtle&amp;hellip; or blast off at the speed of sound! This depends on your computer speed.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>A Finder window will open. You can close it as it is unnecessary. Click &lt;code>Close&lt;/code> in the bottom-right of the installer. Click &lt;code>Move to Trash&lt;/code> because you do not need the installer anymore.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Time to confirm that Python installed correctly. Click the magnifying glass in the top-right of your screen and then type &lt;code>terminal&lt;/code> into Spotlight Search. Double-click &lt;code>Terminal&lt;/code>.&lt;/p>
&lt;ol>
&lt;li>The terminal will be used frequently in this experiment. Consider keeping it in the dock for convenience. Click and hold the Terminal in the dock, go to &lt;code>Options&lt;/code>, and click &lt;code>Keep in Dock&lt;/code>.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Type &lt;code>python3 --version&lt;/code> into the terminal and press Enter. It should output the latest version of Python. Congratulations!
&lt;br>
 &lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="linux">Linux&lt;/h2>
&lt;p>Click the following image to be redirected to a 9-minute YouTube walkthrough. (Linux&amp;rsquo;s tutorial is the longest, but it is worth it.)
This tutorial uses Ubuntu, but it should work on other Linux distros, as well.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/TttmzM-EDmk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>Open a web browser. It can be any browser as long as it can perform a search and navigate to a webpage.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Search for &lt;code>python&lt;/code> by typing it into the address bar and pressing enter. Click on &lt;code>Downloads&lt;/code> underneath the result from &lt;code>https://www.python.org&lt;/code>.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Look at the latest version. It is on the yellow button: &lt;code>Download Python x.x.x&lt;/code>. You do not need to click this button. Remember this version number.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Open a terminal by pressing the Windows key, or by clicking the grid on the bottom left of your screen. Type &lt;code>terminal&lt;/code>. Click on the &lt;code>Terminal&lt;/code> result that appears.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type &lt;code>sudo apt-get update&lt;/code> and press Enter. Wait for it to finish. It may already be up-to-date.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type &lt;code>sudo apt-get install libssl-dev openssl make gcc&lt;/code> and press Enter. This will install the libraries required to connect to an FTP to download Python. Type your password for your Linux user account, if prompted, and press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>You are then asked if you are okay with a certain amount of disk space being taken up. Type &lt;code>y&lt;/code>, which stands for Yes, and then press Enter.&lt;/p>
&lt;ol>
&lt;li>If you want to check how much disk space you have, press the Files icon on the left (on the taskbar) and click &lt;code>Other Locations&lt;/code>. You may have to scroll down on the sidebar in order to see it. It should say how much GB is available. Remember, 1 GB = 1000 MB and 1 MB = 1000 KB.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>After this finishes, type &lt;code>cd /opt&lt;/code> and press Enter. Then, remember which version you read on the Python webpage (the latest version). Type &lt;code>sudo wget https://www.python.org/ftp/python/x.x.x/Python-x.x.x.tgz&lt;/code> after replacing the &lt;code>x.x.x&lt;/code> with the latest Python version number. As of July 2021, it is &lt;code>3.9.6&lt;/code>. Press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Wait for the download to complete. Then, type &lt;code>sudo tar xzvf Python-x.x.x.tgz&lt;/code> after you replace &lt;code>x.x.x&lt;/code> with the latest Python version number. Press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type &lt;code>cd Python-x.x.x&lt;/code> after replacing &lt;code>x.x.x&lt;/code> with the latest version number. Type &lt;code>./configure&lt;/code> and press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once it finishes, type &lt;code>make&lt;/code> and press Enter. Once &lt;em>that&lt;/em> finishes, type &lt;code>sudo make install&lt;/code> and press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the installation finishes, type &lt;code>sudo ln -fs /opt/Python-x.x.x/Python /usr/bin/pythonx.x&lt;/code>. Notice that &lt;code>x.x.x&lt;/code> should be replaced with the full version number and &lt;code>x.x&lt;/code> should have the first two numbers in the version number. Press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Confirm Python&amp;rsquo;s successful installation by typing &lt;code>pythonx.x --version&lt;/code>; be sure to replace x.x with the first two numbers of the version number. It should output the latest version number. Congratulations!
&lt;br>
 &lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Credit to bobbyiliev for making the required commands publicly available. The commands are available here, as well: &lt;a href="https://www.digitalocean.com/community/questions/how-to-install-a-specific-python-version-on-ubuntu">https://www.digitalocean.com/community/questions/how-to-install-a-specific-python-version-on-ubuntu&lt;/a>
&lt;br>
 &lt;/p>
&lt;h2 id="troubleshooting">Troubleshooting&lt;/h2>
&lt;h3 id="incorrect-python-version-on-command-prompt">Incorrect Python Version on Command Prompt&lt;/h3>
&lt;p>If the Windows computer has installed an older version of Python, running &lt;code>python --version&lt;/code> on Command Prompt may output an older version. Typing &lt;code>python3 --version&lt;/code> may output the correct, latest version.&lt;/p></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-366/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-366/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-366/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-366/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-366/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-366/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Firstname Lastname, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-366">su21-reu-366&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-366/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-367/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-367/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-367/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-367/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-367/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-367/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Firstname Lastname, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-367">su21-reu-367&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-367/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-368/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-368/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-368/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-368/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-368/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-368/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Yohn Jairo Parra Bautista, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-368">su21-reu-368&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-368/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-371/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-371/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-371/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-371/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-371/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-371/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Firstname Lastname, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-371">su21-reu-371&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-371/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-373/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-373/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-373/actions">&lt;img src="https://github.com/su21-reu-373/hid-example/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-373/actions">&lt;img src="https://github.com/su21-reu-373/hid-example/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Firstname Lastname, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-373">su21-reu-373&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-373/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-375/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-375/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-375/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-375/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Firstname Lastname, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-375">su21-reu-375&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-375/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-377/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-377/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-377/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-377/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-377/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-377/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>RonDaisja Dunn, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-377">su21-reu-377&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-377/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-379/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-379/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-379/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-379/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-379/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-379/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Carrington Kerr, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-379">su21-reu-379&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-379/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Marine aninmal population analysis using AI</title><link>/report/su21-reu-370/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-370/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-370/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-370/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-370/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-370/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Timia Williams, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-370">su21-reu-370&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-370/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Detection of Autism in Children from a Facial Image using AI</title><link>/report/su21-reu-378/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-378/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-378/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-378/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Myra Saunders, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-378">su21-reu-378&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-378/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Analyzing facial images to detect Autism in children.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Add brief summary that explains what my project is about.&lt;/li>
&lt;/ul>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-using-images">2. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-itemized-lists-only-where-needed">3. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-datasets">4. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-benchmark">5. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgments">7. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Autism, Facial Analysis.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Autism Spectrum Disorder (ASD) is a broad range of lifelong developmental and neurological disorders that usually appear during early childhood. Autism Spectrum Disorder can cause challenges with speech and nonverbal communication, repetitive behaviors, and social skills.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Briefly explain the topic of my report (Detection of autism in children using a facial image).&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> State how AI will be applied/used. (Talk about AI and Autism)&lt;/li>
&lt;/ul>
&lt;h2 id="2-using-images">2. Using Images&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Iniclude images in report. If copied, cite them.&lt;/li>
&lt;/ul>
&lt;h2 id="3-using-itemized-lists-only-where-needed">3. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="4-datasets">4. Datasets&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Querying the dataset autism prevalence studies summary with an area of three cities @data.world&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Find a similar dataset in kaggle.com&lt;/li>
&lt;/ul>
&lt;h2 id="5-benchmark">5. Benchmark&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Include a benchmark. (The easiest is to use cloudmesh-common [^2])&lt;/li>
&lt;/ul>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Conclude my project.&lt;/li>
&lt;/ul>
&lt;h2 id="7-acknowledgments">7. Acknowledgments&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Add acknowledgments.&lt;/li>
&lt;/ul>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Add References.&lt;/li>
&lt;/ul></description></item><item><title>Report: Breast Cancer and Genetics</title><link>/report/su21-reu-362/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-362/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-362/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-362/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-362/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-362/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Kehinde Ezekiel, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-362">su21-reu-362&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-362/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-example-of-an-ai-algorithm-in-healthcare">5. Example of an AI algorithm in Healthcare&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-datasets">6. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-benchmark">7. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-conclusion">8. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-acknowledgments">9. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#10-references">10. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ai, data science, disease, COVID-19.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Find literature about AI and healthcare disparities&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Find literature about AI and COVID-19&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Analyze the literature and explain how data science, ai is better to healthcare.&lt;/li>
&lt;/ul>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Finding datasets in orthodontics&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Can any of the datasets be used in AI?&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What are the challenges with Healthcare dataset? Privacy, Inconsistent Dataset&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/li>
&lt;/ul>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Place a cool image into projects images in my directory&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Correct the following link, replace the fa number with my su number and the chart of png&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> If the image has been copied, I must use a reference as shown in the Figure 1 caption&lt;/li>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.\&lt;/p>
&lt;h2 id="5-example-of-an-ai-algorithm-in-healthcare">5. Example of an AI algorithm in Healthcare&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete datasets that will be used.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete algorithm that is used to analyze the datasets&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Write the program&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Verify that it works&lt;/li>
&lt;/ul>
&lt;h2 id="6-datasets">6. Datasets&lt;/h2>
&lt;p>The dataset contains target sequencing data for about 1680 patients with breast cancer.&lt;/p>
&lt;h2 id="7-benchmark">7. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="8-conclusion">8. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="9-acknowledgments">9. Acknowledgments&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Gregor guided me through this process
Please add acknowledgments to all that contributed or helped on this project.&lt;/li>
&lt;/ul>
&lt;h2 id="10-references">10. References&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/li>
&lt;/ul>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Increasing Cervical Cancer Risk Analysis</title><link>/report/su21-reu-369/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-369/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-369/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-369/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Theresa Jean-Baptistee, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-369">su21-reu-369&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-369/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Cervical Cancer is an increasing matter that is affecting various women across the nation, in this project we will be analyzing risk factors that are producing higher chances of this cancer.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. DataSets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-other-people-works">3. Other People Works&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-images">4. Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-benchmark">5 Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgments">7. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Cervical, Cancer, Diseases, Data, conditions&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;ul>
&lt;li>Find literature on Cervical Cancer, Images of Cervics&lt;/li>
&lt;li>Analyzing reading and explain the importance of cancer in the cervix and along with how it affects the future of
the cervixs&lt;/li>
&lt;/ul>
&lt;h2 id="2-datasets">2. DataSets&lt;/h2>
&lt;ul>
&lt;li>The age of a person&lt;/li>
&lt;li>Medical status&lt;/li>
&lt;li>Tabacoo intake&lt;/li>
&lt;li>resources: &lt;a href="https://ieeexplore.ieee.org/document/8691126">https://ieeexplore.ieee.org/document/8691126&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="3-other-people-works">3. Other People Works&lt;/h2>
&lt;p>research of work others have done with the data set and what I have learned from that work.
put a refrence in section #9 for each work i have found&lt;/p>
&lt;h2 id="4-images">4. Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/blob/main/project/images/download-2021-06-29T15-34-01-628Z.png" alt="Figure 1">&lt;/p>
&lt;h2 id="5-benchmark">5 Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common [^2]&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="7-acknowledgments">7. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p></description></item><item><title>Report: Cyber Attacks Detection Using AI Algorithms</title><link>/report/su21-reu-365/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-365/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-365/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-365/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-365/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-365/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Victor Adankai, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-365">su21-reu-365&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-374/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#types-of-cyber-attacks">Types of Cyber Attacks&lt;/a>&lt;/li>
&lt;li>&lt;a href="#examples-of-ai-algorithms-for-cyber-attacks-detection">Examples of AI Algorithms for Cyber Attacks Detection&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-benchmark">4. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgments">6. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> AI, ML, DL, Cybersecurity, Cyber Attacks.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;ul>
&lt;li>Find literature about AI and Cyber Attacks on IoT Devices Dectection.&lt;/li>
&lt;li>Analyze the literature and explain how AI for Cyber Attacks on IOT Devices Detection are beneficial.&lt;/li>
&lt;/ul>
&lt;h4 id="types-of-cyber-attacks">Types of Cyber Attacks&lt;/h4>
&lt;ul>
&lt;li>Denial of service (DoS) Attack:&lt;/li>
&lt;li>Remote to Local Attack:&lt;/li>
&lt;li>Probing:&lt;/li>
&lt;li>User to Root Attack:&lt;/li>
&lt;li>Adversarial Attacks:&lt;/li>
&lt;li>Poisoning Attack:&lt;/li>
&lt;li>Evasion Attack:&lt;/li>
&lt;li>Integrity Attack:&lt;/li>
&lt;li>Malware Attack:&lt;/li>
&lt;li>Phising Attack:&lt;/li>
&lt;li>Zero Day Attack:&lt;/li>
&lt;li>Sinkhole Attack:&lt;/li>
&lt;li>Causative Attack:&lt;/li>
&lt;/ul>
&lt;h4 id="examples-of-ai-algorithms-for-cyber-attacks-detection">Examples of AI Algorithms for Cyber Attacks Detection&lt;/h4>
&lt;ul>
&lt;li>Convolutional Neural Network (CNN)&lt;/li>
&lt;li>Autoencoder (AE)&lt;/li>
&lt;li>Deep Belief Network (DBN)&lt;/li>
&lt;li>Recurrent Neural Network (RNN)&lt;/li>
&lt;li>Generative Adversal Network (GAN)&lt;/li>
&lt;li>Deep Reinforcement Learning (DIL)&lt;/li>
&lt;/ul>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;ul>
&lt;li>Finding data sets in IoT Devices Cyber Attacks.&lt;/li>
&lt;li>Can any of the data sets be used in AI?&lt;/li>
&lt;li>What are the challenges with IoT Devices Cyber Attacks data set? Privacy, HIPPA, Size, Avalibility&lt;/li>
&lt;li>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/li>
&lt;/ul>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;ul>
&lt;li>Place a cool image into projects images in my directory&lt;/li>
&lt;li>Correct the following link, replace the fa number with my su number and thne chart of png.&lt;/li>
&lt;li>If the image has been copied, you must use a reference such as shown in the Figure 1 caption.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-benchmark">4. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common [^2]&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="6-acknowledgments">6. Acknowledgments&lt;/h2>
&lt;ul>
&lt;li>Gregor von Laszewski&lt;/li>
&lt;li>Yohn Jairo Bautista&lt;/li>
&lt;li>Carlos Theran&lt;/li>
&lt;/ul>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Dentronics: Artifical intelligence in dentistry</title><link>/report/su21-reu-376/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-376/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-376/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-376/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-376/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-376/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Jamyla Young, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-376">su21-reu-376&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-376/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-example-of-an-ai-algorithm-in-dentronics">5. Example of an AI algorithm in Dentronics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Dentronics, neural pathways, prosthodotics.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Find literature about Dentronics and its effects&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Analyze the literature&lt;/li>
&lt;/ul>
&lt;h2 id="2-data-sets">2. Data sets&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Find data sets about effects about dentronics&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the purpose of data sets&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What are the challenges with Dentronics data sets? HIPPA, size, availibility&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub. However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/li>
&lt;/ul>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Place a cool image into projects images in my directory&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Correct the following, replace the fa number with my su number and the chart of png&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> if image is copied , you must use a reference such as shown in the Figure 1 caption&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> list&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="5-example-of-an-ai-algorithm-in-dentronics">5. Example of an AI algorithm in Dentronics&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete data set that will be used&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete algorithm that is used to anaylze the data set&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Write the program&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Verify that it works&lt;/li>
&lt;/ul>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Gregor helped me&lt;/li>
&lt;/ul>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>-[ ] Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at one point automatically change the references from superscript to square brackets it is best to introduce a space before the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analyzing Price Changes on the Electro-Optical System Blockchain</title><link>/report/su21-reu-361/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-361/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-361/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-361/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Jacques Fleischer, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-361">su21-reu-361&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Blockchain-based cryptocurrency is a popular investment of today&amp;rsquo;s age. However, the U.S. Securities and Exchange Commission warns that high-risk accompanies these investments[^1]. This project applies machine learning and Artificial Intelligence (AI) to historical records of these coins to predict future prices; this aims to minimize investment risk.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#to-do">To-do&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tutorials">Tutorials&lt;/a>&lt;/li>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-benchmark">4. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgments">6. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> cryptocurrency, investing, business, blockchain.&lt;/p>
&lt;h2 id="to-do">To-do&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> My first research on blockchain&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Add blockchain explanation to introduction&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What does blockchain have to do with AI?&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What codes exist that demonstrate the use of blockchain with AI using Python?&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What is the code in this project that will be developed?&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What is the performance of this code?&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What is my conclusion?&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Never, ever use the words &amp;lsquo;I&amp;rsquo; or &amp;lsquo;my&amp;rsquo; in a report&lt;/li>
&lt;/ul>
&lt;h2 id="tutorials">Tutorials&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Install software Visual Studio Code (create video tutorial or written instructions)&lt;/li>
&lt;/ul>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Blockchain is &amp;ldquo;an open, distributed ledger&amp;rdquo; which records transactions of cryptocurrency. It is decentralized, which means that these transactions are shared and distributed among all participants on the blockchain for maximum accountability. Furthermore, this new &amp;ldquo;blockchain&amp;rdquo; technology is becoming an increasingly popular alternative to mainstream transactions through traditional banks&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>An attractive new invention such as blockchain is a particularly desirable investment opportunity. However, its severe volatility can scare away possible investors; in 2017, one Bitcoin was worth $1,000 USD; a year later, its value multiplied by 20; and just a few months later, it decreased back down to $5,000&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Artificial intelligence will be applied in this experiment to predict the future price of blockchain cryptocurrencies on the Electro-Optical System.&lt;/p>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;p>This project utilizes a .csv file containing the historical prices of the EOS coin from the first day of its inception on 1 July 2017, to the last scraped day 12 December 2020&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. From this data, the project will attempt to predict the prices from the end date until the present day.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-361/main/project/images/eos_price.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Line graph of EOS price from 1 July 2017 til 12 December 2020. Generated using timeseries_generator.ipynb located in project/code.&lt;/p>
&lt;h2 id="4-benchmark">4. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="6-acknowledgments">6. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Marco Iansiti and Karim R. Lakhani, The Truth About Blockchain, [Online resource]
&lt;a href="https://hbr.org/2017/01/the-truth-about-blockchain">https://hbr.org/2017/01/the-truth-about-blockchain&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Jeremy Swinfen Green, Understanding cryptocurrency market fluctuations, [Online resource]
&lt;a href="https://www.telegraph.co.uk/business/business-reporter/cryptocurrency-market-fluctuations/">https://www.telegraph.co.uk/business/business-reporter/cryptocurrency-market-fluctuations/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Mehmet Tarik Akcay, Historical Data for Top 20 Coins by Market Cap, [Online resource]
&lt;a href="https://www.kaggle.com/mtakcy/historical-data-for-top-20-coins-by-market-cap?select=eos.csv">https://www.kaggle.com/mtakcy/historical-data-for-top-20-coins-by-market-cap?select=eos.csv&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/hid-example/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/hid-example/project/</guid><description>
&lt;p>Fix the links: and than remove this line&lt;/p>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/hid-example/actions">&lt;img src="https://github.com/cybertraining-dsc/hid-example/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/hid-example/actions">&lt;img src="https://github.com/cybertraining-dsc/hid-example/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Fix the links: and than remove this line&lt;/p>
&lt;p>Gregor von Laszewski, &lt;a href="https://github.com/cybertraining-dsc/hid-example/">hid-example&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Hand Tracking with AI</title><link>/report/su21-reu-364/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-364/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-364/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-364/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-364/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-364/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>David Umanzor, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-364">su21-reu-364&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-364/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>In this project we study the ability of an AI to recognize and track hand positions with a raspberry PI 4 with 8 GB of ram and 64 GB of storage.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data Sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-example-of-an-ai-algorithm-with-object-recognition">5. Example of an AI algorithm with Object Recognition&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ai, object recognition, image processing, computer vision, hand tracking.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Find liturature about image processing aand object recognition.&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Analyze liturature &amp;amp; explain how this relates.&lt;/li>
&lt;/ul>
&lt;h2 id="2-data-sets">2. Data Sets&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Collect image sets for the AI to recognize.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What do these data sets specify.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What are the challenges with Object Recognition? Accuracy.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Datasets can be huge and GitHub has lmited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available your program must contain a download function instead that you customize.
Write it using pythons &amp;lsquo;request&amp;rsquo;&lt;/li>
&lt;/ul>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Place cool image into projects images in my directory (data set of its ability to be accurate).&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Correct the following link, replace the fa with su, and the numbers to my reu numbers, change chart.png to file name.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> If the image has been copied, you must use a reference such as shown in the Figure 1 caption.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="5-example-of-an-ai-algorithm-with-object-recognition">5. Example of an AI algorithm with Object Recognition&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete data set that will be used.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the conrete algorithm that completes the image processing for Object Recognition.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Write the program.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Verify that it works.&lt;/li>
&lt;/ul>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Gregor Von Laszewski&lt;/li>
&lt;/ul>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/li>
&lt;/ul>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh CommonH Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Analysing Hashimoto disease causes using AI</title><link>/report/su21-reu-372/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-372/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-372/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-372/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Sheimy Paz, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-372">su21-reu-372&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Hashimoto thyroiditis is an organ-specific autoimmune disorder named after Japanese physician Hajaru Hashimoto, Hashimoto symptoms were first described 1912 but the disease was not recognize until 1957 before that year it was concider to be the early symptoms of Reidel&amp;rsquo;s Thyroiditis. In 1957 Hashimoto was recognize as an autoimmune disorder that destroys thyroid cells and antibody-mediated. Pathologically speaking, Hashimoto stimulates the formation of antithyrod antibodies that attack the thyroid tissue, causing progressive fibrosis. The disorder is difficult to diagnose since in the early course of the disease the patients may or may not exibit symtoms and/or laboratory findings of hyperthyrodism or normal values because the destruction of the gland cells may be intermittent. Clinical and epidemiological studies suggest worldwide that the most common cause of hypothyroidism is an inadecuated dietary intake of iodine and is the most common cause of hypothyroidism in developed countries.&lt;/p>
&lt;p>In a female-to-men radio at least 10:1 women are more often affected than men, and the diagnostics are called between the ages of 30 to 50 years. Studies suggest that an association between high levels of thyroid autoantibodies affect the increased frequencies of mood disorders and there was found a relation between thyroid autoimmunity disease, celiac disease and panic disorder and major depressive disorder.&lt;/p>
&lt;p>Here comes a convincing introduction to the problem:
Note Carlos Theran: It perfect how you explain the concept of Hashimoto thyroiditis and its affection in human&amp;rsquo;s helth. Now that you introduced what is Hashimoto thyroiditis. I have some question that could help you to address the research question (the problem) that you want to solve.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Given the introduction, I see that it is really difficult to detect Hashimoto thyroiditis in a person. So, Do you want to use ML that help you to identify early causes of Hashimoto thyroiditis on a person or you are looking for something else?&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What kind of data you spect to find in order to addres a solution of your problem?&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What kind of technique would you like to use to addres your problem?&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Is there any paper, article, blogs, or any source that have address this problem before, If so, how they did?&lt;/li>
&lt;/ul>
&lt;p>I hope this question help =)&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Querying the datasets of womans with hashimoto diaseas in an are of 3 cities @data.world&lt;/li>
&lt;/ul>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Classification of Hyperspectral Images</title><link>/report/su21-reu-360/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-360/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-360/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-360/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-360/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-360/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Carlos Theran, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-360">su21-reu-360&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-360/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>??
Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Tutorial on Installing Visual Studio Code</title><link>/report/su21-reu-361/tutorials/visual-studio-code/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-361/tutorials/visual-studio-code/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This tutorial teaches how to install Visual Studio Code on Windows 10.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#steps">Steps&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> visual-studio-code&lt;/p>
&lt;h2 id="steps">Steps&lt;/h2>
&lt;p>Click the following image to be redirected to a YouTube video tutorial for installing Visual Studio Code (also called VSCode).&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/DG_wQslWWAc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Sidenote: An exasperated reader may wonder, &amp;ldquo;why go through steps 1-3 when it can be as easy as clicking a link to the VSCode download page?&amp;rdquo; This &lt;em>would&lt;/em> be easier, but hyperlinks (or URLs) are bound to change through the years of website maintenance and alterations. (One could also argue that steps 1-3 could become incorrect, as well, but hopefully they will not.) If you, time-traveler, would like to try your luck, go here: &lt;a href="https://code.visualstudio.com/download">https://code.visualstudio.com/download&lt;/a>&lt;/p>
&lt;p>If the link works, skip to step 4.&lt;/p>
&lt;p>P.S. It should be second-nature to a user to quickly search, find, download, and install a program. It is vital to ensure that the correct program is downloaded and installed, however. Over time, guides like this one can become deprecated, but one must be resilient in problem-solving. Use search engines like Google to find what you are looking for. If one path does not work, take another that will lead to the same destination or a better one.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Open up your favorite web browser. This can be done by pressing the Windows key and typing in the name of the browser, like &lt;code>google chrome&lt;/code> (as long as this browser is already installed on your computer). Then press Enter.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the browser loads, search for &lt;code>visual studio code&lt;/code> through the address bar. Press Enter and you will see a list of results through the default search engine (Google, Bing, or whatever is configured on your browser).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Identify the result that reads &lt;code>code.visualstudio.com&lt;/code>. If using Google, a subresult should read &lt;code>Download&lt;/code>. Click that link.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>This tutorial assumes that the reader is using Windows. Click the blue link that reads &lt;code>Windows&lt;/code>. The download will commence; wait for it to finish.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click and open the file once it finishes; the license agreement will appear. If you are proficient in legalese, you can read the wall of text. Then, click &lt;code>I accept the agreement&lt;/code> and click &lt;code>Next&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Next&lt;/code> again; it is best to leave the default install path alone for reproducibility in this experiment.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Next&lt;/code> again to create a Start Menu folder. Ensure that &lt;code>Add to PATH&lt;/code> is checked. &lt;code>Create a desktop icon&lt;/code> can be checked for convenience; it is up to the reader&amp;rsquo;s choice. Then click &lt;code>Next&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click Install and watch the green progress bar go at the speed of light. Once completed, click Finish. VSCode will open as long as everything went smoothly.&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Report: Project: AI in Orthodontics</title><link>/report/su21-reu-363/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-363/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-363/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-363/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-363/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-363/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Whitney McNair, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-363">su21-reu-363&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-363/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>In this effort we are analzying X-ray images in AI and identify cavitites&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data Sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-example-of-a-ai-algorighm-in-orthodontics">5. Example of a AI algorighm in Orthodontics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ai, orthodontics, x-rays.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Find literature about AI and Orthodontics x-rays.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Analyze the literature and explain how ai or x-rays are beneficial.&lt;/li>
&lt;/ul>
&lt;h2 id="2-data-sets">2. Data Sets&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Finding data sets in Orthodontics.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Can any of the data sets be used in AI?&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What are the challenges with Orthodontics data set? Privacy, HIPPA, Size, Avalibility&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/li>
&lt;/ul>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Place a cool image into projects images in my directory&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Correct the following link, replace the fa number with my su number and thne chart of png.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> If the image has been copied, you must use a reference such as shown in the Figure 1 caption.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="5-example-of-a-ai-algorighm-in-orthodontics">5. Example of a AI algorighm in Orthodontics&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete data sets that will be used.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete algorighm to analyze the data sets.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Write the program.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Verify that it works.&lt;/li>
&lt;/ul>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Gregor guided me throughout this process.&lt;/li>
&lt;/ul>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/li>
&lt;/ul>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: AI Time Series Analysis</title><link>/report/su21-reu-374/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-374/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-374/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-374/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> syntax for refences wron, see original example&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> never ever use the word I or my in your report&lt;/li>
&lt;/ul>
&lt;p>Zyion Morris, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-374">su21-reu-374&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-374/edit/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-benchmark">4. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgments">6. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> AI, ML, DL, Cybersecurity, Cyber Attacks, IoT.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>-[ ] do not indnt&lt;/p>
&lt;p>&lt;del>My&lt;/del> first research on AI Time analysis includes query on stock market data stucture.
The mean (level), maximum, minimum values; Time Series Data Components - trend, seasonality, noise or randomness, a curve, and the level.
Blockchain levies to be the coin flip of how Artificial Intellignece predicts future prices through machine learning.
numpy, matplotlib, pandas, cloudmesh-common, cloudmesh-cmd5s, are codes existing that demonstrate the use of blockchain with AI using Python.
What is the code in this project that will be developed?
What is the performance of this code?
What is my conclusion?
Never, ever use the words ‘I’ or ‘my’ in a report&lt;/p>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;p>This report pertains a .CSV file of a dataset predicting the consumption of electricity in the coming future. &lt;a href="https://github.com/cybertraining-dsc/su21-reu-374/files/6743411/archive.zip">archive.zip&lt;/a>&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Place a cool image into projects images in my directory&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Correct the following link, replace the fa number with my su number and thne chart of png.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> If the image has been copied, you must use a reference such as shown in the Figure 1 caption.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-374/raw/main/project/images/image.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them [^1].&lt;/p>
&lt;h2 id="4-benchmark">4. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common [^2]&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="6-acknowledgments">6. Acknowledgments&lt;/h2>
&lt;p>Needs full sentence&lt;/p>
&lt;ul>
&lt;li>Gregor von Laszewski&lt;/li>
&lt;li>Yohn Jairo Bautista&lt;/li>
&lt;li>Carlos Theran&lt;/li>
&lt;/ul>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;p>-[ ] refernce syntax wrong see original example&lt;/p>
&lt;p>&amp;lt;Marco Iansiti and Karim R. Lakhani, The Truth About Blockchain, [Online resource] &lt;a href="https://hbr.org/2017/01/the-truth-about-blockchain%3E">https://hbr.org/2017/01/the-truth-about-blockchain&amp;gt;&lt;/a>&lt;/p>
&lt;p>&amp;lt;Jeremy Swinfen Green, Understanding cryptocurrency market fluctuations, [Online resource] &lt;a href="https://www.telegraph.co.uk/business/business-reporter/cryptocurrency-market-fluctuations/%3E">https://www.telegraph.co.uk/business/business-reporter/cryptocurrency-market-fluctuations/&amp;gt;&lt;/a>&lt;/p>
&lt;p>&amp;lt;Mehmet Tarik Akcay, Historical Data for Top 20 Coins by Market Cap, [Online resource] &lt;a href="https://www.kaggle.com/mtakcy/historical-data-for-top-20-coins-by-market-cap?select=eos.csv%3E">https://www.kaggle.com/mtakcy/historical-data-for-top-20-coins-by-market-cap?select=eos.csv&amp;gt;&lt;/a>&lt;/p></description></item><item><title>Report: Aquatic Toxicity Analysis with the aid of Autonomous Surface Vehicle (ASV)</title><link>/report/fa20-523-312/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-312/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Saptarshi Sinha, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/">fa20-523-312&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>With the passage of time, human activities
have created and contributed much to the aggrandizing problems of various forms of environmental pollution. Massive amounts of industrial effluents and agricultural waste wash-offs, that often comprise pesticides and other forms of agricultural chemicals, find their way to fresh water bodies, to lakes, and eventually to the oceanic systems. Such events start producing a gradual increase in the toxicity levels of marine ecosystems thereby perturbing the natural balance of such water-bodies. In this endeavor, an attempt will be made to analyze the various water quality metrics (viz. temperature, pH, dissolved-oxygen level, and conductivity) that are measured with the help of autonomous surface vehicles (ASV). The collected data will undergo big data analysis tasks so as to find the general trend of values for the water quality of the given region. These obtained values will then be compared with sample water quality values obtained from neighboring sources of water for ascertaining if these sample values exhibit aberration from the established values that were found earlier from the big data analysis tasks for water-quality standards. In the event, the sample data popints significantly deviate from the standard values established earlier, it can then be successfully concluded that the aquatic system in question, from which the water sample was sourced from, has been degraded and may no longer be utilized for any form of human usage, such as being used for drinking water purposes.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-hardware-component">4.1 Hardware Component&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-software-component">4.2 Software Component&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#421-data-pre-processing">4.2.1 Data Pre-processing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#422-attributes-of-the-preliminary-data">4.2.2 Attributes of the preliminary data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#423-unsupervised-learning-k-means-clustering-analysis-safe--unsafe-centroid-calculation">4.2.3 Unsupervised learning: K-means clustering analysis (&amp;ldquo;Safe&amp;rdquo; &amp;amp; &amp;ldquo;Unsafe&amp;rdquo; Centroid calculation)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#424-display-of-results--analysis-of-any-given-sample-values-set">4.2.4 Display of results &amp;amp; analysis of any given sample values set&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-inference">5. Inference&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-analysis-of-extracted-data-and-statistical-information">5.1 Analysis of extracted data and statistical information&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-centroids--predicted-classesclusters">5.2 Centroids &amp;amp; Predicted Classes/Clusters&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-heatmaps-for-the-years---2017-2018-2019-and-2020">5.3 Heatmaps for the years - 2017, 2018, 2019, and 2020&lt;/a>&lt;/li>
&lt;li>&lt;a href="#54-analysis-of-sample-set-of-values">5.4 Analysis of sample set of values&lt;/a>&lt;/li>
&lt;li>&lt;a href="#55-benchmark-information">5.5 Benchmark information&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> toxicology, pollution, autonomous systems, surface vehicle, sensors, arduino, water quality, data analysis, environment, big data, ecosystem&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>When it comes to revolutionizing our qualities of life and improving standards, there is not another branch of science and technology that has made more impact than the myriad technological capabilities offered by the areas of Artificial Intelligence (AI) and its sub-fields involving Computer Vision, Robotics, Machine Learning, Deep Learning, Reinforcement Learning, etc. It should be borne in mind that AI was developed to allow machines/computer processors to work in the same way as the human brain works and which could make intelligent decisions at every conscious level. It was meant to help with tasks for rendering scientific applications more smarter and efficient. There are many tasks that can be performed in a far more dexterous fashion by employing smart-machines and algorithms than by involving human beings. But even more importantly, AI has also been designed to perform tasks that cannot be successfully completed by employing human beings. This could either be due to the prolonged boredom of the task itself, or a task that involves hazardous environments that cannot sustain life-forms for a long time. Some examples in this regard would involve exploring deep mines or volcanic trenches for mineral deposits, exploring the vast expanse of the universe and heavenly bodies, etc. And this is where the concept employing AI/Robotics based technology fits in perfectly for aquatic monitoring and oceanographical surveillance based applications.&lt;/p>
&lt;p>Toxicity analysis of ecologically vulnerable water-bodies, or any other marine ecosystem for that matter, could give us a treasure trove of information regarding biodiversity, mineral deposits, unknown biophysical phenomenon, but most importantly, it could also provide meaningful and scientific information related to the degradation of the ecosystem itself. In this research endeavor, an attempt will be made to utilize aquatic Autonomous Surface Vehicle (ASV) that will be deployed in marine ecosystems and which can continue collecting data for a prolonged period of time. Such vehicles are typically embedded with different kinds of electronic sensors, that are capable of measuring physical quantities such as temperature, pH, specific conductance, dissolved oxygen level, etc. The data collected by such a system can either be over a period of time (temporal data), or it could cover a vast aquatic geographical region (spatial data). This is the procedure by which environmental organizations record and store massive amounts of data that can be analyzed to obtain useful information about the particular water body in question. Such analytical work will provide us with statistical results that can then be compared with existing sample values so as to decipher whether the water source, from where the sample was obtained, manifests normal trend of values or shows large deviations from established trends that can signify an anomaly or biodegradation of the ecosystem. The datasets used in this endeavor are provided publicly by environmental organizations in the United States, such as the US Geological Survey (USGS). While the primary goal involves conducting big data analysis tasks for the databases so as to obtain useful statistical results, a secondary goal in this project involves finding out the extent to which a particular sample of water, obtained from a specific source of water, deviates from the normal trend of values. The extent of such deviations can then give us an indication about the status of the aquatic degradation of the ecosystem in question. The data analysis framework will be made as robust as possible and in this effort, we will work with data values that are spread over multiple years and not just focused on a single year.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>After reviewing the necessary background literature and previous work that has been done in this field, it can be stated that most of such endeavors focused majorly on environmental data collection with the help of sensors attached to a navigational buoy in a particular location of a water-body. Such works did not involve any significant data analysis framework and focused on particular niche areas. For instance, a particular research effort involved deploying a surface vehicle that collected data from large swaths of geographical areas in various water bodies but concentrated primarily on different algorithms employed for vehicular navigation and their relative success rates &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Other research attempts focussed on even more niche areas such as study of the migration pattern exhibited by zooplanktons upon natural and aritifical irradiance &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>, and detection and monitoring of marine fauna &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Although these are interesting projects and can provide us with novel information about various aspects of biological and aquatic research, such research attempts neither focused much on the data analysis portion for multiple sensory inputs (viz. temperature, pH, specific conductance, and dissolved oxygen level, which are the four most important water quality parameters) nor did they involve an intricate procedure to compare the data with sample observations so as to arrive at a suitable conclusion regarding the extent of environmental degradation of a particular water body.&lt;/p>
&lt;p>As mentioned in the previous section, this research endeavor will exhaustively focus not just on the working principles and deployment of surface vehicles to collect data, but it will also involve employing deeper study towards the subject of big-data analysis of both the current data of the system in question and the past data obtained for the same aquatic profile. In this way, it would be possible to learn more about the toxicological aspects of the ecosystem in question and which can then be also applied to neighboring regions.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>Upon exploring a wide array of available datasets, the following two data repositories were given consideration to get the required water quality based data over a particular period of time and for a particular geographical region:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://waterdata.usgs.gov/nwis/qw">USGS Water Quality Data&lt;/a> &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/li>
&lt;li>&lt;a href="https://www.epa.gov/waterdata/water-quality-data-download">EPA Water Quality Data&lt;/a> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>After going through the sample data values than can be visualized from the respective websites of USGS and EPA, the USGS datasets were chosen over the EPA datasets. This is mainly because the USGS datasets are more commensurate with the research goal of this endeavor, especially since it contains a huge array of databases that focuses on the four most important water quality data which are - temperature, pH, specific conductance, and dissolved oxygen level. Some previous work was conducted on similar USGS datasets by a particular research team &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. However, such work was drastically different in nature, when compared with this research attempt, since its emphasis was on a very broad perspective so as to create an overview of how to use and visualize the data from the USGS water quality portal. Besides, such work emphasizes on characterizing the seasonal variation of lake water clarity in different regions throughout the continental US, something that is very deviant from what would be addressed in this particular article which majorly involves studying environmental degradation and aquatic toxicology from the context of big data analytical tasks.&lt;/p>
&lt;p>To address the questions involving existence of multiple data-sets and motivation of using multiple data-sets, we must keep in mind that the very nature of this study is based on historical trends of the nature of water-quality in a particular region from the past and to this effect, emphasis has been given to use data values from the past years as well in addition to the current year. The geographical location for this analysis was chosen to be the East Fork Whitewater River that is located at Richmond, IN (USA). The years that have been chosen in this case are 2017, 2018, 2019, and 2020. For all these years, focus would be placed on the same time-period, that spans from November 1 to November 14, for all these years so as to establish consistency and high fidelity across the borad. Having multiple data-sets in this way will help us in achieving robust data-analytical results. It would also ensure that too much focus is not given on outlier cases, that may be relevant to just a particular time and day on a given year, or an aberration in the data that may only have surfaced due to an unknown underlying phenomenon or some form of cataclysmic event from the past. Using multiple datasets would help to get a resultant data structure that is more likely to converge towards an approximate level of historical thresholds and which can then be used to find out how a current sample data-point deviates from such established trends of previous patterns.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;h3 id="41-hardware-component">4.1 Hardware Component&lt;/h3>
&lt;p>Although this project focuses more on the data analysis portion than mechanical details, some information relating to the design and working principle of ASVs is provided herewith.The rough outline of an autonomous surface vehicle (ASV) in question has been perceived in Autodesk Fusion 360, which is a software package that helps creating and printing three-dimensional custom designs. A preliminary model has been designed in this software and after printing, it can be interfaced with the appropriate sensors in question. The system can be driven by an Arduino-Uno based microcontroller or even a Raspberry Pi based microprocessor, and it can comprise different types of environmental sensors that helps with collecting and offloading data to remote servers/machines. Some of these sensors can be purchased commercially from the vendor, &amp;ldquo;Atlas Scientific&amp;rdquo; &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. For instance, the following sensors can be used with an ASV to measure the four most important water quality parameters involving temperature, pH, dissolved oxygen level, and specific conductance values:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://atlas-scientific.com/kits/pt-1000-temperature-kit/">PT-1000 Temperature sensor kit&lt;/a> &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/li>
&lt;li>&lt;a href="https://atlas-scientific.com/kits/ph-kit/">Potential of Hydrogen (pH) sensor kit&lt;/a> &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/li>
&lt;li>&lt;a href="https://atlas-scientific.com/kits/dissolved-oxygen-kit/">Dissolved Oxygen (DO) sensor kit&lt;/a> &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/li>
&lt;li>&lt;a href="https://atlas-scientific.com/kits/conductivity-k-1-0-kit/">Conductivity K 1.0 sensor kit&lt;/a> &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/li>
&lt;/ul>
&lt;p>A very rudimentary framework of such a system has been realized in the Autodesk Fusion 360 software architecture as shown below. A two-hull framework is usually more helpful than a single hull based design since the former would help with stability issues especially while navigating through choppy waters. Figure 1 shows the design in the form of a very simplistic platform but which definitely lays down the foundation for a more complex structure for an ASV system.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/asvdesign.png" alt="ASV from Fusion 360">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Nascent framework of an ASV system in Fusion 360&lt;/p>
&lt;p>With the chassis framework out of the way, a careful analysis could be conducted towards the other successful components of such a vehicle so as to complete the entire build process for a fully functional prototype ASV. In essence, an ASV can be thought of being composed of certain key sub-elements. From a broad perspective, they comprise the hardware makeup, a suitable propulsion system, a sensing system, a communication system, and an appropriate source of onboard power source. The hardware makeup being out of the way, the other aspects can now be elaborated as follows:&lt;/p>
&lt;p>&lt;strong>Propulsion System:&lt;/strong> Primarily, the two major possibilities for propulsion systems in an ASV involve using either a single servo motor with an assortment of rudders and propellers for appropriate steering, or using two separate servo motors, one of which will drive the left-hand side of the system and the other would drive the right-hand side. The second arrangement is preferred in many scenarios as it provides with better maneuverability and control of the system as a whole. For instance, to move forward in a rectilinear fashion, both the motors would be given the same level of power. Whereas for steering the system in a particular direction, one of the motors would be assigned a lower power level than the other, thereby enabling the system to curve inwards on the side which has the motor with a lower power level. Of course, there will always be perturbations and natural disturbances that will deter the system from making these correct path changes. For this reason, a Proportional-Integral-Derivative (PID) controlled response could be augmented with the locomotion algorithms.&lt;/p>
&lt;p>&lt;strong>Sensing System:&lt;/strong> An ASV can have as many sensors as possible (dependent upon physical and electrical constraints of microcontroller/microprocessor) but for a study like this, an arrangement involving four different sensors needs to be integrated in the ASV which measures the four principle water quality parameters. This way, when the entire ASV system is deployed in an aquatic environment, it will be able to simultaneously provide readings for all four water-quality parameters in this case. Precisely, these water-quality parameters would be temperature, potential of hydrogen (pH), dissolved oxygen level, and specific conductance value. It should be noted in this perspective that it is possible to include even more sensors in this ASV system. However, the reason why it is generally not helpful to go beyond a certain number of sensors is primarily because of two reasons. Firstly, these parameters are important to most toxicological analysis studies &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, and the readings provided by such a sensory system could be considered as a foundation which could provide future directions (including adding more sensors, if needed). Secondly, we should also keep in mind that the hardware system has certain constraints. In this scenario, it involves a sensory shield (that can be integrated with a microcontroller or microprocessor) which can be used for incorporating multiple sensors. But it also has a maximum of four ports for four different sensors only. Though it is possible to add multiple layers of shield on top of the others (thereby raising the capability of integrating the number of sensors to eight or even more), it leads to unnecessary bandwidth issues, memory depletion possibilities, along with an increased demand of higher power supply. These issues will especially be more consequential if we are dealing with a microcontroller that has very limited memory and power, unlike a microprocessor. Hence, the decision to stick with only a certain number of sensors is really an important one.&lt;/p>
&lt;p>&lt;strong>Communication System:&lt;/strong> This is possibly the most important part of the ASV system as we need to device a technique to offload the data that is collected by the vehicle back to a remote computer/server that would likely be located at a considerable distance away from the ASV. There are different options that can be considered in this regard for establishing a proper communicative functionality between the ASV and the remote computer. Some options that are typically considered involve Bluetooth, IR signals, RF signals, GPS-based system, satellite communication, etc. There are both pros and cons when it comes to using any of these different communication systems for the ASV. However, the most important metric in this case involves the maximum range the communication system could span over. Obviously, some of the options (viz. Bluetooth) would not be possible in this regard as they have a very limited communication range. Some others (viz. satellite communication systems) have a very high range but are nevertheless not feasible for small-scale research endeavors as they require too much onboard processing power to even carry out their most basic operations. Hence, a balanced approach is normally followed in these scenarios and a GPS/RF-based system is often found to be a reliable candidate for carrying out the proposed tasks of an ASV.&lt;/p>
&lt;p>&lt;strong>Power Source:&lt;/strong> Finally, we certainly need an onboard processing power system that can provide the required amount of power to all the functional entities housed in the ASV system. The characteristic of such a desired power source would be that it would not require frequent charging and can sustain proper ASV operations for at least five to six hours. Additionally, the weight of the power source should also not be too clumsy that might put the stability of the entire ASV system in jeopardy. It must have a suitable weight, and should also come in an appropriate shape and size such that the weight of the entire power module is evenly distributed over a large area, thereby further reinforcing the stability of the system.&lt;/p>
&lt;h3 id="42-software-component">4.2 Software Component&lt;/h3>
&lt;p>With the help of the collected data from ASVs, the datasets of USGS are then prepared which meticulously tabulates all the readings from the different water sensors of the ASV. Such tabular data is made public for research and other educational purposes. In this endeavor, such datasets will be analyzed to decipher the median convergent values of the water body for the four different parameters that have been measured (i.e. temperature, pH, dissolved oxygen level, and specific conductance). The results of this data analysis task will manifest the water quality parametric values and standards for the particular aquatic ecosystem. Such a result will then be used to find out if a different water sample value sourced from a particular region deviates by a large proportion from the established standards which was obtained after analyzing the historical data from USGS for a regional source of water. The USGS website makes it easier to find data from a nearby geographical region by making it possible to enter the desired location prior to searching for water quality data in their huge databases. In this way, one can also use these databases to figure out if the water quality parameters of the particular ecological system varies wildly from a neighboring system that has almost the same geographical and ecological attributes.&lt;/p>
&lt;p>The establishment of the degree of variance of the sample data from the normal standards will be carried out by deciphering the number of water quality paramteric values that are aberrant in nature. For instance, a sample value with only an aberrant pH value could be classified as &amp;ldquo;Critical Category 1&amp;rdquo; whereas, a sample value with aberrant values for pH, temperature, and specific conductance would be classified as &amp;ldquo;Critical Category 3&amp;rdquo;. The aberrant nature of a particular parameter is postulated by enumerating how far the values are away from the established median data, which was obtained from the past/historical datasets. This will involve centroid-based calculations for the k-means algorithm (discussed in the next section). Such aberrant nature of a particular water quality paramteric value can also be figured out by using the context of standard deviations and quartile ranges. For instance, if the current data resides in the second quartile, it can be demarcated as being more or less consistent with previously established values. However, if it resides in the first or third quartile then it might be that the particular ecosystem has aberrant aspects, which would then need to be investigated for possible effects of outside pollutants (viz. industrial effluents, agricultural wash-off, etc.), or presence of harmful invasive species that might be altering the delicate natural balance of the ecosystem in question.&lt;/p>
&lt;p>The software logic is located at this &lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/code/toxicologyASV.ipynb">link&lt;/a> &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. It has been created using the aid of Google Colaboratory platform, or simply Colab &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. The code has been appropriately commented and documented in such a way that it can be easily reproduced. Important instructions and vital information about different aspects of the code have been properly written down wherever necessary. The coding framework helps in corroborating the inferences and conclusions made by this research attempt, and which are described in detail in the subsequent sections. The major steps that have been followed in establishing the software logic for this big-data analytical task are discussed below.&lt;/p>
&lt;h4 id="421-data-pre-processing">4.2.1 Data Pre-processing&lt;/h4>
&lt;p>&lt;strong>Meta-data&lt;/strong>: As with any other instance of big data, the data obtained from the USGS website is rife with many unnecessary information. Most of these information relate to meta-data for the particular database and it also contains detailed logistical information such as location information, units used for measurement, contact information, etc. Fortunately, all these information have been bunched up nicely at the beginning of the database and they were conveniently filtered out by skipping the first thirty-four (34) rows while reading the corresponding comma separated value (csv) files.&lt;/p>
&lt;p>&lt;strong>Extraction of required water-quality parameters (Temperature, Specific Conductance, pH, Dissolved Oxygen)&lt;/strong>: After filtering out the meta-data, it is very essential to focus only on the relevant portion of the database which contains the required information that is needed for the data analysis tasks. In this case, if we observe carefully, we will notice that not all the columns contain the required information relating to water-quality parameters. Some of them include information such as date, time, system&amp;rsquo;s unit, etc. Since these will not be required for our analysis task, we extract only those columns that contain information relating to the four primary water-quality parameters. Figure 2 displays the layout of the USGS dataset files containing all the extraneous information that are deemed unnecessary for the big data analysis task.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/sampledatabase.png" alt="database sample">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Sample Database file obtained from the USGS water-quality database for the year 2017&lt;/p>
&lt;h4 id="422-attributes-of-the-preliminary-data">4.2.2 Attributes of the preliminary data&lt;/h4>
&lt;p>&lt;strong>Seasonal Consistency&lt;/strong>: The preliminary data, that was pre-processed and extracted, was plotted to visualize the basic results. The data pertains to a particular duration of time in a specific seasonal time of the year, more importantly that spans the first two weeks of November for all the four years. This has been done to maintain consistency of results across the platform and to create as much as a robust framework as possible that can have high fidelity.&lt;/p>
&lt;p>&lt;strong>Data-points as x-axis&lt;/strong>: Additionally, it should be kept in mind that the data has already been time-stamped rigorously. More precisely, the databases that are uploaded to the USGS website have data tabulated in them that are arranged in a chronological manner. As a result, having the x-axis refer to actual data-points means the same if we had the x-axis refer to time instead. These plotted results have been provided in the next section.&lt;/p>
&lt;p>&lt;strong>Visualization of trends&lt;/strong>: The preliminary plotting of the data helps us to visualize the overall trends of the variation of the four important water-quality parameters. This gives an approximate idea regarding what we should normally expect from the water-quality data, the approximate maximum and minimum range of values, and it further helps in detecting any kind of outlier situations that might arise either due to the presence of artifacts, or nuisance environmental variables.&lt;/p>
&lt;h4 id="423-unsupervised-learning-k-means-clustering-analysis-safe--unsafe-centroid-calculation">4.2.3 Unsupervised learning: K-means clustering analysis (&amp;ldquo;Safe&amp;rdquo; &amp;amp; &amp;ldquo;Unsafe&amp;rdquo; Centroid calculation)&lt;/h4>
&lt;p>&lt;strong>General Clustering Analysis&lt;/strong>: The concept behind any clustering based approach involves an unsupervised learning mechanism. This means that the dataset traditionally does not come labelled and the task in hand is to find patterns within the data-set based on suitable metrics. These patterns help delineate the different clusters and classify the data by assigning them to one of these clusters. This process is usually carried out by measuring the euclidean distance of each point from the &amp;ldquo;centroids&amp;rdquo; of every cluster. The resultant clusters are created in such a way that the distance within the points in a particular cluster are minimized as much as possible whereas, the corresponding distance between points from different clusters are maximized. Figure 3 summarizes this concept of clustering technique.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/clusteranalysis.png" alt="clustering concept">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Concept of Clustering analysis adopted as an unsupervised learning process &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>&lt;/p>
&lt;p>&lt;strong>K-means Classification: Centroid Calculation&lt;/strong>: The algorithm for this step first starts with selecting a random centroid value to start with to begin the iteration process. In order to be rigorous in this regard, the initial points for centroid values were chosen to be one standard deviation away from the median values for the four water-quality parameters. More importantly, two initial centroid values were chosen that would result in two clusters, one belonging to the safe water-standard cluster and the other belonging to the unsafe category. For the safe centroid value, the point chosen was such that it was one standard deviation lower than the median values for the temperature and specific conductance parameters, whereas it was one standard deviation higher than the median values for the pH and disolved oxygen level parameters. This logic was reversed in case of the unsafe centroid starting value. The intuition behind this approach comes from the fact that a lower temperature and specific conductance value means lower degree of exothermic reactions and lower amount dissolved salts which are typically the traits of unpolluted water sources, and which also have higher pH value (that is, less acidic) and higher level of dissolved oxygen. Hence, the initial centroid value for the &amp;ldquo;safe&amp;rdquo; category was chosen in this way. For the unsafe cateory, the metrics were simply reversed.&lt;/p>
&lt;p>&lt;strong>Iteration process&lt;/strong>: The next steps for the centroid calculation involves creating an iteration process which will keep updating and perfecting the centroid values upon every execution of the iteration. In this case, the condition for ending the iteration involved either exceeding fifty iterations or reaching the ideal situation where the new centroid value equals the previous centroid value. In the later case, it can be mentioned that the centroid calculation has converged to a specific ideal value. Fortunately, in the case of this project, it was possible to arrive at this convergence of centroid values with not too many iteration steps. The details of this process has been explained in the coding framework with appropriate comments.&lt;/p>
&lt;p>&lt;strong>Assigning of Clusters&lt;/strong>: At the end of the iteration step, we end up with the final centroid values for the safe and unsafe category. With the help of these values, we calculate the euclidean distance for every points and assign them to appropriate clusters to which they are closest to. In this way, we complete the unsupervised algorithm of clustering process for any unlabelled data that is provided to us. In this particular endeavor, we assign the value &amp;ldquo;0&amp;rdquo; for a predicted cluster indicating a safe set of water-quality values for a given point, and a value of &amp;ldquo;1&amp;rdquo; for an unsafe set of water-quality values for a given point. Figure 4 summarizes this idea behind the K-means clustering method that chiefly works as an unsupervised learning algorithm.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/kmeansclustering.png" alt="k-means idea">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Concept of Clustering analysis adopted as an unsupervised learning process &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>&lt;/p>
&lt;h4 id="424-display-of-results--analysis-of-any-given-sample-values-set">4.2.4 Display of results &amp;amp; analysis of any given sample values set&lt;/h4>
&lt;p>In the final step, we display all the results and relevant plots for this big-data analysis task. Additionally, based on the labelled data and predicted classes for safe and unsafe water-quality standards, it will now be possible to find whether an arbitrary set of data values, that represent water-quality data for a particular region, would be classified as safe or unsafe as per this technique. This has also been shown in the results section.&lt;/p>
&lt;h2 id="5-inference">5. Inference&lt;/h2>
&lt;h3 id="51-analysis-of-extracted-data-and-statistical-information">5.1 Analysis of extracted data and statistical information&lt;/h3>
&lt;p>The first preliminary set of results were analyzed to get a general idea of how the water quality paramters vary for the system in question. For the purposes of data visualization, the processed data (viewed as a data-frame in python) was first analyzed to understand the four primary water-quality parameters that are being worked upon. The data-frames for the big data sets are shown below, along with the corresponding statistical information that were evalauted for such attributes. Figure 5 shows the preliminary results that were obtained for the data visualization and statistical processing tasks.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/prelimresults.png" alt="Preliminary results">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Displaying results of data visualization and statistical information for the water-quality parameters&lt;/p>
&lt;p>In the above set of results, it should be worthwhile to note that temperature is measured in the celsius scale, specific conductance is measured in microsiemens per centimeter at 25 degree celsius, pH is measured in the usual standard range (between 0-14), and the level of dissolved oxygen is measured in milligrams per liter.&lt;/p>
&lt;p>Next, the content of the dataset, after it is processed in the software architecture, is plotted. It displays the alteration of the values (expressed in scatter plots) of the four main water-quality parameters (viz. Temperature, Specific Conductance, pH, and Dissolved Oxygen) over the period of time that starts from November 1 to November 14 for the four years involving 2017, 2018, 2019, and 2020.&lt;/p>
&lt;p>Figure 6 displays the scatter plot data for the &amp;ldquo;temperature&amp;rdquo; attribute in the year 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/seventeentemp.png" alt="Temperature 2017">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Temperature&amp;rdquo; (2017)&lt;/p>
&lt;p>Figure 7 displays the scatter plot data for the &amp;ldquo;specific conductance&amp;rdquo; attribute in the year 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/seventeencond.png" alt="Conductance 2017">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Specific Conductance&amp;rdquo; (2017)&lt;/p>
&lt;p>Figure 8 displays the scatter plot data for the &amp;ldquo;pH&amp;rdquo; attribute in the year 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/seventeenph.png" alt="pH 2017">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;pH&amp;rdquo; (2017)&lt;/p>
&lt;p>Figure 9 displays the scatter plot data for the &amp;ldquo;dissolved oxygen&amp;rdquo; attribute in the year 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/seventeendox.png" alt="Dissolved Oxygen 2017">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Dissolved Oxygen&amp;rdquo; (2017)&lt;/p>
&lt;p>Figure 10 displays the scatter plot data for the &amp;ldquo;temperature&amp;rdquo; attribute in the year 2018.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/eighteentemp.png" alt="Temperature 2018">&lt;/p>
&lt;p>&lt;strong>Figure 10:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Temperature&amp;rdquo; (2018)&lt;/p>
&lt;p>Figure 11 displays the scatter plot data for the &amp;ldquo;specific conductance&amp;rdquo; attribute in the year 2018.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/eighteencond.png" alt="Conductance 2018">&lt;/p>
&lt;p>&lt;strong>Figure 11:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Specific Conductance&amp;rdquo; (2018)&lt;/p>
&lt;p>Figure 12 displays the scatter plot data for the &amp;ldquo;pH&amp;rdquo; attribute in the year 2018.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/eighteenph.png" alt="pH 2018">&lt;/p>
&lt;p>&lt;strong>Figure 12:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;pH&amp;rdquo; (2018)&lt;/p>
&lt;p>Figure 13 displays the scatter plot data for the &amp;ldquo;dissolved oxygen&amp;rdquo; attribute in the year 2018.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/eighteendox.png" alt="Dissolved Oxygen 2018">&lt;/p>
&lt;p>&lt;strong>Figure 13:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Dissolved Oxygen&amp;rdquo; (2018)&lt;/p>
&lt;p>Figure 14 displays the scatter plot data for the &amp;ldquo;temperature&amp;rdquo; attribute in the year 2019.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/nineteentemp.png" alt="Temperature 2019">&lt;/p>
&lt;p>&lt;strong>Figure 14:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Temperature&amp;rdquo; (2019)&lt;/p>
&lt;p>Figure 15 displays the scatter plot data for the &amp;ldquo;specific conductance&amp;rdquo; attribute in the year 2019.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/nineteencond.png" alt="Conductance 2019">&lt;/p>
&lt;p>&lt;strong>Figure 15:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Specific Conductance&amp;rdquo; (2019)&lt;/p>
&lt;p>Figure 16 displays the scatter plot data for the &amp;ldquo;pH&amp;rdquo; attribute in the year 2019.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/nineteenph.png" alt="pH 2019">&lt;/p>
&lt;p>&lt;strong>Figure 16:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;pH&amp;rdquo; (2019)&lt;/p>
&lt;p>Figure 17 displays the scatter plot data for the &amp;ldquo;dissolved oxygen&amp;rdquo; attribute in the year 2019.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/nineteendox.png" alt="Dissolved Oxygen 2019">&lt;/p>
&lt;p>&lt;strong>Figure 17:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Dissolved Oxygen&amp;rdquo; (2019)&lt;/p>
&lt;p>Figure 18 displays the scatter plot data for the &amp;ldquo;temperature&amp;rdquo; attribute in the year 2020.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/twentytemp.png" alt="Temperature 2020">&lt;/p>
&lt;p>&lt;strong>Figure 18:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Temperature&amp;rdquo; (2020)&lt;/p>
&lt;p>Figure 19 displays the scatter plot data for the &amp;ldquo;specific conductance&amp;rdquo; attribute in the year 2020.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/twentycond.png" alt="Conductance 2020">&lt;/p>
&lt;p>&lt;strong>Figure 19:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Specific Conductance&amp;rdquo; (2020)&lt;/p>
&lt;p>Figure 20 displays the scatter plot data for the &amp;ldquo;pH&amp;rdquo; attribute in the year 2020.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/twentyph.png" alt="pH 2020">&lt;/p>
&lt;p>&lt;strong>Figure 20:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;pH&amp;rdquo; (2020)&lt;/p>
&lt;p>Figure 21 displays the scatter plot data for the &amp;ldquo;dissolved oxygen&amp;rdquo; attribute in the year 2020.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/twentydox.png" alt="Dissolved Oxygen 2020">&lt;/p>
&lt;p>&lt;strong>Figure 21:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Dissolved Oxygen&amp;rdquo; (2020)&lt;/p>
&lt;h3 id="52-centroids--predicted-classesclusters">5.2 Centroids &amp;amp; Predicted Classes/Clusters&lt;/h3>
&lt;p>Figure 22 shows the final centroid values for the safe and unsafe water-quality standards for the year 2017. Furthermore, Figure 23 shows the predicted classes for the safe and unsafe clusters, which were calculated based on the results of the centroid values, for the same year of 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/centroidsseventeen.png" alt="centroids for 2017">&lt;/p>
&lt;p>&lt;strong>Figure 22:&lt;/strong> Safe and unsafe centroid values for the year 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/classpredictions.png" alt="clusters for 2017">&lt;/p>
&lt;p>&lt;strong>Figure 23:&lt;/strong> Predicted classes for safe (&amp;ldquo;0&amp;rdquo;) and unsafe (&amp;ldquo;1&amp;rdquo;) clusters for the year 2017.&lt;/p>
&lt;h3 id="53-heatmaps-for-the-years---2017-2018-2019-and-2020">5.3 Heatmaps for the years - 2017, 2018, 2019, and 2020&lt;/h3>
&lt;p>For the all the four years, heatmaps were plotted to get more information about the trend of the data. Chiefly, the heatmaps give us an empirical form of ideology relating to the degree of correlation between the different water-quality parameters in this data analysis task.&lt;/p>
&lt;p>Figure 24 visualizes the heat-map and shows the relationships between the various aquatic parameters for the year of 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/heatmapseventeen.png" alt="hmap2017">&lt;/p>
&lt;p>&lt;strong>Figure 24:&lt;/strong> Heatmap for the water-quality parameters (Year - 2017)&lt;/p>
&lt;p>Figure 25 visualizes the heat-map and shows the relationships between the various aquatic parameters for the year of 2018.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/heatmapeighteen.png" alt="hmap2018">&lt;/p>
&lt;p>&lt;strong>Figure 25:&lt;/strong> Heatmap for the water-quality parameters (Year - 2018)&lt;/p>
&lt;p>Figure 26 visualizes the heat-map and shows the relationships between the various aquatic parameters for the year of 2019.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/heatmapnineteen.png" alt="hmap2019">&lt;/p>
&lt;p>&lt;strong>Figure 26:&lt;/strong> Heatmap for the water-quality parameters (Year - 2019)&lt;/p>
&lt;p>Figure 27 visualizes the heat-map and shows the relationships between the various aquatic parameters for the year of 2020.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/heatmaptwenty.png" alt="hmap2020">&lt;/p>
&lt;p>&lt;strong>Figure 27:&lt;/strong> Heatmap for the water-quality parameters (Year - 2020)&lt;/p>
&lt;h3 id="54-analysis-of-sample-set-of-values">5.4 Analysis of sample set of values&lt;/h3>
&lt;p>In this portion, we test the unsupervised learning mechanism on actual sample sets of water-quality values. For this purpose, we feed the sample values to the coding framework and based on the centroids calculated for the past four years, it is able to identify whether the given water sample belong in the safe or unsafe category. Further, if it belongs in the unsafe category, the system can further inform us the degree of criticality of the water-quality degradation. This is carried out by evaluating how many water quality parametric values are beyond the normal range. Based on this analysis, a critical nature is then displayed as an output result. As an example, a critical category of &amp;ldquo;2&amp;rdquo; would signify two of the water-quality parameters were beyond the normal range of values, while the others were normal. Needless to say, higher is the critical category level, the more degraded the water source is.&lt;/p>
&lt;p>Figure 28 displays the results obtained for specific sample values of water quality parameters.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/sampleresults.png" alt="sample values test">&lt;/p>
&lt;p>&lt;strong>Figure 28:&lt;/strong> Analysis of water sample values by ascertaining the degree of degradation based on critical level&lt;/p>
&lt;h3 id="55-benchmark-information">5.5 Benchmark information&lt;/h3>
&lt;p>Finally, the benchmark analysis results are shown below for the respective tasks carried out by the coding platform. Some important facts that should be kept in mind in this regard are as follows:&lt;/p>
&lt;ul>
&lt;li>The benchmark analysis was carried out using the cloudmesh benchmark procedure in Python (executed in Google Colab).&lt;/li>
&lt;li>A sleep-time of &amp;ldquo;5&amp;rdquo; was selected as the standard for the benchmark analysis and it has been adopted consistently for all the other benchmark calculations.&lt;/li>
&lt;li>The time values for the different benchmark results were not rounded off in this case as it was both important and interesting to know the minute differences between the different kinds of tasks that were carried out in this regard. It should be noted that the final benchmark value is exceptionally high since this step involves the part where the human user inputs the sample values for ascertaining the safety standard for a water source.&lt;/li>
&lt;/ul>
&lt;p>Figure 29 shows the benchmark results that were obtained for specific sections in the coding framework.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/bmresults.png" alt="benchmark results">&lt;/p>
&lt;p>&lt;strong>Figure 29:&lt;/strong> Benchmark results for all the tasks that were carried out for this big data analysis task using Google Colab&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>This research endeavor implements a big data analysis framework for analyzing toxicity of aquatic systems. It is an attempt where hardware and software meet together to give reliable results, and on the basis of which we are able to design an elaborate mechanism that can analyze other sample water sources and provide decisions regarding its degradation. Although the endeavor carried out in this example might not involve a plethora of high-end applications or intricate logic frameworks, it still provides a very decent foundational approach for carrying out toxicological analysis for water sources. Most importantly, it should be noted that the results obtained for sample values correspond to what we would normally expect for polluted and non-polluted sources of water. For instance, it was found that water sources with high values of specific conductance were categorized in the &amp;ldquo;unsafe&amp;rdquo; category which is to be expected since a high conductance value typically signifies the presence of dissolved salts and ions in the water source, which normally indicates that effluents or agricultural run-off might have made its way to the water source. Additionally, it was also found out that water samples with high values of dissolved oxygen levels were categorized in the &amp;ldquo;safe&amp;rdquo; category which is certainly true based on biological postulates.&lt;/p>
&lt;p>Of course, a more diverse array of data coupled with more scientific and enhanced ASV systems would have probably provided us with even better results. But as indicated earlier, this research endeavor provides a pragmatic foundational approach to conducting this kind of big data analytical work. We can build up on the logic presented in this endeavor to come up with even more advanced and robust version of toxicological analysis tasks.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Geoffrey Fox, Dr. Gregor von Laszewski, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Valada A., Velagapudi P., Kannan B., Tomaszewski C., Kantor G., Scerri P. (2014) Development of a Low Cost Multi-Robot Autonomous Marine Surface Platform. In: Yoshida K., Tadokoro S. (eds) Field and Service Robotics. Springer Tracts in Advanced Robotics, vol 92. Springer, Berlin, Heidelberg. &lt;a href="https://doi.org/10.1007/978-3-642-40686-7_43">https://doi.org/10.1007/978-3-642-40686-7_43&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>M. Ludvigsen, J. Berge, M. Geoffroy, J. H. Cohen, P. R. De La Torre, S. M. Nornes, H. Singh, A. J. Sørensen, M. Daase, G. Johnsen, Use of an Autonomous Surface Vehicle reveals small-scale diel vertical migrations of zooplankton and susceptibility to light pollution under low solar irradiance. Sci. Adv. 4, eaap9887 (2018). &lt;a href="https://advances.sciencemag.org/content/4/1/eaap9887/tab-pdf">https://advances.sciencemag.org/content/4/1/eaap9887/tab-pdf&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Verfuss U., et al., (2019, March). A review of unmanned vehicles for the detection and monitoring of marine fauna. Marine Pollution Bulletin, Volume 140, Pages 17-29. Retrieved from &lt;a href="https://doi.org/10.1016/j.marpolbul.2019.01.009">https://doi.org/10.1016/j.marpolbul.2019.01.009&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>USGS Water Quality Data, Accessed: Nov. 2020, &lt;a href="https://waterdata.usgs.gov/nwis/qw">https://waterdata.usgs.gov/nwis/qw&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>EPA Water Quality Data, Accessed Nov. 2020, &lt;a href="https://www.epa.gov/waterdata/water-quality-data-download">https://www.epa.gov/waterdata/water-quality-data-download&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Read, E. K., Carr, L., De Cicco, L., Dugan, H. A., Hanson, P. C., Hart, J. A., Kreft, J., Read, J. S., and Winslow, L. A. (2017), Water quality data for national‐scale aquatic research: The Water Quality Portal, Water Resour. Res., 53, 1735– 1745, doi:10.1002/2016WR019993. &lt;a href="https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2016WR019993">https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2016WR019993&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Atlas Scientific team. Atlas Scientific Environmental Robotics. Retrieved from &lt;a href="https://atlas-scientific.com/">https://atlas-scientific.com/&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Sinha, Saptarshi. (2020) A Big-data analysis framework for Toxicological Study. Retrieved from &lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/code/toxicologyASV.ipynb">https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/code/toxicologyASV.ipynb&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Google Colaboratory team. Google Colaboratory. Retrieved from &lt;a href="https://colab.research.google.com/">https://colab.research.google.com/&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Tan, Steinback, Kumar (2004, April). Introduction to Data Mining, Lecture Notes for Chapter 8, Page 2. Accessed: Nov. 2020, &lt;a href="https://www-users.cs.umn.edu/~kumar001/dmbook/dmslides/chap8_basic_cluster_analysis.pdf">https://www-users.cs.umn.edu/~kumar001/dmbook/dmslides/chap8_basic_cluster_analysis.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Alan J. (2019, November). K-means: A Complete Introduction. Retrieved from &lt;a href="https://towardsdatascience.com/k-means-a-complete-introduction-1702af9cd8c">https://towardsdatascience.com/k-means-a-complete-introduction-1702af9cd8c&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: How Big Data has Affected Statistics in Baseball</title><link>/report/fa20-523-328/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-328/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-328/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-328/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-328/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-328/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: report&lt;/p>
&lt;p>Holden Hunt, &lt;a href="mailto:holdhunt@iu.edu">holdhunt@iu.edu&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-328/">fa20-523-328&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-328/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The purpose of this report is to highlight how the inception of big data in baseball has changed the way baseball is played and how it affects the choices managers make before, during, and after a game. It was found that big data analytics can allow baseball teams to make more sound and intelligent decisions when making calls during games and signing contracts with free agent and rookie players. The significance of this project and what was found was that teams that adopt the &lt;em>moneyball mentality&lt;/em> would be able to perform at much higher levels than before with a much lower budget than other teams. The main conclusion from the report was that the use of data analytics in baseball is a fairly new idea, but if implemented on a larger scale than only a couple of teams, it could greatly change the way baseball is played from a managerial standpoint.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-dataset">2. Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-background">3. Background&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-big-data-in-baseball">4. Big Data in Baseball&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgements">6. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> sports, data analysis, baseball, performance&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Whenever people talk about sports, they will always talk about some kind of statistic to show that their team is performing well, or certain player(s) are playing incredibly well. This is due to the fact that statistics has become extremely important in sports, especially in rating an entity’s performance. While essentially every sport has adopted statistics to quantify performance, baseball is the most well-known sport to use it, due to the obscene number of stats that are tracked for each player and team, as well as the sport that uses stats the most in how they play the game. The MLB publishes about 85 different statistics for individual players, including the stats tracked of the teams, there is likely to be about double the amount if tracked statistics for the sport of baseball. The way that all the statistics are calculated, is, of course, by analyzing big data found from the players and teams. This report will mainly talk about the history of big data and data analytics in baseball, what the data is tracking, what we can learn from the data, and how the data is used.&lt;/p>
&lt;h2 id="2-dataset">2. Dataset&lt;/h2>
&lt;p>The dataset that will be analyzed in this report will be the Lahmen Sabermetrics dataset. This dataset is a large dataset curated by Sean Lahmen, which contains baseball data starting from the year 1871, which is when the Major League Baseball association was founded &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The dataset contains data for many different types of statistics, including batting stats, fielding stats, pitching stats, awards stats, player salaries, and games they played in. The data for this dataset has statistics from the last season that occurred (2020 season), but the data that could be accessed for this report is from 1871-2015. I plan to use this dataset for discussing later how the data in sets like this is used for statistical analysis in baseball and how teams can use this to their advantage.&lt;/p>
&lt;h2 id="3-background">3. Background&lt;/h2>
&lt;p>The concept of baseball has been a sport that has existed for centuries, but the actual sport called baseball started in early to mid 1800s. Baseball became popularized in the United States in the 1850s, where a baseball craze hit the New York area, and baseball quickly was named a &lt;em>national pastime.&lt;/em> The first professional baseball team was the Cincinnati Red Stockings, which was established in 1869, and the first professional league was established in 1871, and was called the National Association of Professional Base Ball Players. This league only lasted a few years and was replaced by a more formally structured league called the National League in 1876, and the American League was established in 1901 from the Western League. A vast majority of the modern rules of baseball were in place by 1893, and the last major change was instituted in 1901 where foul balls are counted as strikes. The World Series was inaugurated in the fall of 1903, where the champion of the National League would play against the champion of the American League. During this time, there were many problems with the league, such as strikes due to poor and unequal pay and the discrimination of African Americans. The era in the time of the early 1900s was the first era of baseball, and the second era of baseball started in the 1920s where a plethora of changes to the game caused the sport to move from more of a &lt;em>pitcher’s game&lt;/em> to a &lt;em>hitter’s game,&lt;/em> which was emphasized by the success of the first &lt;em>power hitter&lt;/em> in professional baseball, Babe Ruth. In the 1960s, baseball was losing revenue due to the rising popularity of football, the league had to make changes to combat this drop in revenue and popularity. The changes lead to the salaries of the players getting increased and also an increase in attendance to games, which means increased revenue &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Big data is able to be used in baseball through the use of statistics, which has become a major part in how the sport is played. The use of statistics in baseball has become known as sabermetrics, which can be used by teams to make calls for the game based on numbers. The idea of using sabermetrics started from the book called &lt;em>Moneyball&lt;/em>. The book was published in 2003, and was about the Oakland Athletics baseball team and how they were able to use sabermetric analysis to compete on equal grounds with teams that had much more money and good players than the A&amp;rsquo;s team &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. This book has had a major impact on the way baseball is played today for several teams. For example, teams like the New York Yankees, the St. Louis Cardinals, and the Boston Red Sox have hired full-time sabermetric analysts in attempts to gain an edge over other teams by using these sabermetrics to influence their decisions. Also, the Tampa Bay Rays were able to make the &lt;em>moneyball&lt;/em> idea a reality by making it to the 2020 World Series with a much lower budget team and using lots of sabermetrics in their decision-making &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. This &lt;em>moneyball strategy&lt;/em> is not the perfect strategy however, because the Rays lost the World Series, and the turning point for their loss could be pointed to a decision they made based on what the analytics said they should do, which ended up being the wrong choice, which lost them a crucial game in the series.&lt;/p>
&lt;h2 id="4-big-data-in-baseball">4. Big Data in Baseball&lt;/h2>
&lt;p>The data that is being analyzed for the statistics in baseball are datasets similar to the one this report is looking into, the Lahmen Sabermetrics dataset. Since this dataset contains a vast amount of data for each player and many different tables containing many different kinds of data, many kinds of statistics are able to be tracked for each player. With this large amount of statistics, teams are able to look to numbers and analytics in order to make the best decision on the actions to make during a game. Teams can also use analytic technology to predict the performance of a player based on their previous accomplishments and comparing that to similar players and situations in the past &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This kind of analysis can be used to gauge the potential performance of a free-agent or rookie player, as well as deciding what player should be in the starting lineup for the upcoming game(s). Since these sabermetric analyses are able to predict the performance of a player in the coming years, they are able to tell if a contract made by a team is likely to not be a smart deal since they tend to make long-term deals for lots of money, even though it is likely that the player will not continue to perform at the same level they are currently at for the entirety of their contract. This is normally due to the inability to play at an incredibly high level consistently for a long period of time and regression of performance from age, which is shown to start occuring at around 32 years old &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This simply means that large contracts have a trend of being a large loss of money in the long run shown from analysis of similar types of contracts in the past. This kind of analysis also allows for a players ranking to be deciding by more areas than before. For example, a player&amp;rsquo;s offensive capabilities can be shown by looking at more categories than the amount of home runs hit and batting average, they can also look at baserunning skill, slugging percentage, and overall baseball intelligence. This ability of looking at a player&amp;rsquo;s overall capabilities in a more analytical manner allows teams to not throw all their budget into one or two top prospected players, but can spread their money across several talented players to have a good and balanced team. Another reason why deciding to not spend lots of money on a long contract for a top prospected player is that the analysis shows that players have started to have shorter lengths of time where they are able to perform at their best, even though other sports have seen the opposite in recent years . However, young players have been performing at a much higher level in recent years and they have had younger players moving from the minor to the major league much faster than before &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The data that is presented in the Lahman Sabermetrics database and other similar databases is able to allow analysts to compare data and statistics of one team with any/every other team with relative ease and in an easy to understand way. For example, the comparative analysis Figure 1 below shows that the payroll of teams and their winning percentage, analysts are able to learn that the New York Yankees have a much higher payroll than all other teams and they have a very good win rate, but there are other teams that do have very high payrolls and have the same good rate rate. Also, Figure 1 shows that there are other teams that have higher payrolls than average, but have a very bad win rate compared to all other teams, including teams that have a much lower payroll &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. This kind of analysis shows us that spending lots of money does not guarantee a strong season, which can strengthen the idea of the &lt;em>moneyball strategy&lt;/em> coined earlier where teams attempt to waste less money by spreading budgets across several players other than spending most of the budget on only one or two players.&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Comparative Analysis of Payroll to Win Percentage &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>
&lt;img src="https://github.com/cybertraining-dsc/fa20-523-328/raw/main/report/images/spending_graph.jpeg" alt="Comparative Analysis of Payroll to Win Percentage Graph">&lt;/p>
&lt;p>This is only one way that the Lahman Sabermetrics dataset can be used, but there are many more ways this data can be used to make league wide analyses and compare a certain team to others. This can be used by teams to possibly learn what they might be doing wrong if they feel as though they should be performing better.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>This report discusses the history of baseball and how big data analytics came to be prevalent in the sport, as well as how big data is used in baseball and what can be learned from the use of it so far. Big data is able to be used to make decisions that could greatly benefit a team from saving money on a contract with a player to making a choice during a game. Big data analytics use in baseball is a fairly new occurrence, but due to the advantages a team can gain from using analytics, it is likely that use of it will increase soon in the future.&lt;/p>
&lt;h2 id="6-acknowledgements">6. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the FA20-BL-ENGR-E534-11530: Big Data Applications course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Lahman, Sean. &amp;ldquo;Lahman&amp;rsquo;s Baseball Database - Dataset by Bgadoci.&amp;rdquo; Data.world, 5 Oct. 2016. &lt;a href="https://data.world/bgadoci/lahmans-baseball-database">https://data.world/bgadoci/lahmans-baseball-database&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Wikipedia. &amp;ldquo;History of Baseball.&amp;rdquo; Wikipedia, Wikimedia Foundation, 27 Oct. 2020. &lt;a href="https://en.wikipedia.org/wiki/History_of_baseball">https://en.wikipedia.org/wiki/History_of_baseball&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Wikipedia. &amp;ldquo;Moneyball.&amp;rdquo; Wikipedia, Wikimedia Foundation, 28 Oct. 2020. &lt;a href="https://en.wikipedia.org/wiki/Moneyball">https://en.wikipedia.org/wiki/Moneyball&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Wharton University of Pennsylvania. &amp;ldquo;Analytics in Baseball: How More Data Is Changing the Game.&amp;rdquo; Knowledge@Wharton, 21 Feb. 2019. &lt;a href="https://knowledge.wharton.upenn.edu/article/analytics-in-baseball/">https://knowledge.wharton.upenn.edu/article/analytics-in-baseball/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Tibau, Marcelo. &amp;ldquo;Exploratory data analysis and baseball.&amp;rdquo; Exploratory Data Analysis and Baseball, 3 Jan. 2017. &lt;a href="https://rstudio-pubs-static.s3.amazonaws.com/239462_de94dc54e71f45718aa3a03fc0bcd432.html">https://rstudio-pubs-static.s3.amazonaws.com/239462_de94dc54e71f45718aa3a03fc0bcd432.html&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Predictive Model For Pitches Thrown By Major League Baseball Pitchers</title><link>/report/fa20-523-343/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-343/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-343/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-343/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-343/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-343/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: report&lt;/p>
&lt;p>Bryce Wieczorek, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-343">fa20-523-343&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-343/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The topic of this review is how big data analysis is used in a predictive model for classifying what pitches are going to be thrown next. Baseball is a pitcher’s game, as they can control the tempo. Pitchers have to decide what type of pitch they want to throw to the batter based on how their statistics compare to that of the batters. They need to know what the batter struggles to hit against, and where in the strike zone they struggle the most.&lt;/p>
&lt;p>With the introduction of technology into sports, data scientists are sliding headfirst into Major League Baseball. And with the introduction of Statcast in 2015, The MLB has been looking at different ways to use technology in the game. In 2020 alone, the MLB introduce several different types of technologies to keep the fans engaged with the games while not being able to attend them [^3]. In this paper, we will be exploring a predictive model to determine pitches thrown by each pitcher in the MLB. We will be reviewing several predictive models to understand how this can be done with the use of big data.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-and-previous-work">2. Background and Previous Work&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-harvard-college-model">2.1 Harvard College Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-north-carolina-state-university-model">2.2 North Carolina State University Model&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-search-and-analysis">4. Search and Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-limitations">5. Limitations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8 References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Pitch type, release speed, release spin, baseball, pitchers, MLB&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big data is everywhere and has been used in sports for years. Especially in Major League Baseball, where big data has been analyzed for various things. For example, in the movie Moneyball (which is based on true events), the general manager of the Oakland A’s uses analytics to find players that will win them games that other teams overlook. They used many different statistics; batting average, batting average with runners on base, fielding percentage, etc., to find these players. Not only do teams use big data to find the right players for them, but the MLB also uses it for their Statcast technology. Statcast was implemented in 2015 and is an automated tool that analyzes data to deliver accurate statistics in real time &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This technology tracks many different aspects of the game, including many different pitching statistics.&lt;/p>
&lt;p>The pitching statistics that are recording is actually through the PITCHf/x system &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This system was implemented nine years before Statcast. The system is made up of three cameras that are located throughout the stadium that measure the speed, spin, and trajectory of the baseball as it is thrown. Statcast adopted this technology as the MLB was looking at ways to track more than just pitching statistics.&lt;/p>
&lt;p>And while there is a lot of data about pitching, we will only be focusing on release speed, release spin, pitch type, and the pitcher for our model. And while there is a lot of data about these different pitches and the pitchers, it can still be difficult to predict exactly what type of pitch is thrown according to the pitcher. For instance, the difference between different types of fastballs, a four-seam and two-seam, can be different by less than a mile per hour and forty rotations per minute (as thrown by Aaron Nola of the Philadelphia Phillies). In our model that we try to create, we are use these variables, along with the last three pitches thrown for each pitch type, to try to predict what pitch was just thrown.&lt;/p>
&lt;h2 id="2-background-and-previous-work">2. Background and Previous Work&lt;/h2>
&lt;p>Baseball players are always looking for a way to have an edge over their opponents. Whether that is through training, recovery, use of supplements, and by watching film or statistics to find what your opponent’s struggle with.&lt;/p>
&lt;p>There are several existing models that have been made to predict pitch type. We plan on examining them to determine how this can be done. We are looking for a general blueprint of which multiple models have used and/or have followed. This also allowed us to see what type of datasets would best be used for an analysis of this sort.&lt;/p>
&lt;h3 id="21-harvard-college-model">2.1 Harvard College Model&lt;/h3>
&lt;p>The first prediction model we studied was done through Harvard University &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. This prediction model was very complex and explored many different types of analysis. They first investigated doing a binary classification model. However, they ultimately decided against this because the label of pitches did not accurately represent what they actual were. This led them into multi-class predictive models in which they used. The other types of analysis were boosted trees, classification trees, random forests, linear discriminant analysis, and vector machines. The main point of this report was to see if they could replicate previous work done, but they ultimately failed at each one. This resulted to them in trying to determine if one can correctly predict pitch type selection. There research showed that machine learning cannot correctly predict pitch type due to the many different aspects that need to be analyzed. They hoped that their work could be used as a reference for future work done.&lt;/p>
&lt;p>We found this article to be very useful in our work since it stands as a reference for future work. We found this to be helpful in our review of predictive models for pitch types due to the different models they tried to replicate.&lt;/p>
&lt;h3 id="22-north-carolina-state-university-model">2.2 North Carolina State University Model&lt;/h3>
&lt;p>The prediction model created by North Carolina State University &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> was created to predict the next pitch that was going to be thrown. Their model compared old data to the current live data of a game, they are using many different types of data to predict the next type of pitch. They used a very large database that consisted of 287 MLB pitchers who had an average of 81 different pitch features (pitch type, ball rotation, speed, etc.). They are trying to determine if the next pitch will be a fastball or an off-speed pitch. Like the previously mentioned model, this model is also using trees to give a classification output. The parent node is the first type of pitch thrown, which then leads to if the pitch was a strike or a ball, then it uses this information and compares it to their dataset to predict the next set of nodes.&lt;/p>
&lt;p>We found this to be very useful for our review since their model worked correctly and it used current data from the game. With Statcast today, we find this to be very important since all of this information is recorded and logged as each pitch is thrown.&lt;/p>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>Major League Baseball first started using Statcast in 2015, after a successful trial run in 2014. Statcast &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> provided the MLB with a way to collect and analyze huge amounts of data in real time. Some of the data it collects, which is used in many different models, includes the pitcher’s name, pitch type, release spin, release speed, and the amount of times each pitch was thrown.&lt;/p>
&lt;p>Since we do not need all the data Statcast collects, we found a dataset with the datatypes listed above &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. We chose this dataset because it contains a significant amount of data that can be used for a prediction model. The dataset contains a lot of information about not only the pitchers, but about the batters. The database shows what each batter has done against every type of pitch they have faced, whether they hit the ball, fouled it, swung and missed, etc. We felt as if this data is very important as well since coaches and pitchers will want to know how the batter reacts to different pitches and the different locations of each pitch. This would be a vital part to any pitch prediction model as coaches signal to the pitchers what types of pitches to throw according the strengths and weaknesses of the batters.&lt;/p>
&lt;h2 id="4-search-and-analysis">4. Search and Analysis&lt;/h2>
&lt;p>After conducting our background research, we determined that in order to build an accurate model to predict which pitch was just thrown, you would need to use a similarity analysis tool model with classification trees. This model allows us to take old data and compare it to the live data of the pitch. By comparing the live data to the older data, it will give the most accurate results due to how the players are playing during that game. A batter may struggle with a certain type of pitch that day and not another.&lt;/p>
&lt;p>The similarity analysis tool model would best be used to find instances of the batter in certain pitch counts. For instance, this tool would analyze the different amount of times the batter has faced a count 1 – 2 (1 ball and two strikes), and then find what pitch they saw next and what the outcome was. This information would then be filtered to see what pitch will have the best outcome for the pitcher, it would tell them what pitch to throw and where to throw it. This is where the classification trees would then be used. The trees would classify the recommended pitches to throw and their location to the types of pitches that they throw.&lt;/p>
&lt;h2 id="5-limitations">5. Limitations&lt;/h2>
&lt;p>There are several limitations to predictive models for pitch types &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. The biggest is pitch classification itself. Pitchers ultimately pick what types of pitches they throw and as some pitches behave like others, different pitchers can classify the same pitch type as something else. For example, the traditional curve ball and a knuckle curve react the same. Other classification challenges that can be faced can be related to how the pitcher is playing. As the game goes on, pitcher’s velocities traditionally decrease due to their arms tiring, this could result in misidentification between different types of fastballs. The last limitation that could affect the pitch classification could be the PITCHf/x system malfunctioning or interference with the system.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>This report discusses the use of predictive model in baseball and how they can be used to predict the next pitch thrown. By reviewing previous models down by Harvard &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> and by North Carolina State University &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>, we determined that the best way to build a predictive model would be by using a similarity analysis tool model with classification trees. With the Carolina State University’s successful model, we also concluded that there were limitations to it, as different pitchers classify their pitches differently and telling different pitches apart due to the similar behaviors. The use of predictive models in baseball could change the game as it gives the pitchers an advantage over the batters. We hope that this report can show how big analytics can affect the game of baseball.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8 References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>[2020. About Statcast. MLB Advanced Media. &lt;a href="http://m.mlb.com/glossary/statcast">http://m.mlb.com/glossary/statcast&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Nathan, Alan M. Tracking Baseballs Using Video Technology: The PITCHf/x, HITf/x, and FIELDf/x Systems. &lt;a href="http://baseball.physics.illinois.edu/pitchtracker.html">http://baseball.physics.illinois.edu/pitchtracker.html&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Plunkett, Ryan. 2019. Pitch Type Prediction in Major League Baseball. Bachelor&amp;rsquo;s thesis, Harvard College. &lt;a href="https://dash.harvard.edu/handle/1/37364634">https://dash.harvard.edu/handle/1/37364634&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Sidle, Glenn. 2017. Using Multi-Class Classification Methods to Predict Baseball Pitch Types. North Carolina State University. &lt;a href="https://projects.ncsu.edu/crsc/reports/ftp/pdf/crsc-tr17-10.pdf">https://projects.ncsu.edu/crsc/reports/ftp/pdf/crsc-tr17-10.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Schale, Paul. 2020. MLB Pitch Data 2015-2018. Kaggle &lt;a href="https://www.kaggle.com/pschale/mlb-pitch-data-20152018">https://www.kaggle.com/pschale/mlb-pitch-data-20152018&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Sharpe, Sam. 2020. MLB Pitch Classification. Medium. &lt;a href="https://technology.mlblogs.com/mlb-pitch-classification-64a1e32ee079">https://technology.mlblogs.com/mlb-pitch-classification-64a1e32ee079&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data Analytics in the National Basketball Association</title><link>/report/fa20-523-317/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-317/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-317/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-317/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-317/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-317/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Igue Khaleel, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-317/">fa20-523-317&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-317/blob/master/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The National Basketball Association and the deciding factors in understanding how the game should be played in terms of coaching styles, positions of players, and understanding the efficiencies of shooting certain shots is something that is prevalent in why analytics is used. Analytics is a topic space within basketball that has been growing and emerging as something that can make a big difference in the outcomes of gameplay. With the small analytic departments that have been incorporated within teams, results have already started coming in with the teams that use the analytics showing more advantages and dominance over opponents who don&amp;rsquo;t. We will analyze positions on the court of players and how big data and analytics can further take those positions and their game statistics and transform them into useful strategies against opponents.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#11-point-guard">1.1 Point Guard&lt;/a>&lt;/li>
&lt;li>&lt;a href="#12-shooting-guard">1.2 Shooting Guard&lt;/a>&lt;/li>
&lt;li>&lt;a href="#13-small-forward">1.3 Small Forward&lt;/a>&lt;/li>
&lt;li>&lt;a href="#14-power-forward">1.4 Power Forward&lt;/a>&lt;/li>
&lt;li>&lt;a href="#15-center">1.5 Center&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#2-era-of-analytics">2. Era of Analytics&lt;/a>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#31-the-houston-rockets">3.1 The Houston Rockets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-tools">3.2 Tools&lt;/a>&lt;/li>
&lt;li>&lt;a href="#33-draft-philosophy">3.3 Draft Philosophy&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-background-work-and-advanced-analytics-in-basketball">4. Background Work and Advanced Analytics in Basketball&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-algorithims-associated-with-nba">5. Algorithims associated with NBA&lt;/a>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#51-k-means">5.1 K-Means&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-linear-regression">5.2 Linear Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-logistic-regression">5.3 Logistic Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#54-support-vector-machines">5.4 Support Vector Machines&lt;/a>&lt;/li>
&lt;li>&lt;a href="#55-artificial-neural-networks">5.5 Artificial Neural Networks&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgment">7. Acknowledgment&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> basketball, sports, team, analytics , statistics, positions&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The National Basketball Association was first created in the year of 1946 with the name of BAA (Basketball Association of America). However, in 1949 the name was changed to the NBA with a total of 17 teams&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. As time progressed the league started picking up steam and more and more teams began to join and it wasn’t until the 90’s that we see the total amount of NBA teams be produced.This league consists of professional basketball players from both national and international spaces of the world. As there are 16 roster spots per team and 32 teams in total, only the very most athletic, skillfull, and colossal individuals are chosen to represent this league. Now, knowing the special skillsets of individual players, the founder of basketball, James Naismith, created positions to maximize these individual players for team success. On the court there are 5 positions : point guard, shooting guard, small forward, power forward, and center&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="11-point-guard">1.1 Point Guard&lt;/h4>
&lt;p>Starting with the point guard, generally these individuals are the smallest players on the court with an average height around 6'2 tall. With what these player lack in height they make up for in skillset in terms of quickness, passing, agility, ball handling, and natural shooting ability. Point guards are generally looked at to be the floor general of the team and take up the job of setting up the coach&amp;rsquo;s gameplan and teamates.&lt;/p>
&lt;h4 id="12-shooting-guard">1.2 Shooting Guard&lt;/h4>
&lt;p>The shooting guard is a generally a slightly taller player than the point guard and like the name suggests they are generally the player known for their indiviualisitc shooting prowess whehter if it is beyond the 3 point line or in the mid-range. Shooting guards are known to be positioned in the perimeter(outside the arc) as a partner to the point guard. On occasion, the role of the shooting guard is expanded in the case that the point guard is pressured so the role may be for the shooting guard to be better at defense or a player that can help in the playmaking duties of the point guard.&lt;/p>
&lt;h4 id="13-small-forward">1.3 Small Forward&lt;/h4>
&lt;p>The small forward is where things change in terms of roles when comparing to the guards of that were previously mentioned above. They can be considered hybrids in the sense that they can both operate on the perimeter like guards and can go down low like power forwards and centers which will be discussed later. Noramlly with wings(another name for small forward) with an average height around 6'7, there are a plethora of responsibilites in order to be considered effective. The reason for this is because generally speaking, small forwards are the most athletic player on the court. They basically have most the agility and ball handling of guards and most of the physicallity and power of power forwards/centers. Understandibly, there are tasked with big defensive assignments and are usually looked at to be a decent to above-average producer on offense.&lt;/p>
&lt;h4 id="14-power-forward">1.4 Power Forward&lt;/h4>
&lt;p>The power forward position is where the physicallity of players matters more. Generally these players are around 6'9 to 6'11 and are heavier than most players. Becuase of this they give up speed and shooting which is why they operate around the free throw line and basket. They are looked at to protect the interior with the center from smaller players and small forwards driving in the lane to the basket.&lt;/p>
&lt;h4 id="15-center">1.5 Center&lt;/h4>
&lt;p>The center is considered mostly the point guard of the defense of the team. They are generally the anchor that protects the rim primarily and takes up defensive assignemtns and calls. Without a competent center, a team can see their defense take a hit. Along with defense, centers are good options to go to when the team has offensive lulls since the easiest shot to make in the nba is a hook shot or layup and the center operates 3 feet from the basket. Centers generally range from 6'11 to as high as 7'6 in height. On rare occasions you can see 6'9 to 6'10 centers take the court and that is generally because of play-style or above-average defense.&lt;/p>
&lt;h2 id="2-era-of-analytics">2. Era of Analytics&lt;/h2>
&lt;p>The National Basketball Association continues to not only grow in the sense of continued personnel but an increase of cap(cash flow) amongst teams as well. Within the scope of this prosperous cap situation that the NBA has accumulated over the years through merchandising, tickets, and tv deals, teams have found flexibility in the ability to create the optimal situation for whatever version of basketball the General Manager sees fit for the vision of the team. In terms of better understanding how this can be accomplished it is best to understand what spurred this action of finding styles to lead to the best team success.&lt;/p>
&lt;p>That particular action is players such as Stephen Curry, a 6-3 NBA point guard, that led to the change in utilizing analytics. The year Steph Curry broke through as an MVP, his team; the Golden State Warriors broke the former Chicago Bulls record of 72-9. This in big part was due to Steph Curry breaking the 3pt record as well as Golden State adopting the small ball philosophy. This particular year gave birth to the era of analytics because of how dominate those two approaches were.&lt;/p>
&lt;h4 id="31-the-houston-rockets">3.1 The Houston Rockets&lt;/h4>
&lt;p>This has then inspired teams to introduce analytics departments to measure ways to beat the game and exploit mismatches in defensive schemes and height within players. An example of a team that spearheaded this change in strategy is the Houston Rockets. Their GM(General Manager) Daryl Morey was a MIT graduate who advocated for a team that primarily shoots three point shots as their main forte&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The science behind this concept was that 33% shooting from the three point line measure to 50% from the two point line respectively. This was in the works in the year of 2017 just two years removed from Steph Curry&amp;rsquo;s three point dominance in his MVP season. In terms of numbers representing the change, the 2018 Houston Rockets attempted approximately 82% of their shot attempts around the three point line and the restricted area(the circle around ~5 feet in diameter surrounding the rim)&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The next best team in that department was eleven percent down at 71% in terms of attempts. In this year, the Rockets won their conference at a record of 65 wins - 17 losses as well as break the NBA record in three pointers attemted and made.&lt;/p>
&lt;h4 id="32-tools">3.2 Tools&lt;/h4>
&lt;p>In order to evaluate these players and acquire the data necessary for analyization, the NBA partnered with a company name STATS to provide the necessary tools for data collection. STATS worked with the NBA by installing six cameras in each basketball arena in order to, &amp;ldquo;track player and referee movements at 25 frames per second to get the most analytical data for teams and the NBA to analyze&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&amp;rdquo; This is very effective in terms of showing the play-by-play moves of players in a system as well as even how referees move. With players, these tools can serve as a chess board where the coach is able to watch pieces move and can determine where certain positions could be optimized to its maximum efficiency. This allows for film sessions to be more productive and helpful for players to better see where they fit and even improve in. In terms of referees, throughout sports it is known that referees have cost some games due to missed calls or questionable decisions. This technology can help in terms of understanding: 1) how a specific referee calls certain fouls and 2) if there seems to be a number count of fouls depending on what team the referee is reffing historically. Understanding both the tendencies of players and refs alike gives coaching staffs a direction to go in when preparing for opponents on a game-by-game basis&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="33-draft-philosophy">3.3 Draft Philosophy&lt;/h4>
&lt;p>Another facet of the game that is likewise impacted by the tools and techniques described in 3.2 is the NBA draft. The NBA draft consists a total of 60 players selected in two rounds combined. The general consensus before this analytics era was to choose the best player avaible most of the time. Teams back then usually drafted big men(e.g. forwards and centers) because it was considered a safe pick and known to help your team better in more areas. As time passed, we&amp;rsquo;ve seen a shift to more guards that are drafted instead to fit the narrative the analytics presents to teams regarding the best path to success. For example, earlier Stephen Curry was mentioned to be one of the foundational reasons that the analytics movement was largely adapted. The year Curry got drafted, the #1 pick in the draft was Blake Griffin who at the time was considered the best Power Forward in the draft while Curry was drafted at 8th overall and even James Harden of the Houston Rockets was drafted 3rd&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. As we fast forward to 2020, both Curry and Harden are looked at as the two best players from their draft class with Curry revolutionizing the three point shot and Harden being the ultimate analytics player with his ability to manipulate the defense and draw free throws from fouls like no player has ever done. As years passed, there has been a shift in drafting players with the mindset of that particular players' potential over fit in the sense that teams look for the best available player that fits the teams system the most efficiently&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. An example is the upcoming 2020 NBA draft where there is a question of who will become the #1 and 2 pick respectively. The Golden State Warriors have the 2nd pick in the draft because of a year of injuries for all of their star players. So, they typically aren&amp;rsquo;t looking for a player like most losing teams are doing in the draft. In the eyes of many scouts, some view a player like Lamelo Ball, a 6'7 point guard as the best player or at least second best and others see players like Anthony Edwards(SG), James Wiseman(C) and Deni Advija(SG) as potentially better fits and safer picks. However, for the warriors rumors over social media from notable sources have shown that they aren&amp;rsquo;t interested in drafting Lamelo Ball as he is a point guard and they have Steph Curry already. They instead prefer to choose a Small Forward or Center that can help their defensive potential and style of play. Years ago, that may not have been the case as the best player available would usually come off the board and the team would figure it out after that. Thus, this shows how analytics has not only persuaded teams to change their play styles and system but also the players that come with it whether they are veterans or incoming rookies.&lt;/p>
&lt;h2 id="4-background-work-and-advanced-analytics-in-basketball">4. Background Work and Advanced Analytics in Basketball&lt;/h2>
&lt;p>Considering how the impacts of how implemented analytics has aided the NBA atmosphere as mentioned above, we look to learn technologies and work that help bring this about. This begins with camera systems that have been implemented by a company named SportVU who&amp;rsquo;ve helped bring about change in NBA arenas since 2013 that track player and basketball movement across the arenas&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. This system goes further in the analysis of collecting data in the context of individual player statistics being captured as well as their positioning on the court and speed in particular instances.&lt;/p>
&lt;p>Thus by capturing the basic statistics such as points, assists, rebounds, steals and blocks. Analytical tools such as Player Efficiency Ratings and Defensive Metrics were better used to analyze players and their individualistic impacts on the basketball court. The impacts of these analytical/computational metrics are represented in many organizations abilities to understand the scope of player&amp;rsquo;s salaries and positioning on the court, who to draft, and helps sports analyst on TV shows such as FS1 and ESPN to easily break down the game of Basketball.&lt;/p>
&lt;p>This is where algorithims come to play as an algorithim needs a dataset from which it can train itself and develop statistical patterns to help in predictive analysis and representation for coaches and teams to utilize respectively. An example of this is show through students named Panna Felsen and Lucy from the University of California Berkely, who are developing a software name Bhostgusters that helped analyze the body positions of players and further the response and movements of a team to certain plays run by the opposition&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The end goal of this for coaches to be able to draw up a play on a tablet and see potential conflicts, results, and how opponents may counteract that particular play.&lt;/p>
&lt;p>Other technologies that are being developed and implemented are things like, CourtVision, which is technology that shows the statistics of a player making a shot based on that players' past statistics and position on the court. As the player is moving through the court the numbers change to reflect his efficiency on certain areas on the court based on this. As stated by Marcus Woo, the author of this article, these technologies aren&amp;rsquo;t meant to replace the systems in place but instead are there the help in efficiency and effectiveness&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="5-algorithims-associated-with-nba">5. Algorithims associated with NBA&lt;/h2>
&lt;p>When it comes to the variety of algorithims used in the National Basketball Association, we will be analyzing
the range of algorithims discussed through articles and papers on google scholar. We looked at a total of five
algorithims that were commonolu shown to be used of the most searches when it came to predictive and learning
analysis within NBA analytics departements and outside agencies. The algorithims as presented are: K-means,
Artificial Neural Networks, Linear Regression, Logistic Regression, and Support Vector Machines&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Linear
Progression was by far the most written on topic within the five algorithims listed above with a total of 11,000
searches. It is followed by the Support Vector Machines with 5,240, Logistic Regression with 4,500, Artificial
Neural Networks with 4,300, and K-Means with 1,590 search results(*all results via google scholar search bar).&lt;/p>
&lt;h4 id="51-k-means">5.1 K-Means&lt;/h4>
&lt;p>The first algorithim we&amp;rsquo;ll look at is K-Means which is classified as generally the &amp;ldquo;clustering algorithim&amp;rdquo; which takes the form of initializing a single point of k or the mean and organizing the data towards that particular mean&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. This is then repeated over and over until the appropriate results are found and compiled. Now as National Basketball Association statistics are inserted this can be used to cluster players together than fit the criteria on certain outcomes of points, rebounds, assists, and blocks.&lt;/p>
&lt;h4 id="52-linear-regression">5.2 Linear Regression&lt;/h4>
&lt;p>Linear Regression, which is very commonly used in machine learning is very effective as a predictor tool. It works by forming &amp;ldquo;regression coefficients&amp;rdquo; that stems from pitting together independent variables which help in predictions within a game&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. So, throught the input and output variables taht are presented predictive measurements can be performed to highlight potential productivety. An example is the &amp;ldquo;Box-Plus-Minus&amp;rdquo;. This was created to show a basketball player&amp;rsquo;s overall court production and effect through their statistics, what position they play on the court and the wins and losses that team incurs because of this&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. This was built through linear regression and shows through charts based on statistics how productive a player is or potentially can be given the system and oppurtunities.&lt;/p>
&lt;h4 id="53-logistic-regression">5.3 Logistic Regression&lt;/h4>
&lt;p>Similarly to Linear Regression, Logistic Regression shares a lot of features in terms of the formula used for prediction except it utilizes a sigmoid as opposed to a linear function when performing calculations. Weight values are the main form of predictions in whatever form of scenario or situation in which that analyst wants to produce&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. An example of this is shown through a logistical regression analysis performed by Oklahoma State University on clutch and non-clutch shots by players in the National Basketball Association. The premise of this is taking the data of an individual player based on their shooting percentages in spots on the floor relative to the distance of the defender on them and using that to figure out the potential of a player making a shot in the clutch(universally known as the last two minutes in a close game)&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. This then shows how a predictive algorithm can be utilized not only based on solely percentages and efficiencies but also with the inclusion of situation on a basketball floor.&lt;/p>
&lt;h4 id="54-support-vector-machines">5.4 Support Vector Machines&lt;/h4>
&lt;p>Support Vector Machines are considered to be a very formidable tool when it comes to measuring classification issues. This modeled machine creates a decision-making tree that helps in the predictions of basketball games and thus can help coaches form strategies and gameplans around what the model predicts can happen. Additional advantages that come with this tool is its ability to operate in high dimensions, the ability to identify kernels, and its memory efficiency&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. The minor issue with this machine is the lack of rule generation but as it is more of an emerging tool overtime this is something that is relatively fixable&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. The advantages&lt;/p>
&lt;h4 id="55-artificial-neural-networks">5.5 Artificial Neural Networks&lt;/h4>
&lt;p>With Artificial Neural Networks the use of the Multi-Layer Perceptron is prevalent and it is highlighted by the vertices of a group in correlation to input varables and comes out with the output&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. This tool according the Beckler is also considered to be, &amp;ldquo;an adaptive system that changes its structure based on external and internal information flows during the network training phase&amp;rdquo;&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. With this, the Artificial Neural Network is considered to be one of the most accurate predictive tools when it comes to basketball and can predict patterns as more data is inputed&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>As time progresses, we will continue to see the use of analytics as well as the expanision of analytics departments in not only the National Basketball Association but other professional sports as well. The impacts of analytics have been highlighted through recent years as mentioned above with the change to styles of play, and the way coaches approach gameplans before each respective game is played. As Adam Silver, the commissioner of the National Basketball Association stated, &amp;ldquo;Analytics have become front and center with precisely when players are rested, how many minutes they get, who they’re matched up against&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>.&amp;rdquo; Through this, Silver explains not only to technical aspect of basketball that analytics supports but the physical aspect which can aid in preventing things like player injuries and rest. Understandibly, this highlights how analytics can help the league now and in the future; especially when more sophisticated machine learning tools and algorithims are produced for this purpose.&lt;/p>
&lt;h2 id="7-acknowledgment">7. Acknowledgment&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Online, N., 2020. NBA History. [online] Nbahoopsonline.com. Available at: &lt;a href="https://nbahoopsonline.com/History/#:~:text=The%20NBA%20began%20life%20as,start%20of%20the%20next%20season">https://nbahoopsonline.com/History/#:~:text=The%20NBA%20began%20life%20as,start%20of%20the%20next%20season&lt;/a>. [ Accessed 20 October 2020].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Editor, M., 2020. How NBA Analytics Is Changing Basketball | Merrimack College. [online] Merrimack College Data Science Degrees. Available at: &lt;a href="https://onlinedsa.merrimack.edu/nba-analytics-changing-basketball/">https://onlinedsa.merrimack.edu/nba-analytics-changing-basketball/&lt;/a> [Accessed 16 November 2020].&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>N. M. Abbas, &amp;ldquo;NBA Data Analytics: Changing the Game,&amp;rdquo; Medium, 21-Aug-2019. [Online]. Available: &lt;a href="https://towardsdatascience.com/nba-data-analytics-changing-the-game-a9ad59d1f116">https://towardsdatascience.com/nba-data-analytics-changing-the-game-a9ad59d1f116&lt;/a>. [Accessed: 17-Nov-2020].&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>C. Ford, &amp;ldquo;NBA Draft 2009,&amp;rdquo; ESPN. [Online]. Available: &lt;a href="http://www.espn.com/nba/draft2009/index?topId=4279081">http://www.espn.com/nba/draft2009/index?topId=4279081&lt;/a>. [Accessed: 17-Nov-2020].&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>M. Woo, &amp;ldquo;Artificial Intelligence in NBA Basketball,&amp;rdquo; Inside Science, 21-Dec-2018. [Online]. Available: &lt;a href="https://insidescience.org/news/artificial-intelligence-nba-basketball">https://insidescience.org/news/artificial-intelligence-nba-basketball&lt;/a>. [Accessed: 07-Dec-2020].&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>M. Beckler and M. Papamichael, &amp;ldquo;NBA Oracle,&amp;rdquo; 10701 Report, 2008. [Online]. Available: &lt;a href="https://www.mbeckler.org/coursework/2008-2009/10701_report.pdf">https://www.mbeckler.org/coursework/2008-2009/10701_report.pdf&lt;/a>. [Accessed: 06-Dec-2020].&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>R. Anderson, &amp;ldquo;NBA Data Analysis Using Python &amp;amp; Machine Learning,&amp;rdquo; Medium, 02-Sep-2020. [Online]. Available: &lt;a href="https://randerson112358.medium.com/nba-data-analysis-exploration-9293f311e0e8">https://randerson112358.medium.com/nba-data-analysis-exploration-9293f311e0e8&lt;/a>. [Accessed: 07-Dec-2020].&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>J. P. Hwang, &amp;ldquo;Learn linear regression using scikit-learn and NBA data: Data science with sports,&amp;rdquo; Medium, 18-Sep-2020. [Online]. Available: &lt;a href="https://towardsdatascience.com/learn-linear-regression-using-scikit-learn-and-nba-data-data-science-with-sports-9908b0f6a031">https://towardsdatascience.com/learn-linear-regression-using-scikit-learn-and-nba-data-data-science-with-sports-9908b0f6a031&lt;/a>. [Accessed: 07-Dec-2020].&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>J. Perricone, I. Shaw, and W. Swie¸chowicz, &amp;ldquo;Predicting Results for Professional Basketball Using NBA API Data,&amp;rdquo; Stanford.edu, 2016. [Online]. Available: &lt;a href="http://cs229.stanford.edu/proj2016/report/PerriconeShawSwiechowicz-PredictingResultsforProfessionalBasketballUsingNBAAPIData.pdf">http://cs229.stanford.edu/proj2016/report/PerriconeShawSwiechowicz-PredictingResultsforProfessionalBasketballUsingNBAAPIData.pdf&lt;/a>. [Accessed: 06-Dec-2020].&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>A. P. B. N. Barakat, J. H. F. L. Breiman, M. T. R. Burbidge, K.-S. S. T. Chen, J. L. R. WW. Cooper, V. N. V. C. Cortes, E. F. M. Hall, J. Holland, R. C. E. J. Kennedy, K. J. Kim, K. H. T. K. Kirchner, J. S. S. P. Kvan, A. C. W. BL. Lee, B. B. D. Martens, J. Mercer, J. K. B. Min, O. B. K. Muata, J. S. L. IS. Oh, P. M. M. M. Pal, J. R. Quinlan, F. P.-C. FJR. Ruiz, W. H. C. JY. Shih, H. M. E. I.-D. MBA. Snousy, P. V. E. Štrumbelj, L. C. FEH. Tay, V. V. S. S. Tripathi, G. Valentini, V. N. Vapnik, G. D. N. Vlastakis, J. N. Wang, E. Y. K. A. Widodo, C. F. H. TA. Zak, and J. S. J. Zhou, &amp;ldquo;Analyzing basketball games by a support vector machines with decision tree model,&amp;rdquo; Neural Computing and Applications, 01-Jan-1970. [Online]. Available: &lt;a href="https://link.springer.com/article/10.1007/s00521-016-2321-9">https://link.springer.com/article/10.1007/s00521-016-2321-9&lt;/a>. [Accessed: 07-Dec-2020].&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>2017 A. S. M. N. A. Jun 01, &amp;ldquo;The NBA&amp;rsquo;s Adam Silver: How Analytics Is Transforming Basketball,&amp;rdquo; Knowledge@Wharton. [Online]. Available: &lt;a href="https://knowledge.wharton.upenn.edu/article/nbas-adam-silver-analytics-transforming-basketball/">https://knowledge.wharton.upenn.edu/article/nbas-adam-silver-analytics-transforming-basketball/&lt;/a>. [Accessed: 07-Dec-2020].&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data in E-Commerce</title><link>/report/fa20-523-329/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-329/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-329/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-329/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-329/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-329/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Wanru Li, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-329/">fa20-523-329&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-329/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The topic of my report is big data in e-commerce. E-commerce is a big part of todays society. During the shopping online, the recommend commodities are fitter and fitter for my liking and willingness to buy. This is the merit of big data. Big data use my purchase history and browsing history to analyze my liking and recommend the goods for me.&lt;/p>
&lt;p>In our everyday lives, e-commerce is now a critical element. It redefines trading practices worldwide. Over the years the growth of eCommerce has been profound. As we move forward, we are learning how to grow eCommerce in this era and how to run an eCommerce company. The dominant mode of trading was brick-and-mortar until the rise of eCommerce. Brick and mortar firms have at least one physical location in supermarket stores. Goods must be bought and sold by active and physical contacts between the buyer and the seller. Brick and mortar trading continues, but eCommerce is increasingly replacing. Many brick and mortar retailers in an evolutionary manner turn themselves into eCommerce stores. This includes an online presence and bringing key company practices online.&lt;/p>
&lt;p>The eCommerce market is increasingly developing as the Internet becomes more available in various areas of the world. Traditional retail companies are migrating to the eCommerce space. Expand their appeal to customers and remain competitive as well. It is clear that the eCommerce shops provide great experiences for customers. An improved flexibility of the Internet, faster purchases, plenty of goods and customized deals, the lack of physical presence restrictions and interaction make it attractive for customers to buy online. E-commerce has many advantages for you, whether you are a company or a customer. Learn all about powering eCommerce sites such as Shopify and Big Commerce online shops.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-search-and-analysis">4. Search and Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-references">6. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> e-commerce, big data, data analysis&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>E-commerce is already changed by big data a lot. As we can see in the lecture slides, the retail store was closed rapidly in the past three years. It can be seen that e-commerce has begun to take shape and has been accepted by customers. E-commerce can make shopping more convenient for customers, and enable companies to better discover current trends and customers' favorite categories for better development.&lt;/p>
&lt;p>For customers, they can find the item they want easier than find it in a retail store. Maybe the customer doesn&amp;rsquo;t know what he wants, doesn&amp;rsquo;t know his brand, only knows its style, but that&amp;rsquo;s enough to search for the item on e-commerce. There are also some e-commerce services that offer photo search, which makes shopping easier. Shopping in e-commerce usually keeps a record of the purchase. In this way, you don&amp;rsquo;t have to go to a retail store to buy some products repeatedly. Instead, you can directly find the products in the record and place orders, which saves a lot of time.&lt;/p>
&lt;p>For companies, there are more changes. They can analyze customers preferences and purchasing power based on their browsing data, shopping cart data, and purchasing data. Large enough to predict the future business trend, small enough to better see the customer&amp;rsquo;s evaluation of the product.&lt;/p>
&lt;p>E-commerce companies have access to a lot of data, which makes it easy for them to analyze product trends and customer preferences. As talent says, &amp;ldquo;Retail websites track the number of clicks per page, the average number of products people add to their shopping carts before checking out, and the average length of time between a homepage visit and a purchase. If customers are signed up for a rewards or subscription program, companies can analyze demographic, age, style, size, and socioeconomic information. Predictive analytics can help companies develop new strategies to prevent shopping cart abandonment, lessen time to purchase, and cater to budding trends. Likewise, e-commerce companies use this data to accurately predict inventory needs with changes in seasonality or the economy&amp;rdquo; &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. There is an example of the Lenovo, to enhance the customer experience and stand out from the competition, Lenovo needs to understand customers' needs, preferences, and purchasing behaviors. By collecting data sets from various touchpoints, Lenovo USES real-time predictive analytics to improve customer experience and increase revenue per retail segment by 11 percent &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Meeting customer needs is not just an immediate problem. E-commerce depends on having the right inventory in the future. Big data can help the company to be prepared for the emerging trend, in the slow or potential prosperity and development of the year, or around major activity plan marketing activities. E-commerce companies will compile large data sets. By evaluating the data of a few years ago, electronics retailers can plan accordingly inventory, inventory to predict peak, simplify the overall business operations, and predict demand. E-commerce sites, for example, can do it in the shopping rush hour in social media significantly depreciate sales promotion, to eliminate redundant products. In order to optimize pricing decisions, e-commerce sites can also provide a special discount. Through big data analysis and machine learning, learn when to offer discounts, how long they should last, and what discount prices are offered more accurately (para 8).&lt;/p>
&lt;p>E-commerce is bound to dominate the retail market in the future because it can help retailers better analyze and predict future trends, which retailers cannot resist. At the same time, e-commerce provides better ways for customers to shop. With better analysis, retail companies will be able to provide better service to customers, so e-commerce will be more and more accepted and popular in the future.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>As Artur Olechowski wrote, &amp;ldquo;According to the IDC, the digital universe of data will grow by 61% to reach a smashing 175 zettabytes worldwide by 2025. There’s no denying that a large chunk of the digital world belongs to e-commerce, which takes advantage of customer social media activity, web browser history, geolocation, and data about abandoned online shopping carts. Most e-commerce businesses are able to collect and process data at scale today. Many of them leverage data analytics to understand their customers’ purchasing behaviors, follow the changing market trends, gain insights that allow them to become more proactive, deliver more personalized experiences to customers. The global Big Data in the e-commerce industry is expected to grow at a CAGR of 13.27% between 2019 and 2028. But what exactly is Big Data? And how can e-commerce businesses capture this powerful technology trend to their advantage? In this article, we take a closer look at the key trends in the usage of Big Data technologies by e-commerce companies and offer you some tips to help you get started in this game-changing field&amp;rdquo; &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The most common and widely used application of big data is in e-commerce. Nowadays, the application of big data in e-commerce is relatively mature. As Artur Olechowski wrote, &amp;ldquo;As businesses scale up, they also collect an increasing amount of data. They need to get interested in data and its processing; this is just inevitable. That’s why a data-driven e-commerce company should regularly measure and improve upon: shopper analysis, customer service personalization, customer experience, the security of online payment processing, targeted advertising&amp;rdquo; &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>There are also some disadvantages of the big data, or to say more need to do after getting the data. Artur Olechowski wrote, &amp;ldquo;Understand the problem of security — Big Data tools gather a lot of data about every single customer who visits your site. This is a lot of sensitive information. If your security is compromised, you could lose your reputation. That’s why before adopting the data technology, make sure to hire a cybersecurity expert to keep all of your data private and secure&amp;rdquo;&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> . Security is always a big problem with big data. This is one of the components will be analyzed in my report. He also wrote, &amp;ldquo;Lack of analytics will become a bigger problem — Big Data is all about gathering information, but to make use of it, your system should also be able to process it. High-quality Big Data solutions can do that and then visualize insights in a simple manner. That’s how you can make this valuable information useful to everyone, from managers to customer service reps&amp;rdquo; &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The analysis is also an important part of the big data. Only collecting data cannot help e-commerce anything. Security and analytics will be talked about in my report.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>QUARTERLY RETAIL E-COMMERCE SALES 2 nd QUARTER 2020:&lt;a href="https://www.census.gov/retail/mrts/www/data/pdf/ec_current.pdf">https://www.census.gov/retail/mrts/www/data/pdf/ec_current.pdf&lt;/a>&lt;/p>
&lt;p>For the dataset, the source website provided in the project requirements will be used, if there needs more information, data and information on the web will be searched for. As a result of recent COVID-19 incidents, many organizations work in a small capacity or have entirely ceased activities. The Census Bureau has tracked and analyzed the response and data quality in this manner.&lt;/p>
&lt;p>Monthly Retail Trade from Census will be analyzed. The Census Bureau of the Department of Commerce today reported that the forecast of U.S. retail e-commerce revenue for the second quarter of 2020 adjusted for seasonal fluctuations, but not for price adjustments, was $211.5 billion, a rise of 31.8 per cent (plus or minus 1.2 per cent) from the first quarter of 2020. Total retail revenues were projected at $1,311.0 billion for the second quarter of 2020, a decline of 3.9 percent (plus or minus 0.4 percent) from the first quarter of 2020. The e-commerce forecast for the second quarter of 2020 increased (para1).&lt;/p>
&lt;p>Retail e-commerce sales are estimated from the same sample used for the Monthly Retail Trade Survey (MRTS) to estimate preliminary and final U.S. retail sales. Advance U.S. online transactions are calculated from a subsample of the MRTS survey that is not of appropriate magnitude to calculate improvements in retail e-commerce transactions.&lt;/p>
&lt;p>A stratified basic random sampling procedure is used to pick approximately 10,800 retailers, except food services, whose transactions are then weighted and benchmarked to reflect the entire universe of over two million retailers. The MRTS sample is focused on probability and represents all employer firms engaged in retail activities as described in the North American Industry Classification System (NAICS). Coverage covers all vendors whether or not they are active in e-commerce. Internet travel agents, financial brokers and distributors, and ticket sales companies are not listed as retail and are not included with either the gross retail or retail e‐commerce sales figures. Non employees are reflected in the projections by benchmarking of previous annual survey estimates that contain non employer revenue based on administrative data. E-commerce revenues are included in the gross monthly sales figures.&lt;/p>
&lt;p>The MRTS sample is revised on a continuous basis to account for new retail employees (including those selling over the Internet), company deaths and other shifts in the retail business environment. Firms are asked to report e-commerce revenue on a monthly basis separately. For each month of the year, data for non-responsive sampling units shall be calculated from reacting sampling units falling under the same class of sector and sales size segment or on the basis of the company&amp;rsquo;s historical results. Responding firms account for approximately 67% of the e-commerce sales estimate and approximately 72% of the U.S. retail sales estimate for any quarter.&lt;/p>
&lt;p>Estimates are obtained by summing the weighted sales (either reported or charged) for each month of the quarter. The monthly figures are benchmarked against previous annual survey estimates. Quartal projections are determined summing up the monthly benchmarked figures. The forecast for the last quarter is a provisional forecast. The calculation is also open to revision. Data consumers who make their own projections using data from this study can only apply to the Census Bureau as the source of input data.&lt;/p>
&lt;p>This article publishes forecasts optimized for seasonal variation and variations in holiday and trade days, but not for adjustments in rates. As feedback for the X‐13ARIMA‐SEATS programme, we have used the updated figures of quarterly figures of e-commerce revenue for the fourth quarter 1999 up to the present quarter. For revenue, we estimated the quarterly adjusted figures for each year with an additional modified monthly revenue forecast. Seasonal estimate adjustment is an approximation based on current and previous experiences.&lt;/p>
&lt;p>The estimates containing sample errors and non-sample errors in this article are based on a survey.&lt;/p>
&lt;p>The difference between the prediction and the results of the full population listing under the same sample conditions is the sampling error. This mistake happens when a national poll only tests a sub-set of the total population. Estimated sampling variance measurements are standard errors and variance coefficients, as stated in Table 2 of this article.&lt;/p>
&lt;h2 id="4-search-and-analysis">4. Search and Analysis&lt;/h2>
&lt;p>This year the pandemic accelerated growth in ecommerce in the US, with online revenues projected to hit just 2022. The top 10 ecommerce retailers will strengthen their hold on the retail market with our Q3 American retail prediction.&lt;/p>
&lt;p>This year, revenues of US eCommerce are projected to hit $794.50 billion, up 32.4% annually. This is even more than the 18.0% predicted in our Q2, since customers are now ignoring shops and opting to buy online in the wake of the pandemic.&lt;/p>
&lt;p>In &amp;ldquo;US Ecommerce Growth Jumps to More than 30%, Accelerating Online Shopping Shift by Nearly 2 Years&amp;rdquo;, it says, &amp;ldquo;&amp;lsquo;We’ve seen ecommerce accelerate in ways that didn’t seem possible last spring, given the extent of the economic crisis,&amp;rsquo; said Andrew Lipsman, eMarketer principal analyst at Insider Intelligence. &amp;lsquo;While much of the shift has been led by essential categories like grocery, there has been surprising strength in discretionary categories like consumer electronics and home furnishings that benefited from pandemic-driven lifestyle needs&amp;rsquo;&amp;rdquo; &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This year, ecommerce revenues are projected to hit 14.4 percent and 19.2 percent of all US retail spending by 2024. Without purchases of petrol and cars, ecommerce penetration leaps to 20.6% (classes sold almost entirely offline).&lt;/p>
&lt;p>In &amp;ldquo;US Ecommerce Growth Jumps to More than 30%, Accelerating Online Shopping Shift by Nearly 2 Years&amp;rdquo;, it writes, &amp;ldquo;&amp;lsquo;There will be some lasting impacts from the pandemic that will fundamentally change how people shop,&amp;rsquo; said Cindy Liu, eMarketer senior forecasting analyst at Insider Intelligence. &amp;lsquo;For one, many stores, particularly department stores, may close permanently. Secondly, we believe consumer shopping behaviors will permanently change. Many consumers have either shopped online for the first time or shopped in new categories (i.e., groceries). Both the increase in new users and frequency of purchasing will have a lasting impact on retail&amp;rsquo;&amp;rdquo; &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Online commerce will be so high that this year, at $4,711 trillion, this will more than compensate for the 3,2 percent fall in brick and mortar expenses. Complete US retail revenue will also remain relatively flat.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>More users are benefiting from the majority of online resources, including eCommerce, as internet penetration and connectivity improve. In everyday life e-commerce has become a mainstream, with fundamental advantages. The e-commerce market is projected to reverse double digit growth in net accounts from anywhere around the world. However, e-commerce can expand enormously as digital payment options are growing in these areas. About 22% of the world&amp;rsquo;s stores are now online. By 2021, e-Commerce online revenues are projected to hit $5 trillion.&lt;/p>
&lt;p>Fru Kerik says, &amp;ldquo;The most popular eCommerce businesses worldwide are Amazon, Alibaba, eBay, and Walmart. These eCommerce giants have redefined the retail industry irrespective of location. They accumulate revenues that exceed billions of dollars yearly. As internet accessibility increases, these estimates would skyrocket. At the time of this writing, Amazon is present in 58 countries, Alibaba in 15, Walmart in 27, MercadoLibre in 18&amp;rdquo; &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>E-Commerce firms have also contributed to the rise of e-Commerce through methodological findings. E-Commerce firms follow customer expectations and make important discoveries about the business-to-consumer model. These insights are then incorporated in market models, ensuring smooth future revenue increase globally.&lt;/p>
&lt;h2 id="6-references">6. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&amp;ldquo;7 Ways Big Data Will Change E-Commerce Business In 2019 | Talend&amp;rdquo;. Talend Real-Time Open Source Data Integration Software, 2020. &lt;a href="https://www.talend.com/resources/big-data-ecommerce/">https://www.talend.com/resources/big-data-ecommerce/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Olechowski, Artur. &amp;ldquo;Big Data in E-Commerce: Key Trends and Tips for Beginners: Codete Blog.&amp;rdquo; Codete Blog - We Share Knowledge for IT Professionals, CODETE, 8 Sept 2020. &lt;a href="https://codete.com/blog/big-data-in-ecommerce/">https://codete.com/blog/big-data-in-ecommerce/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>&amp;ldquo;US Ecommerce Growth Jumps to More than 30%, Accelerating Online Shopping Shift by Nearly 2 Years.&amp;rdquo; EMarketer, 12 Oct. 2020. &lt;a href="https://www.emarketer.com/content/us-ecommerce-growth-jumps-more-than-30-accelerating-online-shopping-shift-by-nearly-2-years">https://www.emarketer.com/content/us-ecommerce-growth-jumps-more-than-30-accelerating-online-shopping-shift-by-nearly-2-years&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Kerick, Fru. &amp;ldquo;The Growth of Ecommerce.&amp;rdquo; Medium, The Startup, 1 Jan. 2020. &lt;a href="https://medium.com/swlh/the-growth-of-ecommerce-2220cf2851f3#:~:text=What%20Exactly%20is%20E%2Dcommerce,%2C%20apparel%2C%20software%2C%20furniture">https://medium.com/swlh/the-growth-of-ecommerce-2220cf2851f3#:~:text=What%20Exactly%20is%20E%2Dcommerce,%2C%20apparel%2C%20software%2C%20furniture&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data Analytics in Brazilian E-Commerce</title><link>/report/fa20-523-330/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-330/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-330/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-330/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Oluwatobi Bolarin, &lt;a href="mailto:bolarint@iu.edu">bolarint@iu.edu&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-330/">fa20-523-330&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-330/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>As the world begins to utilize online service and stores at greater capacity it becomes a greater priority to increase the efficiency of the various processes that are required for online stores to work effectively. By analyzing the data the comes from online purchases, a better understanding can be formed about what is needed and where as well as the quantity. This data should also allow for us to better predict what orders will be needed at future times so shortages can be avoided.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-analysis">5. Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> economics, Brazil, money, shipping, Amazon, density&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Brazil has the largest E-commerce market in Latin America. &amp;ldquo;It has been estimated that the country accounts for over one third of the region&amp;rsquo;s ecommerce market&amp;rdquo;&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. However, the growth of the potential e-commerce giant has problems that could potentially stunt its long-term growth. The concentration of this effort is to determine the areas the money is spent; however, this topic should expand to other countries and nations to determine locations to stores specific products. After amazon, this can be applied to many online stores or stores that have so form of digital commerce. Due to the nature of spending, this method could also be used to determine what regions of a country are in need of what items when the residents are in poverty. The amount of money that is spent can also be a strong indicator about what trends are possible and what type of goods people are willing to spend their income on. Part of the reason why this is important is due is combating shortages. The COVID pandemic showed that working global supply chains to their maximum at all times isn’t just a bad idea, it is detrimental to the citizens of their respective countries and by extension the world. Supply chains that are constantly working to their maximum capacity lives no room for emergency supplies that could be utilized in live saving applications. Examples would be masks, hospital beds, protective equipment, etc.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>The study of economics is understanding the best way how to deal with scarcity. Earning a bachelor’s degree in economics didn’t feel satisfying enough due to my love for technology. Determine how resources are allocated is one of the most important decision that can be made in our modern world regardless of our region, race, occupations, or economical class.&lt;/p>
&lt;p>Working in restaurants for 7 years working through undergrad the opportunity to learn numerous social and management skills. However, there would always be never ending problems present thing themselves through the day. One of the most painful ones would consistently be the fact that we would run out of things. Surplus were hardly considered acceptable and when there was a shortage the workers would pay the price. This would be understandable in places that just opened for less than a year, however in restaurants that had been in business for a while it was inexcusable. People can reason that, it was a busy day so we didn’t know that it would sell out, that a product went bad that wasn’t counted before, or someone made an ordering error. However, none of those excuses are valid in a place that has been running for a while because they should have an estimate about how much they should be growing and how much extra product they should by to ensure the fact that they don’t run out.&lt;/p>
&lt;p>This is a similar issue that supply chains around the world had, except, with one problem you would have to deal with numerous angry customers and the other it was a matter of lives. This matter should increase the emphasis that we should have more relax supply chains in our world then over-worked ones. It seems that it is in worlds best interest that we start formatting our data in a more readable way for ever one to see a understand so we can do a better job of managing where we place our resources. Constantly determining what is being done well and what needs to be working on and improved. This analysis will attempt to better illustrate a better picture to determine more practical ways to increase e-commerce growth in Brazil and possible by extension this method can be superimposed on other regions of the world.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>After exploring a vast amount of data available, it was best to choose the following two datasets in order to analyze Amazon sales data in brazil to get an unbiased look at what sells on average.&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://www.kaggle.com/olistbr/brazilian-ecommerce?select=olist_products_dataset.csv">Amazon Sales Data&lt;/a>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>The four datasets that will speficialy be using in this file will be the &amp;ldquo;olist_customers_dataset.csv&amp;rdquo;, &amp;ldquo;olist_order_items_dataset.csv&amp;rdquo;, &amp;ldquo;olist_orders_dataset.csv&amp;rdquo;, and &amp;ldquo;olist_products_dataset.csv&amp;rdquo;. Both of the datasets are needed for this project because to obtain the location data from customers the zip code of the customer is needed for the olist_customers_dataset.csv dataset and the name of the order and specifically the category that it is in is held in the olist_order_items_dataset.csv dataset.
For this project work is done with a dataset of 100,000 that has various amazon orders ranging from basic items to more expensive and complex things. The only limiter is what was being sold on amazon at that point in time. The data set size is only 120.3 MB and markdown will be used to show the findings and the method got to them.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/datasetImage.png" alt="Dataset Image Map">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Database Map&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The data that is used can be derived from the database map using in figure 1. The data in each data set that is necessary for this project would be the olist_customers_dataset.csv for what location the customer got their goods delivered to. This can be done with both the city that they live in and the zip code. Both should be used to see if there is going to be a notable difference between what people in entire cities will order verses specific zip codes. Then the order can be found in olist_orders_dataset.csv by matching the customer ID in the dataset olist_customers_dataset.csv. After the specific order is found we can find the specific item that was bought with the dataset olist_order_items_dataset.csv. The chain completed by using the product ID from the olist_order_items_dataset.csv and matching it with the product ID in the olist_products_dataset.csv.&lt;/p>
&lt;p>The reason why it needs to be done this way is because the information that is necessary to answer the question is too vast if people are to deal with specific items so it would be more important and helpful if we found the category of items that is necessary instead. In the end, this project deals with four different datasets in the same database that helps us connect the location of where the order is needed and being sent to the type of object that is bought.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;p>To correctly articulate the scope of what the dataset is measuring and the region that it is as well as when the data was collected for the project. It is also needed to come up with a viable solution for any (if there is) missing data that exists in the data set. Looking into ways that that the different data sheets interact with one another to find any patterns or factors that could exist that aren't otherwise easily seen is needed as well.&lt;/p>
&lt;p>To determine the product categories that were sold in each region it is mandator that the relevant data frames were merged together through panda. When that is done it is optional to remove the data that isn’t necessary. The reason it is a necessity to merge the data sets together is because the way that it is currently organized is not helpful to the analysis that is trying to be done. The olist_order_customer_dataset contains only information about the customer ordering the objects. The Customer Dataset has no information on the item that they ordered or what. The only relative data that it has for this analysis is the Zip Code of the consumer, the City of the consumer and the State of the consumer. With all of this information we can make any size analysis base on an area as small as people in the same Zip Code to be people as large as a region by putting together multiple cities together. This concept can be utilized with any area data, so it is helpful here as well. However, the customer_id is needed to link the olist_order_customer_dataset with the olist_orders_dataset.&lt;/p>
&lt;p>Although the olist_orders_dataset doesn’t have any information that directly makes it important for this analysis it does have the order_id that is linked to customer_id. Thus, the Order that the Customer made can be linked back to the customer and from the customer the location that it was ordered to. Then with the order_id, we can link the olist_order_items_dataset. Interesting things can be determined from this information.&lt;/p>
&lt;p>For example, we can determine the average freight_value for any area in our data size regardless of how big. It can also be used compare that to other areas or regions to determine how much each large shipment of items should be ensured by. The same thing can also be done with the price and that can help determine economic status, although without knowing the specific type of item this type of analysis may not be accurate and, by extension, not beneficial. However, if any really meaningful analysis that will come out of this the olist_products_data set is a one of the most important sets of data that we need for this analysis. To that end, the product_id from olist_order_items_dataset needs to be linked with the product_id in the olist_products_dataset. With that the area that the products go to are linked with what the products are.&lt;/p>
&lt;p>By linking the products_id to the olist_products_dataset an analysis can be done with about specific product information and the region that it is going to. The most important for this analysis, however, would be the product_category_name. This is important because the product category that is ordered can now be linked with the region that it is going to. Down to an area as small as a zip code. Ultimately, this could potential help determine how much what should be store and where to best help the customers and increase 2-day completion rates without over burdening the work force any more then necessary. From this large data frame, it can now be determining what type of products are ordered this most in what regions. If this information is utilized correctly and efficiently it can greatly reduce the stress of a supply chain. By increase the supply chain’s efficiency without increasing the load we can create slack that will be able to minimize the stress a supply chain suffers when demand is higher than usual for any reason.&lt;/p>
&lt;h2 id="5-analysis">5. Analysis&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/figure1.png" alt="ZIP from Python">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> First set of preliminary data showing zicode density&lt;/p>
&lt;p>For figure 2, this is a histogram of the various zip codes in Brazil. The problem with this chart is the fact that it doesn’t really tell you much. It doesn’t properly illustrate the density each area nor do a good job of showing exactly how many orders were made in each area. All this graph could show is the number of customers that reside in each area, but it doesn’t do that well either. What would have worked better for this would have been to first of all understand the fact that there is a lot of data that should have been broken up from the beginning. The main problem with this graph, as well as many others in the series, is the fact that it is trying to show a lot with far too little. The problem would be &amp;ldquo;solved&amp;rdquo; in a sense if, instead of showing all of the zip codes, only finding the zip codes in a part of Brazil. That would enable people to be able to see multiple different useful graphs. One graph could be made to show the number of customers per region and then a few others could be regional showing the Zip codes, states, and/or cities for each region. This would produce graphs that could inform the reader more about how many customers are living in each area.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/figure3.png" alt="Heatmap Data">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Shows Heat map of all the data&lt;/p>
&lt;p>Figure 3 is a heatmap showing all of the data. This type of graph really doesn’t make sense with the parameters that it was set with. It would have been more helpful to show, like in figure 2, the relationship between the areas and the amount of people that were in the areas. It would also be good to use it for the visualization of areas and the number of products for each area as well as how much is made revenue in each area. The problem with the current format is the fact that it is trying to incorporate strings as well as various numbers that just make all the data harder to understand.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/figure2.png" alt="Sales Per City">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Shows Sales per State&lt;/p>
&lt;p>Figure 4 is a histogram that shows the number of sales for each state in Brazil. Although this graph is an improvement in terms of relying relevant information it doesn’t do a good enough job. Because this graph is a histogram and not a bar graph it doesn’t show the number of sales in individual states, but rather shows the sales numbers of groups of states. The reason why that isn’t optimal is because it gives the illusion of a trend by helping viewers assume that the states group together are group for a reason, when the only reason is for the groupings is the number of sales that is in each area not the area that the products were delivered.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/figure4.png" alt="Products">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Histogram for different products in Brazil&lt;/p>
&lt;p>Figure 5 also has similar problems in terms of visualization. One problem is the fact that it mimics at lot of the problems that figure 4 had, like how things are group. However, the second problem is the worst one; the fact that the labels can’t be read. The reason why this occurs is due to the large amount of data that includes 73 different categories of products as well the fact that the graph is also a histogram. This problem is a slightly more complex on to fix just because of the amount of data, no matter how it would be addressed it would be too large. The most beneficial method would be to show the top 6 products and one more bar of the remainder of the sections and that may be able to better demonstrate the different levels of products in a more impactful way. Couple with this would be a list that shows the amount for all the categories as well as the percent that the categories fill.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/figure5.png" alt="SP State Products">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Histogram for products sold In SP State.&lt;/p>
&lt;p>Figure 6 has similar problems to Figure 4 and 5 however, it would show more information if than the others because the scope is much smaller. One of the most glaring problems is the exact same as figure 5, which is having too much on the X-axis. The majority of the problems can be fixed by reducing the scope of that data that we are trying to look at as well as not visualizing big sets and just listing them out in an organized fashion.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>E-Commerce will only continue to grow world around the world, not just in Brazil and the United States. With the constant exponential growth and possibility for expansions being able to identity and eventually predict when and where warehouse should be places as well as where they should be place will help not only in day-to-day functions but in times of duress. The analysis and methods done had good intentions but did not achieve the desired result.&lt;/p>
&lt;p>The problems with the methodology and the analysis are the fact that the scope of the data that each graph was trying to visualize was just far too large and not organized inherently to accommodate the smaller scopes of data that was needed in order to perform a helpful analysis that could bring anything meaningful to observe. This in itself is a lesson about how to handle big data.&lt;/p>
&lt;p>With accurate and precise data analysis it can be said that we can improve the logistical capability of all shipping company and companies that ship things. With improve logistics most items can be move more efficiently and consistently to customers with items that are order frequently. With the extra efficiency, if allocated correctly, could be used as a preemptive measure to allow for emergency supplies to always be able to be distributed on the supply chain regardless of the circumstance.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Navarro, José Gabriel. &amp;ldquo;Topic: E-Commerce in Brazil.&amp;rdquo; Statista, &amp;lt;www.statista.com/topics/4697/e-commerce-in-brazil/&amp;gt;.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Olist. &amp;ldquo;Brazilian E-Commerce Public Dataset by Olist.&amp;rdquo; Kaggle, 29 Nov. 2018, &amp;lt;www.kaggle.com/olistbr/brazilian-ecommerce&amp;gt;.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Rank Forecasting in Car Racing</title><link>/report/fa20-523-349/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-349/project/project/</guid><description>
&lt;h1 id="rank-forecasting-in-car-racing">Rank Forecasting in Car Racing&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-349/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-349/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-349/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-349/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;ul>
&lt;li>Jiayu Li, fa20-523-349&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-349/blob/master/project/project.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The IndyCar Series is the premier level of open-wheel racing in North America. Computing System and Data analytics is critical to the game, both in improving the performance of the team to make it faster and in helping the race control to make it safer. IndyCar ranking prediction is a practical application of time series problems. We will use the LSTM model to analyze the state of the car, and then predict the future ranking of the car. Rank forecasting in car racing is a challenging problem, which is featured with highly complex global dependency among the cars, with uncertainty resulted from existing exogenous factors, and as a sparse data problem. Existing methods, including statistical models, machine learning regression models, and several state-of-the-art deep forecasting models all perform not well on this problem. In this project, we apply deep learning methods to racing telemetry data. And compare deep learning with traditional statistical methods (SVM, XGBoost).&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#structure-of-the-log-file">Structure of the log file&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#data-preprocessing">Data preprocessing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#feature-selection">Feature selection&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-inference">5. Inference&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Time series forecasting, deep learning.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Indy500 is the premier event of the IndyCar series&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Each year, 33 cars compete on a 2.5-mile oval track for 200 laps. The track is split into several sections or timeline. E.g., SF/SFP indicate the start and finish line on the track or on the pit lane, respectively. A local communication network broadcasts race information to all the teams, following a general data exchange protocol.
We aim to predict the leading car in the future through telemetry data generated in real time during the race. Given a prediction step t_p, and a time point t_0 in the game, we predict the following two events:&lt;/p>
&lt;ol>
&lt;li>Whether the currently leading car continue to lead at time t_0 + t_p.&lt;/li>
&lt;li>Which car is the leading car at time t_0 + t_p.&lt;/li>
&lt;/ol>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>In many real-world applications, data is captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them.
The traditional statistical learning model (Naive Bayes, SVM, Simple Neural Networks) is difficult to deal with the problem of time series prediction, since the model is unable to understand the time-series dependence of data. Traditional time series prediction models such as autoregressive integrated moving average (ARIMA) can only deal with linear time series with certain periodicity. The anomaly events and human strategies in the racing competition make these methods no longer applicable. Therefore, time series prediction models (RNN, GRU, LSTM, etc.) based on deep learning are more suitable for solving such problems.
Previous racing prediction attempts such as &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> could not make real-time predictions because the data they used was based on Lap, that is, new data would only be generated when the car passed a specific position. And we will try to use high-frequency telemetry data to make predictions.&lt;/p>
&lt;p>This article is different from &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> in at least three points:&lt;/p>
&lt;ol>
&lt;li>The resolution of the data is different. This article uses telemetry data. The telemetry data generates 7 to 8 data points per second. After preprocessing, we sample the data to 1 data point per second. The data used in &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> is based on &amp;ldquo;Lap&amp;rdquo;, that is, the new data will only be recorded when the car passes the starting point.&lt;/li>
&lt;li>The prediction model is different. This article uses LSTM for prediction, while &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> uses DeepAR-based models for prediction.&lt;/li>
&lt;li>The definition of ranking is different. What this article predicts is: Given a certain time t, predict which car will lead at t+tp. And the output of &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> is: predict the rank of each car to complete the nth lap. One is to predict the position of the car in space at a given time; the other is to give a spatial position (usually the starting point of the track), and then predict the time for the car to pass that position.&lt;/li>
&lt;/ol>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>There are two main sources of data: One is the game record from 2013 to 2019. The other is telemetry data for 2017 and 2018.&lt;/p>
&lt;p>The race record only includes the time spent in each section and does not include the precise location of every two cars at a certain point in time.
Telemetry data is a high-resolution data, each car will produce about 7 records per second, we use telemetry data to estimate the position of each car at any time.
In order to expand the training data set, we used interpolation to convert ordinary race records into a time series of car positions.
If we assume that the speed of the car within each section does not change, then the position of the car at time T can be calculated as follows:
LapDistance(T) \approx L \frac{T-T_1}{T_2 - T_1} . T_1 and T_2 are the start and end time of the current section. L=2.5 miles is the length of the section.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-349/main/project/images/Untitled6.png" alt="alt text">&lt;/p>
&lt;p>&lt;strong>Table 1&lt;/strong> : Indy 500 data sets&lt;/p>
&lt;h3 id="structure-of-the-log-file">Structure of the log file&lt;/h3>
&lt;p>The Multi-Loop Protocol is designed to deliver specific dynamic and static data that is set up and produced by the INDYCAR timing system. This is accomplished by serially streaming data that is broken down into different record sets. This information includes but is not limited to the following:&lt;/p>
&lt;ul>
&lt;li>Completed lap results&lt;/li>
&lt;li>Time line passing or crossing results&lt;/li>
&lt;li>Completed section results&lt;/li>
&lt;li>Current run or session information&lt;/li>
&lt;li>Flag information&lt;/li>
&lt;li>Track set up information including segment definitions&lt;/li>
&lt;li>Competitor information&lt;/li>
&lt;li>Announcement information&lt;/li>
&lt;/ul>
&lt;p>The INDYCAR MLP is based on the AMB Multi-Loop Protocol version 1.3. This document contains the
INDYCAR formats for specific fields not defined in the AMB document.&lt;/p>
&lt;p>Record Description:&lt;/p>
&lt;p>Every record starts with a header and ends with a CR/LF. Inside the record, the fields are separated by a &amp;ldquo;broken bar&amp;rdquo; symbol 0xA6 (not to be confused with the pipe symbol 0x7C). The length of a record is not defined and can therefore be more than 256 characters. The data specific to each record Command is contained between the header and CR/LF.
&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-349/main/project/images/Untitled8.png" alt="alt text">&lt;/p>
&lt;p>&lt;strong>Figure 1&lt;/strong> : Indy 500 track map&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;h3 id="data-preprocessing">Data preprocessing&lt;/h3>
&lt;p>There are two main sources of data: One is the game record from 2013 to 2019. The other is telemetry data for 2017 and 2018.&lt;/p>
&lt;p>The race record only includes the time spent in each section and does not include the precise location of every two cars at a certain point in time.
Telemetry data is a high-resolution data, each car will produce about 7 records per second, we use telemetry data to estimate the position of each car at any time.
In order to expand the training data set, we used interpolation to convert ordinary race records into a time series of car positions.
If we assume that the speed of the car within each section does not change, then the position of the car at time T can be calculated as follows:
LapDistance(T) \approx L \frac{T-T_1}{T_2 - T_1} . T_1 and T_2 are the start and end time of the current section. L=2.5 miles is the length of the section.&lt;/p>
&lt;p>The preprocessing mainly includes 3 operations.&lt;/p>
&lt;ol>
&lt;li>Stream data interpolation. In order to expand the training data set, we used interpolation to convert ordinary race records into a time series of car positions.&lt;/li>
&lt;li>Data normalization, scale the input data to the range of -1 to 1.&lt;/li>
&lt;li>Data sorting. Due to the symmetry of the input data, that is, any data exchanged between two cars can still get a legal data set. Therefore, a model with more parameters is required to learn this symmetry. In order to avoid unnecessary complexity, we sort the data according to the position of the car. That is, the data of the current leading car is placed in the first column, the data of the currently ranked second car is placed in the second column, and so on. This helps to compress the model and improve performance.&lt;/li>
&lt;/ol>
&lt;h3 id="feature-selection">Feature selection&lt;/h3>
&lt;p>The future ranking of a car is mainly affected by two factors:
One is its current position: the car that is currently leading has a greater probability of being ahead in the future; the other is the time of the last pit stop: because the fuel tank of each car is limited, Entering pit stop, the possibility of it leading in the future will be reduced.&lt;/p>
&lt;p>Therefore, we choose the following characteristics to predict ranking:&lt;/p>
&lt;ul>
&lt;li>The current position of each car (Lap and Lap Distance)&lt;/li>
&lt;li>Time of the last Pit Stop of each car&lt;/li>
&lt;/ul>
&lt;p>Time series prediction problems are a difficult type of predictive modeling problem. Unlike regression predictive modeling, time series also adds the complexity of a sequence dependence among the input variables. A powerful type of neural network designed to handle sequence dependence is called recurrent neural networks. The Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained.
&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-349/main/project/images/Untitled5.png" alt="alt text">&lt;/p>
&lt;p>&lt;strong>Figure 2&lt;/strong> : Work flow and model structure&lt;/p>
&lt;h2 id="5-inference">5. Inference&lt;/h2>
&lt;p>Table shows the experimental results, which verify our hypothesis that the time series prediction model based on deep learning obtained the highest accuracy. Although the LSTM model achieves the highest accuracy, its advantages are not as obvious as RankNet. This is because the telemetry data of racing cars is non-public, and the data available for training are limited.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-349/main/project/images/Untitled4.png" alt="alt text">&lt;/p>
&lt;p>&lt;strong>Table 2&lt;/strong> : LSTM model parameters&lt;/p>
&lt;p>According to the experimental results in Table 6, we draw the following conclusions:&lt;/p>
&lt;ol>
&lt;li>The LSTM model has higher accuracy in time series forecasting.&lt;/li>
&lt;li>Limited by the size of the training data set (only the telemetry data for 2 games is available), the accuracy improvement obtained by LSTM is not as obvious as RankNet.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-349/main/project/images/Untitled7.png" alt="alt text">&lt;/p>
&lt;p>&lt;strong>Table 3&lt;/strong> : Model accuracy comparison&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>The prediction problem of racing cars has the characteristics of non-linearity, non-periodicity, randomness, and timing dependence. The traditional statistical learning model (Naive Bayes, SVM, Simple Neural Networks) is difficult to deal with the problem of time series prediction, since the model is unable to understand the time-series dependence of data. Traditional time series prediction models such as ARMA / ARIMA can only deal with linear time series with certain periodicity. The anomaly events and human strategies in the racing competition make these methods no longer applicable. Therefore, time series prediction models (RNN, GRU, LSTM, etc.) based on deep learning are more suitable for solving such problems.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>IndyCar Dataset. &lt;a href="https://racetools.com/logfiles/IndyCar/">https://racetools.com/logfiles/IndyCar/&lt;/a>. visited on 04/15/2020&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>M4 Competition. &lt;a href="https://forecasters.org/resources/time-series-data/m4-competition/">https://forecasters.org/resources/time-series-data/m4-competition/&lt;/a>.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>C. L. W. Choo. Real-time decision making in motorsports: analytics forimproving professional car race strategy, PhD Thesis, MassachusettsInstitute of Technology, 2015.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>T. Tulabandhula.Interactions between learning and decision making.PhD Thesis, Massachusetts Institute of Technology, 2014.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Peng B, Li J, Akkas S, Wang F, Araki T, Yoshiyuki O, Qiu J. Rank Position Forecasting in Car Racing. arXiv preprint &lt;a href="https://arxiv.org/abs/2010.01707/">https://arxiv.org/abs/2010.01707/&lt;/a>.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Change of Internet Capabilities Throughout the World</title><link>/report/fa20-523-334/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-334/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-334/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-334/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-334/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-334/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Matthew Cummings, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-334/">fa20-523-334&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-334/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>In 2050 the United Nations is projecting that 90% of the world will have access to the internet. With the recent pandemic and the shift to most things being online we see how desperate people need internet to be able to do everyday tasks. The internet is a valuable utility and more people are getting access to it every day. We also are seeing more data is being sent over the internet with more than 24,000 Gigabytes being uploaded and processed per second across the entire internet. In this report we look at the progression of the internet and how it has changed over the years.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-and-current-works">2. Background and Current Works&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-data-within-internet">4. Data Within Internet&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-data-analysis-of-internet-change">4.1 Data Analysis of Internet Change&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-americas-internet-history">5. America’s Internet History&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51--americas-population-data">5.1 America’s Population Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-how-america-started-the-network-and-government-help">5.2 How America Started the Network and Government Help&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53--national-science-foundation-involvement">5.3 National Science Foundation Involvement&lt;/a>&lt;/li>
&lt;li>&lt;a href="#54-present-geographical-challenges-for-spread-of-internet-within-america">5.4 Present Geographical Challenges for Spread of Internet Within America&lt;/a>&lt;/li>
&lt;li>&lt;a href="#55-summary-of-americas-internet">5.5 Summary of America’s Internet&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-africas-struggles-to-get-continent-wide-internet">6. Africa’s Struggles to get Continent Wide Internet&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-how-did-africa-get-their-internet">6.1 How did Africa get their Internet?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-africas-geographical-problems">6.2 Africa’s Geographical Problems&lt;/a>&lt;/li>
&lt;li>&lt;a href="#63-current-trends-and-future-for-africa">6.3 Current Trends and Future for Africa&lt;/a>&lt;/li>
&lt;li>&lt;a href="#64-summary-of-africas-internet">6.4 Summary of Africa’s Internet&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#71-limitations">7.1 Limitations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#72-future-work">7.2 Future Work&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#8-acknowledgements">8. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> internet, internet development, progression of internet, population, data analysis, big data, government&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Everyday people throughout the world connect to the internet with speeds never before seen. The internet has not always been this way and for some countries they are still not at the same speeds. The internet started out to be a slow connection of one computer to the other with large machines helping pass the data. This quickly changed to become a vast network of computers all connected to one another and using packet processing systems to transport data. With the internet seen as a new and important technology governments and companies soon started their own development of networks and expansions. These expansions and networks would be the ground work for what we call the internet today.&lt;/p>
&lt;p>With the United Nations projecting that 90% of the entire population will have internet in 2050 and currently only 50% of the entire population &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> this work will look at how we started this movement and in what areas we need to improve on. Currently about 87% of American’s have access to the internet and use it daily while other countries like Chad only have 6.7% of their population using the internet. This can is from the vast resources America used to expand their networks and create the ideal internet connection that other countries strive to have. While other underdeveloped countries are trying to catchup and build their own infrastructure in the modern age, first world countries, like America, are expanding their networks to be better and more reliable. &lt;strong>Figure 1&lt;/strong> shows the current status of the percentage of the population in each country that has internet. Dark blue color is the best with greater than 70% of their population having internet access while the lighter blue is countries whose population is less than 18%.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-334/raw/main/report/images/WorldPresent.png" alt="Present World Population">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Shows the current percentage of the world’s population that has/uses internet within each country &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="2-background-and-current-works">2. Background and Current Works&lt;/h2>
&lt;p>In 1965 Thomas Merrill and Lawrence G. Roberts started the first ever wide-area computer network ever built. The internet first started out as huge machines that were sizes of small houses that were only capable of transferring small amounts of data or packets as they soon invented. The internet at the time was not even called internet but the Advanced Research Projects Agency Network (ARPANET). This Network backed by the U.S. Department Of Defense used node-to-node communication to send a messages. The first test was a simple test of sending the message of &lt;em>LOGIN&lt;/em> from one computer to the other. It crashed after &lt;em>LO&lt;/em> was sent. Following this devastating start improvements were made with the capabilities of that these computers could perform. With data now being sent throughout the network researchers needed to develop a standard on how packets should be sent. This is when the transmission control protocol and internet protocol (TCP/IP) was invented. It was soon adopted into the APARTNET in 1983 and became the standard on how computers should send and process data &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. TCP/IP is how packets are sent all over the internet. This system uses the packet-switched network where information is broken into packets and sent to different routers, the IP section of the system, and following the packets it was then put back together on the receiving end and resembled into what it was originally, TCP &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>When the internet was developed there was many different communities and the growth of these communities brought problems. There was no one group or organization that organized these groups and all these communities were on different platforms and areas. This is when the Internet Activities Board (IAB) and Tim Berners-Lee from MIT came together to develop the World Wide Web (WWW) and its primary community of World Wide Web Consortium (W3C). W3C is now the primary group who make protocols and standards for the WWW. This group is still actively watching and helping the WWW to make sure it is growing steadily and supported throughout the internet &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>To compare countries internet usage and how many people in each country use the internet two datasets will be used. These datasets will look at the percentage of the population that has internet access &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> and the percentage of the population that use smartphones &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> as that is another way people can access the internet. Once the data has been analyzed we looked at why the data is like this and how the data has changed to the way it is now.&lt;/p>
&lt;h2 id="4-data-within-internet">4. Data Within Internet&lt;/h2>
&lt;p>The internet has progressed largely from not being able to send a simple message like &lt;em>LOGIN&lt;/em> from one computer to another to being able to process terabytes of data. We are now seeing 24,000 gigabytes per second being passed and uploaded throughout the internet &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. To give perspective on this 1 gigabyte can hold up to 341 average sized digital pictures, or one megabyte is equal to 873 plain text pages and there are 1000 megabytes in 1 gigabyte. That is 873,000 pages of plain text per gigabyte and the internet is sending and uploading about 24,000 gigabytes per second, that is 24,952,000,000 pages of plain text pages being sent over the internet. This large quantity of data and information is passed throughout the internet every second and usually without any hiccups or data issues. This data and information has not always been here &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Most of it is relatively new with 90% of most data on the internet being created post 2016. This data is still growing and will continue to grow. With more than 4.4 billion people on the internet all these users are pumping more data and this data is being passed around other users. With 7.5 billion people on earth almost 60% of people on earth are using the internet and are contributing to the amount of data on it. This change has not been a slow change but an explosion of change and new users. In 2014 there was only 2.4 billion users on the internet. Within 5 years we see a growth of 2 billion users and with that we see an ever increasing amount of data being sent and uploaded &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Some of this data is being stored and used throughout the world every day and we have companies like Google, Facebook, Amazon, and Microsoft who store this data. Between them it is estimated that they store 1.2 million terabytes of internet data. Just looking at these 4 companies we see the magnitude of the data that is being stored and kept throughout the internet and each day these numbers grow &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. To give some perspective on how large that data storage is 1 Terabyte is 1000 gigabytes of data. Cisco even estimates that there has been over 1 zettabyte of data created and uploaded on the internet in 2016, zettabyte is 1000 exabytes an exabyte is 1000 petabytes and a petabyte is 1000 terabytes &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. That is a lot of data and information being sent and passed throughout the internet. It is also estimated that in 2018 we have reached 18 zettabytes. We see the growth of this data being passed throughout the internet in 2 years from being 1 zettabyte to being 18. This growth will not stop or slow down but continue to expand and grow as it is being predicted to be about 175 zettabytes within the year 2025. &lt;em>That is enough information to store on DVDs that can circle the earth 222 times&lt;/em> &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. Now this data is not all being stored but just uploaded or sent to other users. With an overwhelmingly number of this data coming from the social media companies and posts people are sending to each other. We are also seeing the growth within searches with Google getting over 3.5 Billion searches every day. We will see how this data came to be and how the internet slowly took over the world.&lt;/p>
&lt;h3 id="41-data-analysis-of-internet-change">4.1 Data Analysis of Internet Change&lt;/h3>
&lt;p>Using the datasets &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> we can analyze how this transition took place throughout the world and how the internet progressed. Looking at 2017 data we see that all first world countries have more than 70% of their population using the internet but other countries are still struggling. Looking at the progression of the internet we see that America started off the strongest and most explosive growth out of all countries and has maintain that growth while other countries and continents have fallen behind. We will look at how the internet started so strong in America and how it has teetered in other parts of the world.&lt;/p>
&lt;h2 id="5-americas-internet-history">5. America’s Internet History&lt;/h2>
&lt;p>Looking at the 1990’s data we see that America is at .79% of the population having internet while the entire world as a whole is less than .0495%. The only countries who are close to America is Norway, Canada, Sweden, Finland, and Australia. These countries populations are all less than American’s percentage with Norway the closest at .7%. &lt;strong>Figure 2&lt;/strong> shows these percentages and how drastic these differences are with some countries having 0% of their entire population having access to the internet &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. How is America and these five countries populations have so many more users using the internet compared to the rest of the world?&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-334/raw/main/report/images/World1990.png" alt="World Population in 1990">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> This figure shows the 1990’s current population % of people accessing the internet &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="51--americas-population-data">5.1 America’s Population Data&lt;/h3>
&lt;p>With the WWW being developed and deployed within America it makes sense that America would be the first country to expand their internet and have a larger portion of their population using the internet over the rest of the world. The dramatic difference of America having .79% of their population over the worlds .0495%, America was able to drastically take advantage of their early start and develop it to their needs. When looking at the world’s population with internet compared to America’s, American population with internet accounts for more than half of the world’s population with internet, with America’s population being 250 million in 1990’s and .79% of their population having internet that means 1.975 million people had internet in America while only 2.614 million globally had internet &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. America at the time was a leader in the world on internet and it is interesting how it got there.&lt;/p>
&lt;h3 id="52-how-america-started-the-network-and-government-help">5.2 How America Started the Network and Government Help&lt;/h3>
&lt;p>With the development of APARTNET within the United States the Government saw the opportunity to help expand it within its own country. The U.S. Government helped the expansion of the internet by financing contracts and building satellite sites across the world to gain access to the internet. These sites where built mostly in military bases across the world and helped countries tap into the internet without the huge cost of infrastructure . The Department of Defense (DOD) also saw an opportunity in the new Internet with their creation of MILNET. MILNET connected numerous military compounds and their computers to the United States main hub. These connections used the APARTNET but were able to disconnect and use its own network if it became compromised. These connections made it able for the internet to spread to different countries since the United States military is set up in so many countries and have bases all over the world. Most of these connections were prioritized in the United States allied countries like Norway, Australia, Canada, and EU hence why these countries have a large population using their internet in 1990, &lt;strong>Figure 2&lt;/strong>. All the infrastructure was already set up because of the military bases and the satellites the United States built for it. These countries took advantage of it and connected their people to the internet for the fraction of the cost &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="53--national-science-foundation-involvement">5.3 National Science Foundation Involvement&lt;/h3>
&lt;p>The internet also progressed quickly to these countries and throughout the United States population because of the use of super computers which helped the connections and speed of the internet. The Reagan administration saw an important need to develop these super computers that could broaden the connection and networks of the computers. National Science Foundation (NSF) had a super computer already on their network but with the help of the administration they were able to expand that network to all internet users. The new network that NSF developed was the NSFNET. This network connected academic users to the super computer and those computers to other computers. Creating this intertwined network that was all able to use the speed and power of a super computer. It expanded the speed to be 1.5 megabits per second and would replace the APARTNET network which only could handle 50 kilobytes per second. Because of NSFNET we saw an explosion of growth within the internet with it growing over 10% each month in 1990. This is why United States had so many more users than any other country because they were able to use this faster and better network. NSF saw the need to expand elsewhere and used the NSFNET to start connecting the world globally. Using the pre-installed infrastructure of the old Satellite internet they were able to connect countries that already had internet with ease. This is why the American allies were able to have so many of their own population on the internet when comparing it to the world &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="54-present-geographical-challenges-for-spread-of-internet-within-america">5.4 Present Geographical Challenges for Spread of Internet Within America&lt;/h3>
&lt;p>One of the largest challenges with the spread of the internet in the United States is how vast the country is. With a size of 4 million miles^2 it was a challenge to reach everyone within the United States with internet. With an also ever increasing size of rural living more people are living outside of cities were it can be harder to get internet. About 3% of American’s living in urban areas lack access to broadband but comparing that to the rural we see that 35% the population lack it. That is 20 million American’s that do not have access to high speed broadband or internet because of where they live. The reason for why rural users lack the broadband capabilities is that the cost to run high speed internet to those areas is too much and the high speeds cannot reach that far from the hubs &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. When comparing these rural areas to urban areas we also see that urban areas almost always have more than three options for broadband while rural areas can be limited. Based on &lt;strong>Figure 3&lt;/strong> we see that all of the urban centers have access to these high speed broadbands and have options for what type of company they want. While rural areas struggle to have more than 2. .12% of rural areas have 0 access to any company that give broadband while only .01% of urban areas have 0 &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. To help the spread of internet to these rural areas the Government has decided to help again. Recently the Government has signed in the Broadband Data Act which will help identify the rural areas that need help and how they can fix the issues at hand. Congress has also packaged in 100 million to help this digital divide and try and better the situation. This process was expedited because of the recent pandemic and how desperate people are to get on the internet &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-334/raw/main/report/images/AmericaBroadband.jpg" alt="America’s BroadBand">&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-334/raw/main/report/images/LegendAmerica.jpg" alt="America’s Legend">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> This image shows the broadband capabilities throughout the nation. The legend below the picture depicts what each color means. We see from this chart that all urban areas have more than 12 high speed broadband providers will rural areas mostly have 1-2 options. This is the challenge for rural America when they are stuck with one option and that option can sometimes not even provide the fastest internet and will also be expensive &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="55-summary-of-americas-internet">5.5 Summary of America’s Internet&lt;/h3>
&lt;p>Without the direct involvement of the United States Government the internet would have had a slower progression than the one we are seeing today. Without the military and the NSF the internet would not have been the same and could have looked a lot different. We would not have had the same widespread start without all the military bases having all the required infrastructure for the internet and we would not have had the required needs for the internet because of the vast contracts that the military supplied. This is how America was able to be so far ahead of the rest of the World and why the American allies were able to also build up their own infrastructure and take advantage of the infrastructure supplied by America. We also would have had a lot slower internet if it was not for America and the connection of the super computers on the networks.&lt;/p>
&lt;h2 id="6-africas-struggles-to-get-continent-wide-internet">6. Africa’s Struggles to get Continent Wide Internet&lt;/h2>
&lt;p>Looking at the data within Data World Bank we can see the vast change within countries. Some countries populations double how many people are using the internet each year. One region to look at specifically is Africa. Africa did not have each country within it have internet until 2000. That is 20 years after the start and development of the internet. When looking at the entire population of Africa only 16.18% of the them had internet access or used internet access. Comparing this America’s 43.1% and Canada’s 51.3% we see a huge disparity here. We also see that the Average population that uses internet in Africa being .385%. This is dramatically low but given that Africa is an underdeveloped country and has had lots of struggles makes sense.&lt;/p>
&lt;h3 id="61-how-did-africa-get-their-internet">6.1 How did Africa get their Internet?&lt;/h3>
&lt;p>It started very early with Africa getting the first computer in 1980’s. This computer started how their network would work and how they would setup their own infrastructure. Most of the universities in Africa were the ones who lead the new technology age for Africa. These universities became the hotspots for computing and the internet. Africa also got some major help from the Internet Society &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. The Internet Society is a nonprofit organization that helps people connect to the internet and can help countries setup their own networks &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. In 1993 the Internet Society held a large workshop that helped a lot of underdeveloped countries connect to the internet and teach them all how to supply their population with this critical utility. Each following year the workshop hosted a new event which grew in size and over 447 citizens attended it who were from African Countries. This helped bring the discussion how Africa can setup their network and get their people the vital resources they need &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. Africa has also gotten help from other countries to help develop their infrastructure and get their internet systems going. On major help was America’s USAID LELAND initiative which supplied Africa 15$ million to help develop their infrastructure. This agreement made it possible for African countries to develop primary connections to their own networks from USA’s high speed network. With this deal that started in 1996 and ended in 2001 we see a large spike in the populations and countries who finally have access to internet &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. With the help of this resource Africa was able to start developing their own network organization for all of Africa called AfriNIC. AfriNic is similar to America’s IAB were they help to make sure the networks are open to everyone within Africa and that if there are issues they are solved. Africa began to develop into the modern age of the internet with this and started to catch up to others &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="62-africas-geographical-problems">6.2 Africa’s Geographical Problems&lt;/h3>
&lt;p>Africa’s geography is causing similar problems to their internet distribution just like America’s. Africa is a ginormous continent with an area of 11.73 million miles^2. With that much land it can be hard to cover this entire continent with internet and provide people in rural areas with internet. About one third of the population is out of reach of all mobile broadband reach and can not get access to any internet. To cover this land Africa needs to invest an more infrastructure to cover this land with 250,000 base stations and 250,000 kilometers of fiber. Africa’s population is spread out throughout the whole country, based on &lt;strong>Figure 4&lt;/strong> we see all the connections and networks throughout Africa there are. It is estimated that 45% of African’s are to far away from any fiber network to connect to the internet. These areas with no networks or infrastructure are areas with low populations and not hotspots or major cities. This problem might not be solved for a while as companies will focus on hotspots rather than helping people in need to access the internet. One of the more recent developments that might help Africa is the SpaceX Satellite internet &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>. This can help them as they wont need the vast infrastructure to create the network and the connection will be able to reach all over the continent. Africa’s geography limits people connection based on where they live and if they are near the water to connect the submerged connections. With more investment and new inventions we could see this geographical challenge tackled within 20 years.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-334/raw/main/report/images/AfricaGeo.png" alt="Africa Geography">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Seeing the hotspots throughout the continent and where the internet access is shows the disparity of rural vs urban living. Hotspots of populations are targeted more for internet connection as it makes sense for companies financially to target those areas. This creates a large divide for areas inside the continent who do not have networks or any chance of connecting to the networks. This figure shows how this disparity is present within Africa.&lt;/p>
&lt;h3 id="63-current-trends-and-future-for-africa">6.3 Current Trends and Future for Africa&lt;/h3>
&lt;p>Looking at the recent data for World Data Bank we see that African countries have been having a comeback with the amount of people using the internet. Currently Africa has seen a large spike in people using the internet. This can be because of all these programs that have helped start its program but it is can also be from two-thirds of the population having phones that can connect to the internet. Africa still has a problem with less than 50% of its entire population having access to internet and a computer. There are still a lot of struggling countries like Niger that only have 5.3% of it is population that use and connect to the internet. Within this day and age having access to the internet is almost critical to survive and be a part of the world. With only 5.3% of their population having internet they are still struggling to have access for everyone. Comparing this to current America with 83% of the population having internet Africa is still far behind the curve of first world countries. This lack of internet has created an opportunity of some people to help or take advantage of Africa’s lack of infrastructure. China’s Company Huawei has agreed to build the first ever 5G network within Africa. This network will help the current population of phone users to connect to higher speeds and see a great increase in use of the internet. American countries Vanu and Parallel have also been tackling this issue with new network plans and innovative ideas to help Africa’s internet networks expand to the vast region. With a projected $160 Billion annual cost to develop and maintain a country wide infrastructure a lot of people believe they need more than just companies to help them &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. With no more help from outside countries it looks grim for Africa’s continued growth within the internet unless more companies come try to develop their own system and networks. Looking at &lt;strong>Figures 5&lt;/strong> we see how African countries have had a much slower progression towards internet when compared to other countries, &lt;strong>Figure 6&lt;/strong> shows America’s progression. &lt;strong>Figure 7&lt;/strong> also depicts the current status of Africa and how most countries within Africa are still below 18% of their population using internet within the current day and age.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-334/raw/main/report/images/AfricaChange.png" alt="Africa % Change">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Seeing these countries all mostly below 50% while major countries are over 70%-80% depicts how far behind Africa is within the internet. We can also see the change and growth of the internet within these countries and compare it to other countries. Comparing it to America’s growth they are nowhere near as explosive or close to being the current rate of America’s internet growth &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-334/raw/main/report/images/AmericaChange.png" alt="America % Change">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> America’s growth within the internet &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-334/raw/main/report/images/AfricaPresent.png" alt="Africa Current">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> This is the current status of Africa’s internet population percentage. Notice that most countries within Africa are less than 18% while the world population % is greater than 50%. This depicts how far behind Africa is with building their infrastructure &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="64-summary-of-africas-internet">6.4 Summary of Africa’s Internet&lt;/h3>
&lt;p>With less help from outside countries Africa is almost on their own with decided how they should improve their Internet Capabilities. Companies are see this as an opportunity to develop and use their own technologies to help Africa with their issues. These companies will develop a new network to try and get the entire country connected and online. This could be a problem were Africa is not entirely in control of their internet and data but with no outside help from any other countries they might have to take the best options available and go with these companies. With the help of Internet Society and AfriNET Africa has already started developing programs and networks to connect the country but with the infrastructure cost being too high they still need help. These organizations could help maintain oversite within this work and make sure Africa does not get taken advantage of. As the world progresses to complete internet involvement Africa is still far behind the rest.&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>After viewing the data and analyzing trends the team saw the large differences between countries with who had an early start within the area of internet development and those countries who are far behind. Analyzing the reason behind this we see that countries who had access to the internet early and were able to take advantage of their early start were able to have a much larger population percentage than the countries who did now. We also see that some countries would get help from outside countries but when those contracts expired they were left out to dry. Seeing America and the vast amount of allies it had in 1990’s-2000’s with their population with access to internet quadrupling those in less developed countries is staggering but when looking behind the data and the reason why we see that some countries are just not as equally equipped and don’t have the infrastructure to compete with other countries networks. With 90% of the population being projected to have access to the internet it will be interesting to see the change in these areas. As we get closed to that 90% it will be these less developed countries who will make staggering changes within their percentage of population compared to the other developed countries.&lt;/p>
&lt;h3 id="71-limitations">7.1 Limitations&lt;/h3>
&lt;p>With only one large dataset that had all the population sizes and percentages it can be difficult to check the accuracy of the data within the set. The datasets also contained a numerous amount of null data and/or incomplete data for a vast majority of countries. This hindered the ability to further look at the correct trends and analysis of the dataset. With also having not a vast knowledge of data analysis within data the team was not able to analyze the datasets for the planned project.&lt;/p>
&lt;h3 id="72-future-work">7.2 Future Work&lt;/h3>
&lt;p>The team will continue to analyze these datasets and build their own programs to look see the different trends within these areas and countries. The team wants to keep working on the reason why they trends are happening and how these trends started. It is important to understand the reason behind the data and factors that lead to these data points. As the team progresses through the dataset, the team will continue to understand the factors and reasons within the data.&lt;/p>
&lt;h2 id="8-acknowledgements">8. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Figures used from The Data World Bank. W. Bank, &amp;ldquo;Individuals using the Internet (% of population),&amp;rdquo; Data, 2017. [Online]. Available: &lt;a href="https://data.worldbank.org/indicator/IT.NET.USER.ZS?most_recent_value_desc=true.">https://data.worldbank.org/indicator/IT.NET.USER.ZS?most_recent_value_desc=true.&lt;/a> [Accessed: 07-Oct-2020].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>B. Leiner, V. Cerf, D. Clark, R. Kahn, L. Kleinrock, D. Lynch, J. Postel, L. Roberts, and S. Wolff, &amp;ldquo;Brief History of the Internet,&amp;rdquo; Internet Society, 14-Aug-1997. [Online]. Available: &lt;a href="https://www.internetsociety.org/internet/history-internet/brief-history-internet/.">https://www.internetsociety.org/internet/history-internet/brief-history-internet/.&lt;/a> [Accessed: 07-Nov-2020].&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>B. Company, &amp;ldquo;TCP/IP,&amp;rdquo; Encyclopædia Britannica, 2018. [Online]. Available: &lt;a href="https://www.britannica.com/technology/TCP-IP.">https://www.britannica.com/technology/TCP-IP.&lt;/a> [Accessed: 15-Nov-2020].&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>W. Bank, &amp;ldquo;Individuals using the Internet (% of population),&amp;rdquo; Data, 2017. [Online]. Available: &lt;a href="https://data.worldbank.org/indicator/IT.NET.USER.ZS?most_recent_value_desc=true.">https://data.worldbank.org/indicator/IT.NET.USER.ZS?most_recent_value_desc=true.&lt;/a> [Accessed: 07-Oct-2020].&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>W. Bank, &amp;ldquo;Mobile cellular subscriptions (per 100 people),&amp;rdquo; Data, 2017. [Online]. Available: &lt;a href="https://data.worldbank.org/indicator/IT.CEL.SETS.P2?most_recent_value_desc=true.">https://data.worldbank.org/indicator/IT.CEL.SETS.P2?most_recent_value_desc=true.&lt;/a> [Accessed: 07-Oct-2020].&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>J. Schultz, &amp;ldquo;How Much Data is Created on the Internet Each Day?,&amp;rdquo; Micro Focus Blog, 08-Jun-2019. [Online]. Available: &lt;a href="https://blog.microfocus.com/how-much-data-is-created-on-the-internet-each-day/.">https://blog.microfocus.com/how-much-data-is-created-on-the-internet-each-day/.&lt;/a> [Accessed: 07-Nov-2020].&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>R. Company, &amp;ldquo;Byte Size Infographic: Visualising data,&amp;rdquo; redcentric, 03-Feb-2020. [Online]. Available: &lt;a href="https://www.redcentricplc.com/resources/byte-size-infographic/.">https://www.redcentricplc.com/resources/byte-size-infographic/.&lt;/a> [Accessed: 13-Nov-2020].&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>B. Marr, &amp;ldquo;How Much Data Is There In the World?,&amp;rdquo; Bernard Marr, 2020. [Online]. Available: &lt;a href="https://www.bernardmarr.com/default.asp?contentID=1846.">https://www.bernardmarr.com/default.asp?contentID=1846.&lt;/a> [Accessed: 07-Nov-2020].&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>R. E. Kahn, Revolution in the U.S. information infrastructure. Washington, D.C., DC: National Academy Press, 1995. Chapter The Role Of Government in the Evolution of the Internet [Accessed: 12-Nov-2020].&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>C. B. S. News, &amp;ldquo;The digital divide between rural and urban America&amp;rsquo;s access to internet,&amp;rdquo; CBS News, 04-Aug-2017. [Online]. Available: &lt;a href="https://www.cbsnews.com/news/rural-areas-internet-access-dawsonville-georgia/.">https://www.cbsnews.com/news/rural-areas-internet-access-dawsonville-georgia/.&lt;/a> [Accessed: 07-Dec-2020].&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>FCC, &amp;ldquo;Fixed Broadband Deployment&amp;rdquo;, FCC, 2020 [online]. Available: &lt;a href="https://broadbandmap.fcc.gov/#/area-summary?version=dec2019&amp;amp;type=nation&amp;amp;geoid=0&amp;amp;tech=acfosw&amp;amp;speed=25_3&amp;amp;vlat=39.40549184229633&amp;amp;vlon=-99.73724455499007&amp;amp;vzoom=2.987482657657667">https://broadbandmap.fcc.gov/#/area-summary?version=dec2019&amp;amp;type=nation&amp;amp;geoid=0&amp;amp;tech=acfosw&amp;amp;speed=25_3&amp;amp;vlat=39.40549184229633&amp;amp;vlon=-99.73724455499007&amp;amp;vzoom=2.987482657657667&lt;/a> [Accessed: 07-Dec-2020]. Map layer based on FCC Form 477&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>B. A. R. Association, &amp;ldquo;Expanding Broadband Access to Rural Communities,&amp;rdquo; American Bar Association, 2020. [Online]. Available: &lt;a href="https://www.americanbar.org/advocacy/governmental_legislative_work/publications/washingtonletter/march-washington-letter-2020/broadband-032020/.">https://www.americanbar.org/advocacy/governmental_legislative_work/publications/washingtonletter/march-washington-letter-2020/broadband-032020/.&lt;/a> [Accessed: 07-Dec-2020].&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Afrinic Organization, &amp;ldquo;A Short History of the Internet in Africa (1980-2000),&amp;rdquo; AFRINIC BLOG, 26-Sep-2016. [Online]. Available: &lt;a href="https://afrinic.net/blog/153-a-short-history-of-the-internet-in-africa-1980-2000.">https://afrinic.net/blog/153-a-short-history-of-the-internet-in-africa-1980-2000.&lt;/a> [Accessed: 05-Dec-2020].&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>I. Society , &amp;ldquo;History of the Internet in Africa,&amp;rdquo; Internet Society, 04-Aug-2020. [Online]. Available: &lt;a href="https://www.internetsociety.org/internet/history-of-the-internet-in-africa/.">https://www.internetsociety.org/internet/history-of-the-internet-in-africa/.&lt;/a> [Accessed: 05-Dec-2020].&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>R. Fukui, C. J. Arderne, and T. Kelly, &amp;ldquo;Africa&amp;rsquo;s connectivity gap: Can a map tell the story?,&amp;rdquo; World Bank Blogs, 07-Nov-2019. [Online]. Available: &lt;a href="https://blogs.worldbank.org/digital-development/africas-connectivity-gap-can-map-tell-story.">https://blogs.worldbank.org/digital-development/africas-connectivity-gap-can-map-tell-story.&lt;/a> [Accessed: 08-Dec-2020].&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Chat Bots in Customer Service</title><link>/report/sp21-599-355/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/sp21-599-355/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-355/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-355/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-355/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-355/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final , Type: Project&lt;/p>
&lt;p>Anna Everett, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-355/">sp21-599-355&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-355/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-355/blob/main/project/code/twitter_support_analysis.ipynb">twitter_support_analysis.pynb&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-355/blob/main/project/code/twitter_support_analysis.pdf">twitter_support_analysis.pdf&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Automated customer service is a rising phenomon for buisnesses with an online presence. As customer service bots advance in complication of problems they can handle one concern about the altered customer experiece is how the information is conveyed. Using customer support data tweets on twitter this project runs sentiment analysis on it customer tweets and then train a convolutional neural network to examine if conversation tone can be detected early in the conversation.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-procedure">2. Procedure&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-the-dataset">2.1 The Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-simplifying-the-dataset">2.2 Simplifying the Dataset&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-the-algorithm">3. The Algorithm&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-sentiment-analysis-overview-and-implementation">3.1 Sentiment Analysis Overview and Implementation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-convolutional-neural-networks-cnn">3.2 Convolutional Neural Networks (CNN)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#33-splitting-up-the-data">3.3 Splitting Up the Data&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-training-the-model">4. Training the Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-references">6. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>Introduction&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Procedure&lt;/p>
&lt;ul>
&lt;li>The Dataset&lt;/li>
&lt;li>Simplifying the dataset&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>The Algorithm&lt;/p>
&lt;ul>
&lt;li>Sentiment Analysis Overview and Implementation&lt;/li>
&lt;li>Convolutional Neural Networks (CNN)&lt;/li>
&lt;li>Spliting Up the Data&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Training the Model&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Conclusion&lt;/p>
&lt;/li>
&lt;li>
&lt;p>References&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Keywords:&lt;/strong> AI, chat bots, tone, nlp, twitter, customer service.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Please not ethat an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Some studies report that customers prefer live chat for their customer sevice interactions&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Currently, most issues are simple enough that they can be resolved by a bot; and over 70% of companies are already using or have plans to use some form of software for automation. Existing Ai&amp;rsquo;s in use are limited to simple and common enough questions and or problems that allow for generalizable communication. As AI technology develops and allows it to handle more complicated problems the communication methods will also have to evolve.&lt;/p>
&lt;p>&lt;a href="https://acquire.io/wp-content/uploads/2017/09/chat-session.png">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-355/raw/main/project/images/live_chat_pref.png" alt="Customer Support Preferences">&lt;/a>
&lt;strong>Fig 1&lt;/strong>: Customer Support Preferences &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>
&lt;p>An article by Forbes &amp;ldquo;AI Stats News: 86% Of Consumers Prefer Humans To Chatbots&amp;rdquo;, states that only 30% of consumers beleve that an AI would be better at solving their problem than a human agent &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Human agents are usually prefered becuase humans are able to personalize converstaions to the indiviual customer. On the other hand, automated software allows for 24/7 assistance if needed, the scale of how many customers a bot would be able to handle is considered larger and more efficient in comparision to what humans can handle, and a significant amount of questions are simple enough to be handled by a bot &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;a href="https://www.superoffice.com/blog/live-chat-statistics/">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-355/raw/main/project/images/chat-session.png" alt="Support Satisfaction">&lt;/a>
&lt;strong>Fig 2&lt;/strong>: Support Satisfaction &lt;a href="https://www.superoffice.com/blog/live-chat-statistics/">Image source&lt;/a>&lt;/p>
&lt;p>To get the best out of both versions of service, this project uses natural language processing to analyze social media customer service conversations. This is then run through a convolutional neural network to predict if tone can be determined early in the converstaion.&lt;/p>
&lt;h2 id="2-procedure">2. Procedure&lt;/h2>
&lt;h3 id="21-the-dataset">2.1 The Dataset&lt;/h3>
&lt;p>The dataset comes from the public dataset compilation website, kaggle, and can be found at &lt;a href="https://www.kaggle.com/thoughtvector/customer-support-on-twitter">Kaggle Dataset&lt;/a>. This dataset was chosen due to twitter support&amp;rsquo;s informal nature that is expected to come with quicker and more automated customer interactions.&lt;/p>
&lt;p>The content of the data consists of over 2.5 million tweets from both customer and various companies that have twitter account representation. Each row of data consists of: the unique tweet id, an anonymized id of the author, if the tweet was sent directly to a company, date and time sent, the content of the tweet, the ids of any tweets that responded to this tweet if any, and the id of the tweet that this was sent in response to if any.&lt;/p>
&lt;h3 id="22-simplifying-the-dataset">2.2 Simplifying the Dataset&lt;/h3>
&lt;p>The raw dataset is large and contins unncessery information that isn&amp;rsquo;t needed for this porpose. In order to trim the dataset only the first 650 samples are taken.&lt;/p>
&lt;p>Next, since the project goal is to predict customer sentiment any tweet and related data sent by a company is removed. Luckily, companies author id&amp;rsquo;s don&amp;rsquo;t get anonymized and therefore we can filter those out by removing any data associated with an author id that contains letters. For speed and simplicity these author id are only checked for vowels.&lt;/p>
&lt;h2 id="3-the-algorithm">3. The Algorithm&lt;/h2>
&lt;h3 id="31-sentiment-analysis-overview-and-implementation">3.1 Sentiment Analysis Overview and Implementation&lt;/h3>
&lt;p>Sentiment analysis is the process of taking in natrual language text and determining if it has a positive or negative sentiment &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This process is useful for when doing market research and tracking attitudes towards a particualar company. In a similar fashion this project uses sentiment analysis to determine the text sentiment of the customer initially with their first inquiery and also the sentiment of their side of the conversation as a whole.&lt;/p>
&lt;p>The goal of analyzing both the first and general tone of the text is to determine if a general correlation between them can be found.&lt;/p>
&lt;p>The main library used for the sentiment analysis of the data was &amp;ldquo;nltk&amp;rdquo; and its subpackage &amp;ldquo;SentimentIntensityAnalyzer&amp;rdquo;&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-355/raw/main/project/images/customer_dist.png" alt="Customer Sentiment Distribution">
&lt;strong>Fig 3&lt;/strong>: Customer Sentiment Distribution&lt;/p>
&lt;p>As can be seen in the Fig 3 the distribution of the sentiment values are generally on a normal distribution. Looking at the binary classifications of both the first and average sentiment distribution it can be seen that while the majority can be classified as positive, 1, there&amp;rsquo;s still a significant amount that are classified as negative, 0.&lt;/p>
&lt;h3 id="32-convolutional-neural-networks-cnn">3.2 Convolutional Neural Networks (CNN)&lt;/h3>
&lt;p>While convolutional Neural Networks are traditionally used for image processing. &lt;a href="https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c">Some articles&lt;/a> suggest that CNN&amp;rsquo;s also work well for Natural Language Processing. Traditionally, convolutional neural networks are networks that apply filters over a dataset of pixels and process the pixel as well as those that surrounds it. Typically this is used for images as pictured below in Fig 4, for filtering and edge detecting for identifying objects.&lt;/p>
&lt;p>&lt;a href="https://miro.medium.com/max/1575/1*EPpYI-llkbtwHgfprtTJzw.png">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-355/raw/main/project/images/image_convolved.png" alt="Convolution visual">&lt;/a>
&lt;strong>Fig 4&lt;/strong>: Image convolution &lt;a href="https://miro.medium.com/max/1575/1*EPpYI-llkbtwHgfprtTJzw.png">Image source&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://miro.medium.com/max/1575/1*NBtZgyBC1oSuqs2SswwwbA.png">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-355/raw/main/project/images/cnn_visual.png" alt="Image convolution">&lt;/a>
&lt;strong>Fig 5&lt;/strong>: Convolution Visual &lt;a href="https://miro.medium.com/max/1575/1*NBtZgyBC1oSuqs2SswwwbA.png">Image source&lt;/a>&lt;/p>
&lt;p>CNN&amp;rsquo;s also work well for natrual language processing. Thinking about the english language, meaning and tone of a scentence or text is caused by the relation of words, rather than each word on its own. NLP through CNNs work in a similar fashion to how it processes images but instead of pixels its encoded words that are being convolved.&lt;/p>
&lt;p>As can be seen by Fig 6, this project used a multi-layered CNN: beginning with an embedding layer, alternating keras 1 dimension convolution and max pooling layers, a keras dropout layer with a rate of 0.2 to prevent overfitting of the model&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> and a keras dense layer that implements the activation function into the output &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-355/raw/main/project/images/cnn_model.png" alt="CNN model">
&lt;strong>Fig 6&lt;/strong>: CNN Model&lt;/p>
&lt;h3 id="33-splitting-up-the-data">3.3 Splitting Up the Data&lt;/h3>
&lt;p>In order for the data to be suitable to be run through the CNN the input features must be reshaped into a 2-D array. Afterwards the data was split up using the &amp;ldquo;sklearn.model_selection&amp;rdquo; package &amp;ldquo;train_test_split&amp;rdquo; with the features being input as the encoded tweets and the lables being input as the general classification sentiment.&lt;/p>
&lt;h2 id="4-training-the-model">4. Training the Model&lt;/h2>
&lt;p>The model was trained using the training text and training sentiment, because of the small samples used a batch size of 50 was input and run for 10 epochs.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>In conclusion, while sentiment prediction of customer tweets were not able to be executed there is still useful information found to aid in future attempts. In dealing with the informal raw twitter data ad performing a non-binary sentiment analysis allowed for visualization of the true spread of what can be expected when anticipating dealing with customers, especially in an informal setting such as social media. This project also brought to light the theorectial versatility of convolutional neural networks, though not further examined in this project. Using the model model fit as an indication, there is reasonable evidence that the first inquiry will be enough to predict and take proactive measures in future customer service chat bots. Future execution of this project should include altering the data pre-processing technique and using a larger set of samples from the dataset.&lt;/p>
&lt;h2 id="6-references">6. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Super Office, [online resource] &lt;a href="https://www.superoffice.com/blog/live-chat-statistics/">https://www.superoffice.com/blog/live-chat-statistics/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Forbes, [online resource] &lt;a href="https://www.forbes.com/sites/gilpress/2019/10/02/ai-stats-news-86-of-consumers-prefer-to-interact-with-a-human-agent-rather-than-a-chatbot/?sh=5f5d91422d3b">https://www.forbes.com/sites/gilpress/2019/10/02/ai-stats-news-86-of-consumers-prefer-to-interact-with-a-human-agent-rather-than-a-chatbot/?sh=5f5d91422d3b&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Acuire, [online resource] &lt;a href="https://acquire.io/blog/chatbot-vs-live-chat/">https://acquire.io/blog/chatbot-vs-live-chat/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Monkey learn, [online resource] &lt;a href="https://monkeylearn.com/sentiment-analysis/">https://monkeylearn.com/sentiment-analysis/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Keras, [online documentation] &lt;a href="https://keras.io/api/layers/regularization_layers/dropout/">https://keras.io/api/layers/regularization_layers/dropout/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: COVID-19 Analysis</title><link>/report/fa20-523-342/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-342/project/project/</guid><description>
&lt;h1 id="covid-19-analysis">COVID-19 Analysis&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-342/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-342/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final approved, Type: Project&lt;/p>
&lt;p>Hany Boles, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-342/">fa20-523-342&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-342/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>By the end of 2019, healthcare across the world started to see a new type of Flu and they called it Coronavirus or Covid-19. This new type of Flu developed across the world and it appeared there is no one treatment could be used to treat it yet, scientists found different treatments that apply to different age ranges. In this project, We will try to work on comparison analysis between USA and China on number of new cases and new deaths and trying to find factors played big roles in this spread.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data-Sets&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-preparing-covid-19-data-set">2.1 Preparing COVID 19 Data-Set&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-preparing-weather-data-set">2.2 preparing Weather Data-Set&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-methodology">3. Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-processing-the-data">4. Processing the Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion-and-future-work">5. Conclusion and Future Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgements">6. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Corona Virus, Covid 19, Health&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>While the world is ready to start 2020 we heard about a new type of the Flu that it appears to be started in China and from there it went to the entire world. It appeared to affect all ages but its severity did depend on other factors that related to age, health conditions if the patient is a smoker or not?&lt;/p>
&lt;p>This new disease attacked aggressively the respiratory system for the patient and then all the human body causing death. The recovery from this disease appeared to vary from area to area across the globe, Also the death percentage as well was and still vary from area to area. and then we decided to perform the analysis on weather temperature from one side and the covid 19 new cases and new death on the other side to see if the weather temperature plays a role or not with it.&lt;/p>
&lt;h2 id="2-data-sets">2. Data-Sets&lt;/h2>
&lt;p>After observing many datasets &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> to get a better understanding if there are common factors in the areas that have the larger number of new Covid 19 cases, we decided to proceed with the dataset provided by the World Health Organization &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> because this dateset is being updated on a daily basis and has the most accurate data. Currently it appears that we are getting a second wave of coronavirus and so we will try to get the most recent data. We were able to use Webscraping to get the data we need from the World Health Organization website which is updated daily.&lt;/p>
&lt;p>For the weather datasets, we looked at several datasets &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> and we decided to use the data provided by visualcrossing website &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This website helped us in getting the data we need which is daily average temperatures in the United States of America and China. We started to collect the data from 1/3/2020.&lt;/p>
&lt;h3 id="21-preparing-covid-19-data-set">2.1 Preparing COVID 19 Data-Set&lt;/h3>
&lt;p>We started to work on Covid 19 dataset and we found that it is better to use webscraping to gather the dataset so every time we run the python script, we will get the most recent data and then we opened the CSV file and added it to a dataframe.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/webscrap.jpg" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1&lt;/strong>: Downloading the Covid 19 dataset.&lt;/p>
&lt;p>We then filtered only on United States of America so we can get all data belong to United States of America.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/usa.jpg" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2&lt;/strong>: Capturing only USA data&lt;/p>
&lt;p>Then we made 2 seperate graphs to depict the number of new cases and the number of new deaths in the USA throughout 2020 as shown below.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/usa_new_cases_deaths1.jpg" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3&lt;/strong>: USA New Covid 19 cases and new deaths.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/china.jpg" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4&lt;/strong>: Capturing China data.&lt;/p>
&lt;p>We also made 2 seperate graphs to depict the number of new cases and the number of new deaths in China throughout 2020 as shown below.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/china_new_cases_deaths.jpg" alt="Figure 5">&lt;/p>
&lt;p>&lt;strong>Figure 5&lt;/strong>: China New Covid 19 cases and new deaths&lt;/p>
&lt;h3 id="22-preparing-weather-data-set">2.2 preparing Weather Data-Set&lt;/h3>
&lt;p>For the weather temperature dataset, we cleaned all the un-needed data from the csv file we received from Visualcrossing website &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>, then we merged the temperature column with the csv file we prepared for the covid 19, so now we have one file per country that contains Date, Temperature, New cases, new deaths.&lt;/p>
&lt;h2 id="3-methodology">3. Methodology&lt;/h2>
&lt;p>We utilized the Indiana University system to process the collected data as it will need a strong system to process it. Also, we utilized Python, matplotlib for visualization purposes, and Jupyter notebook as programming software and platform.&lt;/p>
&lt;h2 id="4-processing-the-data">4. Processing the Data&lt;/h2>
&lt;p>We started to process the final dataset we prepared and we examined the data for any correlation between the temperature and new cases for the United States of America and we found that there is no correlation there.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/usa_new_dataset.jpg" alt="Figure 6">&lt;/p>
&lt;p>&lt;strong>Figure 6&lt;/strong>: USA Dataset after merging the data.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/usacorr.jpg" alt="Figure 7">&lt;/p>
&lt;p>&lt;strong>Figure 7&lt;/strong>: Correlation between the temperature and the new covid 19 cases for the United States of America.&lt;/p>
&lt;p>Then we also processed the data for China and got the same results as the United States of America. We then looked at the analysis and in the case of the United States of America we found there is a correlation between the new covid cases and the current (cumulative) cases. On the contrary, for China we found no correlation between the new Covid cases and the current (cumulative) cases.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/usacorrheat.jpg" alt="Figure 8">&lt;/p>
&lt;p>&lt;strong>Figure 8&lt;/strong>: Heat map depicting the correlations between new cases, cumulative cases, new deaths, and cumulative deaths, respectively for the United States of America.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/chinacorr.jpg" alt="Figure 9">&lt;/p>
&lt;p>&lt;strong>Figure 9&lt;/strong>: Correlation between the new covid 19 cases and the current cases for China.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/chinacorrheat.jpg" alt="Figure 10">&lt;/p>
&lt;p>&lt;strong>Figure 10&lt;/strong>: Heat map depicting the correlations between new cases, cumulative cases, new deaths, and cumulative deaths, respectively for China.&lt;/p>
&lt;p>We started to observe more data from another country so we chose the United Kingdom, and we found a correlation between the new covid 19 cases and the current (cumulative) cases.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/uk_correlation.jpg" alt="Figure 11">&lt;/p>
&lt;p>&lt;strong>Figure 11&lt;/strong>: Correlation between the new covid 19 cases and the current cases for the United Kingdom.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/ukcorrheat.jpg" alt="Figure 12">&lt;/p>
&lt;p>&lt;strong>Figure 12&lt;/strong>: Heat map depicting the correlations between new cases, cumulative cases, new deaths, and cumulative deaths, respectively for the United Kingdom.&lt;/p>
&lt;h2 id="5-conclusion-and-future-work">5. Conclusion and Future Work&lt;/h2>
&lt;p>After processing all the data gathered in search of a correlation between the weather, more specifically temperature, and the number of new cases in both China and the United States of America, the results clearly indicate that the number of new cases and temperature are uncorrelated. Nonetheless, the results suggest that there is a strong positive correlation (correlation coefficient &amp;gt; 0.8) between the number of new cases and the cumulative number of current cases in United states of America and United Kingdom. Hence, it appears that, in the absence of other mitigating factors, the number of the new cases will increase as long as the cumulative number of current cases keeps increasing.&lt;/p>
&lt;p>While the new covid 19 cases and the current cases at China are uncorrelated, this might be due to false reporting or due to different factors are being used at China to reduce the number of the new cases.&lt;/p>
&lt;p>Given the more recent developments pertaining to the discovery and distribution of vaccines it is suggested that the model be modified to include the number of vaccinations administered. The objective in this case will be to discover any correlation between the number of new cases and both the number of the current cases as well as the number of vaccinations being given across at least the United Sates. Depending on the outcome it maybe possible to determine how effective the vaccines are and maybe predict, if possible, if ever the number of cases will diminish to zero.&lt;/p>
&lt;h2 id="6-acknowledgements">6. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Geoffrey Fox, Dr. Gregor von Laszewski, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their assistance, suggestions, and aid provided during working on this project.&lt;/p>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Covid19.who.int. 2020. [online] Available at: &lt;a href="https://covid19.who.int/table">https://covid19.who.int/table&lt;/a> [Accessed 19 December 2020].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Datatopics.worldbank.org. 2020. Understanding The Coronavirus (COVID-19) Pandemic Through Data | Universal Health Coverage Data | World Bank. [online] Available at: &lt;a href="http://datatopics.worldbank.org/universal-health-coverage/coronavirus/">http://datatopics.worldbank.org/universal-health-coverage/coronavirus/&lt;/a> [Accessed 19 December 2020].&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Kaggle.com. 2020. COVID-19 Open Research Dataset Challenge (CORD-19). [online] Available at: &lt;a href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge">https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge&lt;/a> [Accessed 19 December 2020].&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Visualcrossing.com. 2020. Weather Data Services | Visual Crossing. [online] Available at: &lt;a href="https://www.visualcrossing.com/weather/weather-data-services#/editDataDefinition">https://www.visualcrossing.com/weather/weather-data-services#/editDataDefinition&lt;/a> [Accessed 19 December 2020].&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>The Weather Channel. 2020. National And Local Weather Radar, Daily Forecast, Hurricane And Information From The Weather Channel And Weather.Com. [online] Available at: &lt;a href="https://weather.com/">https://weather.com/&lt;/a> [Accessed 22 December 2020].&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Climate.gov. 2020. Dataset Gallery | NOAA Climate.Gov. [online] Available at: &lt;a href="https://www.climate.gov/maps-data/datasets/formats/csv/variables/precipitation">https://www.climate.gov/maps-data/datasets/formats/csv/variables/precipitation&lt;/a> [Accessed 22 December 2020].&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analyzing the Relationship of Cryptocurrencies with Foriegn Exchange Rates and Global Stock Market Indices</title><link>/report/fa20-523-332/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-332/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-332/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-332/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Krish Hemant Mhatre&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-332/">fa20-523-332&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-332/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="mailto:krishmhatre@icloud.com">krishmhatre@icloud.com&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The project involves analyzing the relationships of various cryptocurrencies with Foreign Exchange Rates and Stock Market Indices. Apart from analyzing the relationships, the objective of the project is also to estimate the trend of the cryptocurrencies based on Foreign Exchange Rates and Stock Market Indices. We will be using historical data of 6 different cryptocurrencies, 25 Stock Market Indices and 22 Foreign Exchange Rates for this project. The project will use various machine learning tools for analysis. The project also uses a fully connected deep neural network for prediction and estimation. Apart from analysis and prediction of prices of cryptocurrencies, the project also involves building its own database and giving access to the database using a prototype API. The historical data and recent predictions can be accessed through the public API.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-resources">2. Resources&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-analysis">4. Analysis&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-principal-component-analysis">4.1 Principal Component Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-tsne-analysis">4.2 TSNE Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-weighted-features-analysis">4.3 Weighted Features Analysis&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-neural-network">5. Neural Network&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-data-preprocessing">5.1 Data Preprocessing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-model">5.2 Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-training">5.3 Training&lt;/a>&lt;/li>
&lt;li>&lt;a href="#54-prediction">5.4 Prediction&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-deployment">6. Deployment&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-daily-update">6.1 Daily Update&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-rest-service">6.2 REST Service&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgement">8. Acknowledgement&lt;/a>&lt;/li>
&lt;li>&lt;a href="#references">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> cryptocurrency, stocks, foreign exchange rates.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The latest type of investment in the finance world and one of the latest global medium of exchange is Cryptocurrency. The total market capitalizations of all cryptocurrencies added up to $237.1 Billion as of 2019&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, making it one of the fastest growing industries in the world. Cryptocurrency systems do not require a central authority as its state is maintained through distributed consensus&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Therefore, determining the factors affecting the prices of cryptocurrencies becomes extremely difficult. There are several factors affecting the prices of cryptocurrency like transaction cost, reward system, hash rate, coins circulation, forks, popularity of cryptomarket, speculations, stock markets, exchange rates, gold price, interest rate, legalization and restriction&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. This project involves studying and analysing the relationships of various cryptocurrencies with Foreign Exchange Rates and Stock Market Indices. Furthermore, the project also involves predicting the cryptocurrency price based on stock market indices and foreign exchange rates of the previous day. The project also involves development of a public API to access the database of the historical data and the predictions.&lt;/p>
&lt;h2 id="2-resources">2. Resources&lt;/h2>
&lt;p>&lt;strong>Table 2.1:&lt;/strong> Resources&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>No.&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Name&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Version&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Type&lt;/strong>&lt;/th>
&lt;th style="text-align:right">&lt;strong>Notes&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">1.&lt;/td>
&lt;td style="text-align:center">Python&lt;/td>
&lt;td style="text-align:center">3.6.9&lt;/td>
&lt;td style="text-align:center">Programming language&lt;/td>
&lt;td style="text-align:right">Python is a high-level interpreted programming language.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">2.&lt;/td>
&lt;td style="text-align:center">MongoDB&lt;/td>
&lt;td style="text-align:center">4.4.2&lt;/td>
&lt;td style="text-align:center">Database&lt;/td>
&lt;td style="text-align:right">MongoDB is a NoSQL Database program that uses JSON-like documents.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">3.&lt;/td>
&lt;td style="text-align:center">Heroku&lt;/td>
&lt;td style="text-align:center">0.1.4&lt;/td>
&lt;td style="text-align:center">Cloud Platform&lt;/td>
&lt;td style="text-align:right">Heroku is a cloud platform used for deploying applications. It uses a Git server to handle application repositories.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">4.&lt;/td>
&lt;td style="text-align:center">Gunicorn&lt;/td>
&lt;td style="text-align:center">20.0.4&lt;/td>
&lt;td style="text-align:center">Server Gateway Interface&lt;/td>
&lt;td style="text-align:right">Gunicorn is a python web server gateway interface . It is mainly used in the project for running python applications on Heroku.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">5.&lt;/td>
&lt;td style="text-align:center">Tensorflow&lt;/td>
&lt;td style="text-align:center">2.3.1&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Tensorflow is an open-source machine learning library. It is mainly used in the project for training models and predicting results.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">6.&lt;/td>
&lt;td style="text-align:center">Keras&lt;/td>
&lt;td style="text-align:center">2.4.3&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Keras is an open-source python library used for interfacing with artificial neural networks. It is an interface for the Tensorflow library.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">7.&lt;/td>
&lt;td style="text-align:center">Scikit-Learn&lt;/td>
&lt;td style="text-align:center">0.22.2&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Scikit-learn is an open-source machine learning library featuring various algorithms for classification, regression and clustering problems.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">8.&lt;/td>
&lt;td style="text-align:center">Numpy&lt;/td>
&lt;td style="text-align:center">1.16.0&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Numpy is a python library used for handling and performing various operations on large multi-dimensional arrays.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">9.&lt;/td>
&lt;td style="text-align:center">Scipy&lt;/td>
&lt;td style="text-align:center">1.5.4&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Scipy is a python library used for scientific and technical computing. It is not directly used in the project but serves as a dependency for tensorflow.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">10.&lt;/td>
&lt;td style="text-align:center">Pandas&lt;/td>
&lt;td style="text-align:center">1.1.4&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Pandas is a python library used mainly for large scale data manipulation and analysis.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">11.&lt;/td>
&lt;td style="text-align:center">Matplotlib&lt;/td>
&lt;td style="text-align:center">3.2.2&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Matplotlib is a python library used for graphing and plotting.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">12.&lt;/td>
&lt;td style="text-align:center">Pickle&lt;/td>
&lt;td style="text-align:center">1.0.2&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Pickle-mixin is a python library used for saving and loading python variables.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">13.&lt;/td>
&lt;td style="text-align:center">Pymongo&lt;/td>
&lt;td style="text-align:center">3.11.2&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Pymongo is a python library containing tools for working with MongoDB.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">14.&lt;/td>
&lt;td style="text-align:center">Flask&lt;/td>
&lt;td style="text-align:center">1.1.2&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Flask is a micro web framework for python. It is used in the project for creating the API.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">15.&lt;/td>
&lt;td style="text-align:center">Datetime&lt;/td>
&lt;td style="text-align:center">4.3&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Datetime is a python library used for handling dates as date objects.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">16.&lt;/td>
&lt;td style="text-align:center">Pytz&lt;/td>
&lt;td style="text-align:center">2020.4&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Pytz is a python library used for accurate timezone calculations.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">17&lt;/td>
&lt;td style="text-align:center">Yahoo Financials&lt;/td>
&lt;td style="text-align:center">1.6&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Yahoo Financials is an unofficial python library used for extracting data from Yahoo Finance website by web scraping.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">18&lt;/td>
&lt;td style="text-align:center">Dns Python&lt;/td>
&lt;td style="text-align:center">2.0.0&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">DNS python is a necessary dependency of Pymongo.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>The project builds its own dataset by extracting the data from Yahoo Finance website using Yahoo Financial python library &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The data includes cryptocurrency prices, stock market indices and foreign exchange rates from September 30 2015 to December 5 2020. The project uses historical data of 6 cryptocurrencies - Bitcoin (BTC), Ethereum (ETH), Dash (DASH), Litecoin (LTC), Monero (XMR) and Ripple (XRP), 25 stock market indices - S&amp;amp;P 500 (USA), Dow 30 (USA), NASDAQ (USA), Russell 2000 (USA), S&amp;amp;P/TSX (Canada), IBOVESPA (Brazil), IPC MEXICO (Mexico), Nikkei 225 (Japan), HANG SENG INDEX (Hong Kong), SSE (China), Shenzhen Component (China), TSEC (Taiwan), KOSPI (South Korea), STI (Singapore), Jakarta Composite Index (Indonesia), FTSE Bursa Malaysia KLCI (Malaysia), S&amp;amp;P/ASX 200 (Australia), S&amp;amp;P/NZX 50 (New Zealand), S&amp;amp;P BSE (India), FTSE 100 (UK), DAX (Germany), CAC 40 (France), ESTX 50 (Europe), EURONEXT 100 (Europe), BEL 20 (Belgium), and 22 foreign exchange rates - Australian Dollar, Euro, New Zealand Dollar, British Pound, Brazilian Real, Canadian Dollar, Chinese Yuan, Hong Kong Dollar, Indian Rupee, Korean Won, Mexican Peso, South African Rand, Singapore Dollar, Danish Krone, Japanese Yen, Malaysian Ringgit, Norwegian Krone, Swedish Krona, Sri Lankan Rupee, Swiss Franc, New Taiwan Dollar, Thai Baht. This data is, then, posted to MongoDB Database. The three databases are created for each of the data types - Cryptocurrency prices, Stock Market Indices and Foreign Exchange Rates. The three databases each contain one collection for every currency, index and rate respectively. These collections have a uniform structure containing 6 columns - &amp;ldquo;id&amp;rdquo;, &amp;ldquo;formatted_date&amp;rdquo;, &amp;ldquo;low&amp;rdquo;, &amp;ldquo;high&amp;rdquo;, &amp;ldquo;open&amp;rdquo; and &amp;ldquo;close&amp;rdquo;. The tickers used to extract data from Yahoo Finance &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> are stated in Figure 3.1.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/tickers.png" alt="Ticker Information">&lt;/p>
&lt;p>&lt;strong>Figure 3.1:&lt;/strong> Ticker Information&lt;/p>
&lt;p>The data is, then, preprocessed to get only one column per date (&amp;ldquo;close&amp;rdquo; price) and to add missing information by replicating previous day&amp;rsquo;s values, which is used to make a large dataset including the prices of all indices and rates for all the dates within the given range. This data is saved in a different MongoDB Database and collection, both, called nn_data. This collection has 54 columns containing closing prices for each cryptocurrency price, stock market index and foreign exchange rate and the date. The rows represent different dates.&lt;/p>
&lt;p>One additional database is also created - Predictions - which contain the predictions of cryptocurrency prices for each day and it&amp;rsquo;s true value. The collection has 13 columns containing a date column and 2 columns for each cryptocurrency (prediction value and true value). New rows are inserted everyday for all collections except the &amp;ldquo;nn_data&amp;rdquo; collection. Figure 3.2 represents the overview of the MongoDB Cluster. Figure 3.3 shows the structure of the nn_data collection.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/mongodb.png" alt="MongoDB Cluster Overview">&lt;/p>
&lt;p>&lt;strong>Figure 3.2:&lt;/strong> MongoDB Cluster Overview&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/sample_data.png" alt="Short Structure of NN_data Collection">&lt;/p>
&lt;p>&lt;strong>Figure 3.3:&lt;/strong> Short Structure of NN_data Collection&lt;/p>
&lt;h2 id="4-analysis">4. Analysis&lt;/h2>
&lt;h3 id="41-principal-component-analysis">4.1 Principal Component Analysis&lt;/h3>
&lt;p>Principal Component Analysis uses Singular Value Decomposition (SVD) for dimensionality reduction, exploratory data analysis and making predictive models. PCA helps understand a linear relationship in the data&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. In this project, PCA is used for the preliminary analysis to find a pattern between the target and the features. Here we have tried to make some observations by performing PCA on various cryptocurrencies with stocks and forex data. In this analysis, we reduced the dimension of the dataset to 3D, represented in Figure 4.1. The first and second dimension is on x-axis and y-axis respectively whereas the third dimension is used in the color. On observing the scatter plots in Figure 4.1, we can clearly see the patterns formed by various relationships. Therefore, it can be stated that the target and features are related in some way based on the principal component analysis.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/pca.png" alt="Principal Component Analysis">&lt;/p>
&lt;p>&lt;strong>Figure 4.1:&lt;/strong> Principal Component Analysis&lt;/p>
&lt;h3 id="42-tsne-analysis">4.2 TSNE Analysis&lt;/h3>
&lt;p>T-Distributed Stochastic Neighbour Embedding is mainly used for non-linear dimensionality reduction. TSNE uses local relationships between points to create a low-dimensional mapping. TSNE uses Gaussian distribution to create a probability distribution. In this project, TSNE is used to analyze non-linear relationships between cryptocurrencies and the features (stock indices and forex rates), which were not visible in the principal component analysis. It can be observed in Figure 4.2, that there are visible patterns in the data i.e. same colored data points are in some pattern, proving a non linear relationship. The t-SNE plots in Figure 4.2 are not like the typical t-SNE plots i.e. they do not have any clusters. This might be because of the size of the dataset.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/tsne.png" alt="t-SNE Analysis">&lt;/p>
&lt;p>&lt;strong>Figure 4.2:&lt;/strong> t-SNE Analysis&lt;/p>
&lt;h3 id="43-weighted-features-analysis">4.3 Weighted Features Analysis&lt;/h3>
&lt;p>Layers of neural networks have weights assigned to each feature column. These weights are updated continuously while training. Analyzing the weights of the model which is trained for this project, can give us a picture of the important features. To perform such an analysis, the top five feature weights are noted for each layer. The number of times a feature is present in the top five of a layer, is also noted. This is represented in Figure 4.3, where we can observe that the New Zealand Dollar and the Canadian Dollar are repeated most number of times in the top five weights of layers.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/TOP_WEIGTHS.png" alt="No. of repetitions in top five weights">&lt;/p>
&lt;p>&lt;strong>Figure 4.3:&lt;/strong> No. of repetitions in top five weights&lt;/p>
&lt;p>The relationships of these two features - New Zealand Dollar and Canadian Dollar with various cryptocurrencies are, then, analyzed in Figure 4.4 and Figure 4.5. It can be observed that Bitcoin has a direct relationship with these rates. Bitcoin can be observed to increase with an increase in NZD to USD rate and an increase in CAD to USD rate. For the rest of the cryptocurrencies, we can observe that they tend to rise when the NZD to USD rate and the CAD to USD rate are stable and tend to fall when the rates move towards either of the extremes.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/nz_vs_crypto.png" alt="Relationship of NZD with Cryptocurrencies">&lt;/p>
&lt;p>&lt;strong>Figure 4.4:&lt;/strong> Relationship of NZD with Cryptocurrencies&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/cad_vs_crypto.png" alt="Relationship of CAD with Cryptocurrencies">&lt;/p>
&lt;p>&lt;strong>Figure 4.5:&lt;/strong> Relationship of CAD with Cryptocurrencies&lt;/p>
&lt;h2 id="5-neural-network">5. Neural Network&lt;/h2>
&lt;h3 id="51-data-preprocessing">5.1 Data Preprocessing&lt;/h3>
&lt;p>The first step to build a neural network for predicting cryptocurrency prices, is to clean the data. In this step, data from the &amp;ldquo;NN_data&amp;rdquo; collection is imported. Two scalers are used to normalize the data, one for feature columns and other for the target columns. For this purpose, &amp;ldquo;StandardScaler&amp;rdquo; from Scikit-learn library is used. These scalers are made to fit with the data and then saved to a file using pickle-mixin, in order to use it later for predictions. These scalers are then used to normalize the data using mean and standard deviation. This normalized data is shuffled and split into a training set and a test set. This procedure is done by using the &amp;ldquo;train_test_split()&amp;rdquo; function from the Scikit-learn library. The data is split into 94:6 ratio for training and testing respectively. The final data is split into four - X_train, X_test, y_train and y_test and is ready for training the neural network model.&lt;/p>
&lt;h3 id="52-model">5.2 Model&lt;/h3>
&lt;p>For the purpose of predicting the prices of cryptocurrency based on previous day’s stock indices and forex rates, the project uses a fully connected neural network. The solution to this problem could have been perceived in different ways like making it a classification problem by predicting rise or fall in price or by making it a regression problem by either predicting the actual price or by predicting the growth. After trying all these ways of solution, it was concluded that predicting the price regression problem was the best option.&lt;/p>
&lt;p>The final model comprises three layers - one input layer, one hidden layer and one output layer. The first layer uses 8 units with an input dimension of (None, 47) and uses Rectified Linear Unit (ReLU) as its activation function, and He Normal as its kernel initializer. The second layer which is a hidden layer uses 2670 hidden units with Rectified Linear Unit (ReLU) Activation function. ReLU is used because of its faster and effective training in regression models. The third layer which is the output layer has 6 units, one each for predicting 6 cryptocurrencies. The output layer uses linear activation function.&lt;/p>
&lt;p>The overview of the final model can be seen in Figure 5.2. The predictions using the un-trained model can be seen in Figure 5.3, where we can observe the initialization of weights.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/model.png" alt="Model Overview">&lt;/p>
&lt;p>&lt;strong>Figure 5.2:&lt;/strong> Model Overview&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/intialize.png" alt="Visualization of Initial Weights">&lt;/p>
&lt;p>&lt;strong>Figure 5.3:&lt;/strong> Visualization of Initial Weights&lt;/p>
&lt;h3 id="53-training">5.3 Training&lt;/h3>
&lt;p>The neural network model is compiled before training. The model is compiled using Adam optimizer with a default learning rate of 0.001. The model uses Mean Squared Error as its loss function in order to reduce the error and give a close approximation of the cryptocurrency prices. Mean squared error is also used as a metric to visualize the performance of the model.&lt;/p>
&lt;p>The model is, then, trained by using X_train and y_train, as mentioned above, for 5000 epochs and by splitting the dataset for validation (20% for validation). The performance of the training of the final model for first 2500 epochs can be observed in Figure 5.4.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/dnn_with_normal_init.png" alt="Final Model Training">&lt;/p>
&lt;p>&lt;strong>Figure 5.4:&lt;/strong> Final Model Training&lt;/p>
&lt;p>This particular model was chosen because of its low validation mean squared error as compared to the performance of other models. Figure 5.5 represents the performance of a similar fully connected model with Random Normal as its initializer instead of He Normal. Figure 5.6 represents the performance of a Convolutional Neural Network. This model was trained with a much lower mean squared error but had a higher validation mean squared error and was therefore dropped.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/dnn_with_random_normal_init.png" alt="Performance of Fully Connected with Random Normal">&lt;/p>
&lt;p>&lt;strong>Figure 5.5:&lt;/strong> Performance of Fully Connected with Random Normal&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/cnn.png" alt="Performance of Convolutional Neural Network">&lt;/p>
&lt;p>&lt;strong>Figure 5.6:&lt;/strong> Performance of Convolutional Neural Network&lt;/p>
&lt;h3 id="54-prediction">5.4 Prediction&lt;/h3>
&lt;p>After training, the model is stored in a .h5 file, which can be used to make predictions. For making predictions, the project preprocesses the data provided which needs to be of the input dimension of the model i.e. of shape (1, 47). Both the scalers which were saved earlier in the preprocessing stage are loaded again using pickle-mixin. The feature scaler is used to transform the new data to normalized data. This normalized data of the given dimension is then used to predict the prices for six cryptocurrencies. Since regression models do not show accuracy directly, it can be measured manually by rounding off the predicted values and the corresponding true values to the decimal place of one or two and then getting the difference between the two and comparing it to a preset threshold. If the values are rounded off to one decimal place and the threshold is set to 0.05 on the normalized predictions, the accuracy of the prediction is approximately 88% and if the values are rounded off to two decimal places, the accuracy is approximately 62%. The predictions of the test data and the corresponding true values for Bitcoin can be observed in Figure 5.7, where similarities can be observed. Prediction for a new date for the prices of all six cryptocurrencies and its true values can be observed in Figure 5.8. Figure 5.9 also displays the actual result of this project as it can be observed that the predictions and the true values have similar trend with a low margin of error.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/prediction.png" alt="Prediction vs. True">&lt;/p>
&lt;p>&lt;strong>Figure 5.7:&lt;/strong> Prediction vs. True&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/pred.png" alt="Prediction vs. True for one day’s test data">&lt;/p>
&lt;p>&lt;strong>Figure 5.8:&lt;/strong> Prediction vs. True for one day’s test data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/pred_vs_true.png" alt="Prediction vs. True for all cryptocurrencies">&lt;/p>
&lt;p>&lt;strong>Figure 5.9:&lt;/strong> Prediction vs. True for all cryptocurrencies&lt;/p>
&lt;h2 id="6-deployment">6. Deployment&lt;/h2>
&lt;h3 id="61-daily-update">6.1 Daily Update&lt;/h3>
&lt;p>The database is supposed to be updated daily using a web-app deployed on Heroku. Heroku is a cloud platform used for deploying web-apps of various languages and also uses a Git-server for repositories &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. This daily update web-app is triggered daily at 07.30 AM UTC i.e 2.00 AM EST. The web-app extracts the data for the previous day and updates all the collections. The new data is then preprocessed by using the saved feature normalizer. This normalized data is used to get predictions for the prices of cryptocurrencies for the day that just started. The web-app then gets the true values of the cryptocurrency prices for the previous day and updates the predictions collection using this data for future comparison. The web-app is currently deployed on Heroku and is triggered daily using Heroku Scheduler. The web-app is entirely coded in Python.&lt;/p>
&lt;h3 id="62-rest-service">6.2 REST Service&lt;/h3>
&lt;p>The data from the MongoDB databases can be accessed using a public RESTful API. The API is developed using Flask-Python. The API usage is given below.&lt;/p>
&lt;p>URL - &lt;code>https://crypto-project-api.herokuapp.com/&lt;/code>&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;strong>/get_data/single/market/index/date&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>Type - GET&lt;/em>&lt;/p>
&lt;p>Sample Request -&lt;/p>
&lt;p>&lt;code>https://crypto-project-api.herokuapp.com/get_data/single/crypto/bitcoin/2020-12-05&lt;/code>&lt;/p>
&lt;p>Sample Respose -&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;data&amp;quot;:
[
{
&amp;quot;close&amp;quot;:19154.23046875,
&amp;quot;date&amp;quot;:&amp;quot;2020-12-05&amp;quot;,
&amp;quot;high&amp;quot;:19160.44921875,
&amp;quot;low&amp;quot;:18590.193359375,
&amp;quot;open&amp;quot;:18698.384765625
}
],
&amp;quot;status&amp;quot;:&amp;quot;Success&amp;quot;
}
&lt;/code>&lt;/pre>&lt;hr>
&lt;p>&lt;em>&lt;strong>/get_data/multiple/market/index/start_date/end_date&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>Type - GET&lt;/em>&lt;/p>
&lt;p>Sample Request -&lt;/p>
&lt;p>&lt;code>https://crypto-project-api.herokuapp.com/get_data/multiple/crypto/bitcoin/2020-12-02/2020-12-05&lt;/code>&lt;/p>
&lt;p>Sample Respose -&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;data&amp;quot;:
[
{
&amp;quot;close&amp;quot;:&amp;quot;19201.091796875&amp;quot;,
&amp;quot;date&amp;quot;:&amp;quot;2020-12-02&amp;quot;,
&amp;quot;high&amp;quot;:&amp;quot;19308.330078125&amp;quot;,
&amp;quot;low&amp;quot;:&amp;quot;18347.71875&amp;quot;,
&amp;quot;open&amp;quot;:&amp;quot;18801.744140625&amp;quot;
},
{
&amp;quot;close&amp;quot;:&amp;quot;19371.041015625&amp;quot;,
&amp;quot;date&amp;quot;:&amp;quot;2020-12-03&amp;quot;,
&amp;quot;high&amp;quot;:&amp;quot;19430.89453125&amp;quot;,
&amp;quot;low&amp;quot;:&amp;quot;18937.4296875&amp;quot;,
&amp;quot;open&amp;quot;:&amp;quot;18949.251953125&amp;quot;
},
{
&amp;quot;close&amp;quot;:19154.23046875,
&amp;quot;date&amp;quot;:&amp;quot;2020-12-05&amp;quot;,
&amp;quot;high&amp;quot;:19160.44921875,
&amp;quot;low&amp;quot;:18590.193359375,
&amp;quot;open&amp;quot;:18698.384765625
}
],
&amp;quot;status&amp;quot;:&amp;quot;Success&amp;quot;
}
&lt;/code>&lt;/pre>&lt;hr>
&lt;p>&lt;em>&lt;strong>/get_predictions/date&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>Type - GET&lt;/em>&lt;/p>
&lt;p>Sample Request -&lt;/p>
&lt;p>&lt;code>https://crypto-project-api.herokuapp.com/get_predictions/2020-12-05&lt;/code>&lt;/p>
&lt;p>Sample Respose -&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;data&amp;quot;:
[
{
&amp;quot;bitcoin&amp;quot;:&amp;quot;16204.04&amp;quot;,
&amp;quot;dash&amp;quot;:&amp;quot;24.148237&amp;quot;,
&amp;quot;date&amp;quot;:&amp;quot;2020-12-05&amp;quot;,
&amp;quot;ethereum&amp;quot;:&amp;quot;503.43005&amp;quot;,
&amp;quot;litecoin&amp;quot;:&amp;quot;66.6938&amp;quot;,
&amp;quot;monero&amp;quot;:&amp;quot;120.718414&amp;quot;,
&amp;quot;ripple&amp;quot;:&amp;quot;0.55850273&amp;quot;
}
],
&amp;quot;status&amp;quot;:&amp;quot;Success&amp;quot;
}
&lt;/code>&lt;/pre>&lt;hr>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>After analyzing the historical data of Stock Market Indices, Foreign Exchange Rates and Cryptocurrency Prices, it can be concluded that there does exist a non-linear relationship between the three. It can also be concluded that cryptocurrency prices can be predicted and its trend can be estimated using Stock Indices and Forex Rates. There is still a large scope of improvement in reducing the mean squared error. The project can further improve the neural network model for better predictions. In the end, it is safe to conclude that the indicators of international politics like Stock Market Indices and Forex Exchange Rates are factors affecting the prices of cryptocurrency.&lt;/p>
&lt;h2 id="8-acknowledgement">8. Acknowledgement&lt;/h2>
&lt;p>Krish Hemant Mhatre would like to thank Indiana University and Luddy School of Informatics, Computing and Engineering for providing me with the opportunity to work on this project. He would also like to thank Dr. Geoffrey C. Fox, Dr. Gregor von Laszewski and the Assistant Instructors of ENGR-E-534 Big Data Analytics and Applications for their constant guidance and support.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Szmigiera, M. &amp;ldquo;Cryptocurrency Market Value 2013-2019.&amp;rdquo; Statista, 20 Jan. 2020, &lt;a href="https://www.statista.com/statistics/730876/cryptocurrency-maket-value">https://www.statista.com/statistics/730876/cryptocurrency-maket-value&lt;/a>.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Lansky, Jan. &amp;ldquo;Possible State Approaches to Cryptocurrencies.&amp;rdquo; Journal of Systems Integration, University of Finance and Administration in Prague Czech Republic, &lt;a href="http://www.si-journal.org/index.php/JSI/article/view/335">http://www.si-journal.org/index.php/JSI/article/view/335&lt;/a>.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Sovbetov, Yhlas. &amp;ldquo;Factors Influencing Cryptocurrency Prices: Evidence from Bitcoin, Ethereum, Dash, Litcoin, and Monero.&amp;rdquo; Journal of Economics and Financial Analysis, London School of Commerce, 26 Feb. 2018, &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3125347">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3125347&lt;/a>.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Sanders, Connor. &amp;ldquo;YahooFinancials.&amp;rdquo; PyPI, JECSand, 22 Oct. 2017, &lt;a href="https://pypi.org/project/yahoofinancials/">https://pypi.org/project/yahoofinancials/&lt;/a>.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Jaadi, Zakaria. &amp;ldquo;A Step-by-Step Explanation of Principal Component Analysis.&amp;rdquo; Built In, &lt;a href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis">https://builtin.com/data-science/step-step-explanation-principal-component-analysis&lt;/a>.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>&amp;ldquo;What Is Heroku.&amp;rdquo; Heroku, &lt;a href="https://www.heroku.com/what">https://www.heroku.com/what&lt;/a>.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Deep Learning in Drug Discovery</title><link>/report/sp21-599-359/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/sp21-599-359/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Anesu Chaora, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/">sp21-599-359&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Code: &lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/blob/main/project/code/predicting_molecular_activity.ipynb">predicting_molecular_activity.ipynb&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Machine learning has been a mainstay in drug discovery for decades. Artificial neural networks have been used in computational approaches to drug discovery since the 1990s [^1]. Under traditional approaches, emphasis in drug discovery was placed on understanding chemical molecular fingerprints, in order to predict biological activity. More recently however, deep learning approaches have been adopted instead of computational methods. This paper outlines work conducted in predicting drug molecular activity, using deep learning approaches.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#11-de-novo-molecular-design">1.1. De novo molecular design&lt;/a>&lt;/li>
&lt;li>&lt;a href="#12-bioactivity-prediction">1.2. Bioactivity prediction&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#2-related-work">2. Related Work&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-merck-molecular-activity-challenge-on-kaggle">2.1. Merck Molecular Activity Challenge on Kaggle&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-the-dataset">2.2. The Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-a-deep-learning-algorithm">2.3. A Deep Learning Algorithm&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-project-implementation">3. Project Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-tools-and-environment">3.1. Tools and Environment&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-implementation-overview">3.2. Implementation Overview&lt;/a>&lt;/li>
&lt;li>&lt;a href="#33-benchmarks">3.3. Benchmarks&lt;/a>&lt;/li>
&lt;li>&lt;a href="#34-findings">3.4. Findings&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-discussion">4. Discussion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgments">6. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-appendix">7. Appendix&lt;/a>&lt;/li>
&lt;li>&lt;a href="#references">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Deep Learning, drug discovery.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;h3 id="11-de-novo-molecular-design">1.1. De novo molecular design&lt;/h3>
&lt;p>Deep learning (DL) is finding uses in developing novel chemical structures. Methods that employ variational autoencoders (VAE) have been used to generate new chemical structures. Approaches have involved encoding input string molecule structures, then reparametrizing the underlying latent variables, before searching for viable solutions in the latent space by using methods such as Bayesian optimizations. The results are then decoded back into simplified molecular-input line-entry system (SMILES) notation, for recovery of molecular descriptors. Variations to this method involve using generative adversarial networks (GAN)s, as subnetworks in the architecture, to generate the new chemical structures &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Other approaches for developing new chemical structures involve recurrent neural networks (RNN), to generate new valid SMILES strings, after training the RNNs on copious quantities of known SMILES datasets. The RNNs use probability distributions learned from training sets, to generate new strings that correspond to molecular structures &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Variations to this approach incorporate reinforcement learning to reward models for new chemical structures, while punishing them for undesirable results &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="12-bioactivity-prediction">1.2. Bioactivity prediction&lt;/h3>
&lt;p>Computational methods have been used in drug development for decades &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The emergence of high-throughput screening (HTS), in which automated equipment is used to conduct large assays of scientific experiments on molecular compounds in parallel, has resulted in generation of enormous amounts of data that require processing. Quantitative structure activity relationship (QSAR) models for predicting the biological activity responses to physiochemical properties of predictor chemicals, extensively use machine learning models like support vector machines (SVM) and random decision forests (RF) for processing &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>While deep learning (DL) approaches have an advantage over single-layer machine learning methods, when predicting biological activity responses to properties of predictor chemicals, they have only recently been used for this &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The need to interpret how predictions are made through computationally oriented drug discovery, is seen - in part - as a factor to why DL approaches have not been adopted as quickly in this area &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. However, because DL models can learn complex non-linear data patterns, using their multiple hidden layers to capture patterns in data, they are better suited for processing complex life sciences data, than other machine learning approaches &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Their applications have included profiling tumors at molecular level and predicting drug responses, based on pharmacological and biological molecular structures, functions, and dynamics. This is attributed to their ability to handle high dimensionality in data features, making them appealing for use in predicting drug response &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>For example, deep neural networks were used in models that won NIH’s Toxi21 Challenge &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup> on using chemical structure data only to predict compounds of concern to human health &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. DL models were also found to perform better than standard RF models &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> in predicting the biological activities of molecular compounds in the Merck Molecular Activity Challenge on Kaggle &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. Details of the challenge follow.&lt;/p>
&lt;h2 id="2-related-work">2. Related Work&lt;/h2>
&lt;h3 id="21-merck-molecular-activity-challenge-on-kaggle">2.1. Merck Molecular Activity Challenge on Kaggle&lt;/h3>
&lt;p>A challenge to identify the best statistical techniques for predicting molecular activity was issued by Merck &amp;amp; Co Pharmaceutical, through Kaggle in October of 2012. The stated goal of the challenge was to ‘help develop safe and effective medicines by predicting molecular activity’ for effects that were both on and off target &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="22-the-dataset">2.2. The Dataset&lt;/h3>
&lt;p>A &lt;a href="https://www.kaggle.com/c/MerckActivity/data">dataset&lt;/a> was provided for the challenge &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. It consisted of 15 molecular activity datasets. Each dataset contained rows corresponding to assays of biological activity for chemical compounds. The datasets were subdivided into training and test set files. The training and test dataset split was done by dates of testing &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>, with test set dates consisting of assays conducted after the training set assays.&lt;/p>
&lt;p>The training set files each had a column with molecular descriptors that were formulated from chemical molecular structures. A second column in the files contained numeric values, corresponding to raw activity measures. These were not normalized, and indicated measures in different units.&lt;/p>
&lt;p>The remainder of the columns in each training dataset file indicated disguised substructures of molecules. Values in each row, under the substructure (atom pair and donor-acceptor pair) codes, corresponded to the frequencies at which each of the substructures appeared in each compound. Figure 1 shows part of the head row for one of the training dataset files, and the first records in the file.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/raw/develop/project/images/training_set.jpg" alt="Figure 1">
&lt;strong>Figure 1&lt;/strong>: Head Row of 1 of 15 Training Dataset files&lt;/p>
&lt;p>The test dataset files were similar (Figure 2) to the training files, except they did not include the column for activity measures. The challenge presented was to predict the activity measures for the test dataset.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/raw/develop/project/images/test_set.jpg" alt="Figure 2">
&lt;strong>Figure 2&lt;/strong>: Head Row of 1 of 15 Test Dataset files&lt;/p>
&lt;h3 id="23-a-deep-learning-algorithm">2.3. A Deep Learning Algorithm&lt;/h3>
&lt;p>The entry that won the Merck Molecular Activity Challenge on Kaggle used an ensemble of methods that included a fully connected neural network as the main contributor to the high accuracy in predicting molecular activity &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Evaluations of predictions for molecular activity for the test set assays were then determined using the mean of the correlation coefficient (R2) of the 15 data sets. Sample code in R was provided for evaluating the correlation coefficient. The code, and formula for R2 are appended in Appendix 1.&lt;/p>
&lt;p>An approach of employing convolutional networks on substructures of molecules, to concentrate learning on localized features, while reducing the number of parameters in the overall network, was also proposed in literature on improving molecular activity predictions. This methodology of identifying molecular substructures as graph convolutions, prior to further processing, was discussed by authors &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>, &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In line with the above research, an ensemble of networks for predicting molecular activity was planned for this project, using the Merck dataset, and hyperparameter configurations found optimal by the cited authors. Recognized optimal activation functions, for different neural network types and prediction types &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>, were also earmarked for use on the project.&lt;/p>
&lt;h2 id="3-project-implementation">3. Project Implementation&lt;/h2>
&lt;p>Implementation details for the project were as follows:&lt;/p>
&lt;h3 id="31-tools-and-environment">3.1. Tools and Environment&lt;/h3>
&lt;p>The Python programming language (version 3.7.10) was used on Google Colab (&lt;a href="https://colab.research.google.com">https://colab.research.google.com&lt;/a>).&lt;/p>
&lt;p>A subscription account to the service was employed, for access to more RAM (High-RAM runtime shape) during development, although the free standard subscription will suffice for the version of code included in this repository.&lt;/p>
&lt;p>Google Colab GPU hardware accelerators were used in the runtime configuration.&lt;/p>
&lt;p>Prerequisites for the code included packages from &lt;a href="http://cloudmesh.github.io/">Cloudmesh&lt;/a>, for benchmarking performance, and from &lt;a href="https://www.kaggle.com/">Kaggle&lt;/a>, for API access to related data.&lt;/p>
&lt;p>Keras libraries were used for implementing the molecular activity prediction model.&lt;/p>
&lt;h3 id="32-implementation-overview">3.2. Implementation Overview&lt;/h3>
&lt;p>This project&amp;rsquo;s implementation of a molecular activity prediction model consisted of a fully connected neural network. The network used the Adam &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup> optimization algorithm, at a learning rate of 0.001 and beta_1 calibration of 0.5. Mean Squared Error (MSE) was used for the loss function, and R-Squared &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup> for the metric. Batch sizes were set at 128. These parameter choices were selected by referencing the choices of other prior investigators &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The network was trained on the 15 datasets separately, by iterating through the storage location containing preprocessed data, and sampling the data into training, evaluation and prediction datasets - before running the training. The evaluation and prediction steps, for each dataset, where also executed during the iteration of each molecular activity dataset. Running the processing in this way was necessitated by the fact that the 15 datasets each had different feature set columns, corresponding to different molecular substructures. As such, they could not be readily processed through a single dataframe.&lt;/p>
&lt;p>An additional compounding factor was that the data was missing the molecular activity results (actual readings) associated with the dataset provided for testing. These were not available through Kaggle as the original competition withheld these from contestants, reserving them as a means for evaluating the accuracy of the models submitted. In the absence of this data, for validating the results of this project, the available training data was split into samples that were then used for the exercise. The training of the fully connected network was allocated 80% of the data, while the testing/evaluation of the model was allocated 10% of the data. The remaining data (10%) was used for evaluating predictions.&lt;/p>
&lt;h3 id="33-benchmarks">3.3. Benchmarks&lt;/h3>
&lt;p>Benchmarks captured during code execution, using cloudmesh-common &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>, were as follows:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The data download process from Kaggle, through the Kaggle data API, took 29 seconds.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Data preprocessing scripts took 8 minutes and 56 seconds to render the data ready for training and evaluation. Preprocessing of data included iterating through the issued datasets separately, since each file contained different combinations of feature columns (molecular substructures).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The model training, evaluation and prediction step took 7 minutes and 45 seconds.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="34-findings">3.4. Findings&lt;/h3>
&lt;p>The square of the correlation coefficient (R^2) values obtained (coefficient of determination) &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup> during training and evaluation were considerably low (&amp;lt; 0.1). A value of one (1) would indicate a goodness of fit for the model that implies that the model is completely on target with predicting accurate outcomes (molecular activity) from the independent variables (substructures/feature sets). Such a model would thus fully account for the predictions, given a set of substructures as inputs. A value of zero (0) would indicate a total lack of correlation between the input feature values and the predicted outputs. As such, it would imply that there is a lot of unexplained variance in the outputs of the model. The square of the correlation coefficient values obtained for this model (&amp;lt;0.1) therefore imply that it either did not learn enough, or other unexplained (by the model) variance caused unreliable predictions.&lt;/p>
&lt;h2 id="4-discussion">4. Discussion&lt;/h2>
&lt;p>An overwhelming proportion of the data elements provided through the datasets were zeros (0)s, indicating that no frequencies of the molecular substructures/features were present in the molecules represented by particular rows of data elements. This disproportionate representation of absent molecular substructure frequencies, versus the significantly lower instances where there were frequencies appears to have had an effect of dampening the learning of the fully connected neural network.&lt;/p>
&lt;p>This supports approaches that advocated for the use of convolutional neural networks &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>, &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup> as auxiliary components to help focus learning on pertinent substructures. While the planning phase of this project had incorporated inclusion of such, the investigator ran out of time to implement an ensemble network that would include the suggestions.&lt;/p>
&lt;p>Apart from employing convolutions, other preprocessing approaches for rescaling, and normalizing, the data features and activations &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup> could have helped the learning, and subsequently the predictions made. This reinforces the fact that deep learning models, as is true of other machine learning approaches, rely deeply on the quality and preparation of data fed into them.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>Deep learning is a very powerful new approach to solving many machine learning problems, including some that have eluded solutions till now. While deep learning models offer robust and sophisticated ways of learning patterns in data, they are still only half the story. The quality and appropriate preparation of the data fed into models is equally important when seeking to have meaningful results.&lt;/p>
&lt;h2 id="6-acknowledgments">6. Acknowledgments&lt;/h2>
&lt;p>Acknowledgements go to Dr. Geoffrey Fox for his excellent guidance on ways to think about deep learning approaches, and for his instructorship of the course &amp;lsquo;ENG-E599: AI-First Engineering&amp;rsquo;, for which this project is a deliverable. Acknowledgements also go to Dr. Gregor von Laszewski for his astute tips and recommendations on technical matters, and on coding and documention etiquette.&lt;/p>
&lt;h2 id="7-appendix">7. Appendix&lt;/h2>
&lt;p>Square of the Correlation Coefficient (R2) Formula:&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/raw/develop/project/images/correlation_coefficient.jpg" alt="Figure 3">
&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>&lt;/p>
&lt;p>Sample R2 Code in the R Programming Language:&lt;/p>
&lt;pre>&lt;code>Rsquared &amp;lt;- function(x,y) {
# Returns R-squared.
# R2 = \frac{[\sum_i(x_i-\bar x)(y_i-\bar y)]^2}{\sum_i(x_i-\bar x)^2 \sum_j(y_j-\bar y)^2}
# Arugments: x = solution activities
# y = predicted activities
if ( length(x) != length(y) ) {
warning(&amp;quot;Input vectors must be same length!&amp;quot;)
}
else {
avx &amp;lt;- mean(x)
avy &amp;lt;- mean(y)
num &amp;lt;- sum( (x-avx)*(y-avy) )
num &amp;lt;- num*num
denom &amp;lt;- sum( (x-avx)*(x-avx) ) * sum( (y-avy)*(y-avy) )
return(num/denom)
}
}
&lt;/code>&lt;/pre>&lt;p>&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Hongming Chen, O. E. (2018). The rise of deep learning in drug discovery. Elsevier.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Marwin H. S. Segler, T. K. (2018). Generating Focused Molecule Libraries for Drug Discovery with Recurrent Neural Networks. America Chemical Society.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>N Jaques, S. G. (2017). Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control. Proceedings of the 34th International Conference on Machine Learning, PMLR (pp. 1645-1654). MLResearchPress.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Gregory Sliwoski, S. K. (2014). Computational Methods in Drug Discovery. Pharmacol Rev, 334 - 395.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Delora Baptista, P. G. (2020). Deep learning for drug response prediction in cancer. Briefings in Bioinformatics, 22, 2021, 360–379.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Erik Gawehn, J. A. (2016). Deep Learning in Drug Discovery. Molecular Informatics, 3 - 14.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>National Institute of Health. (2014, November 14). Tox21 Data Challenge 2014. Retrieved from [tripod.nih.gov:] &lt;a href="https://tripod.nih.gov/tox21/challenge/">https://tripod.nih.gov/tox21/challenge/&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Andreas Mayr, G. K. (2016). Deeptox: Toxicity Prediction using Deep Learning. Frontiers in Environmental Science.&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Junshui Ma, R. P. (2015). Deep Neural Nets as a Method for Quantitative Structure-Activity Relationships. Journal of Chemical Information and Modeling, 263-274.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Kaggle. (n.d.). Merck Molecular Activity Challenge. Retrieved from [Kaggle.com:] &lt;a href="https://www.kaggle.com/c/MerckActivity">https://www.kaggle.com/c/MerckActivity&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Kearnes, S., McCloskey, K., Berndl, M., Pande, V., &amp;amp; Riley, P. (2016). Molecular graph convolutions: moving beyond fingerprints. Switzerland: Springer International Publishing .&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Mikael Henaff, J. B. (2015). Deep Convolutional Networks on Graph-Structured Data.&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Bronlee, J. (2021, January 22). How to Choose an Activation Function for Deep Learning. Retrieved from [https://machinelearningmastery.com:] &lt;a href="https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/">https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Keras. (2021). Adam. Retrieved from [https://keras.io:] &lt;a href="https://keras.io/api/optimizers/adam/">https://keras.io/api/optimizers/adam/&lt;/a>&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>Keras. (2021). Regression Metrics. Retrieved from [https://keras.io:] &lt;a href="https://keras.io/api/metrics/regression_metrics/">https://keras.io/api/metrics/regression_metrics/&lt;/a>&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16" role="doc-endnote">
&lt;p>RuwanT (2017, May 16). Merk. Retrieved from [https://github.com:] &lt;a href="https://github.com/RuwanT/merck/blob/master/README.md">https://github.com/RuwanT/merck/blob/master/README.md&lt;/a>&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17" role="doc-endnote">
&lt;p>Wikipedia (2021). Coefficient of Determination. Retrieved from [https://wikipedia.org:] &lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">https://en.wikipedia.org/wiki/Coefficient_of_determination&lt;/a>&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data Application in E-commerce</title><link>/report/fa20-523-339/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-339/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-339/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-339/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Tao Liu, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-339/">fa20-523-339&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-339/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>As a result of the last twenty year&amp;rsquo;s Internet development globally, the E-commerce industry is getting stronger and stronger. While customers enjoyed their convenient online purchase environment, E-commerce sees the potential for the data and information customers left during their online shopping process. One fundamental usage for this information is to perform a Recommendation Strategy to give customers potential products they would also like to purchase. This report will build a User-Based Collaborative Filtering strategy to provide customer recommendation products based on the database of previous customer purchase records. This report will start with an overview of the background and explain the dataset it chose &lt;em>Amazon Review Data&lt;/em>. After that, each step for the code and step made in a corresponding file &lt;em>Big_tata_Application_in_E_commense.ipynb&lt;/em> will be illustrated, and the User-Based Collaborative Filtering strategy will be presented step by step.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background">2. Background&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-data-preprocessing-and-cleaning">4. Data Preprocessing and cleaning&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-recommendation-rate-and-similarity-calculation">5. Recommendation Rate and Similarity Calculation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-accuracy">6. Accuracy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-benchmark">7. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-conclusion">8. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-acknowledgements">9. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#10-references">10. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> recommendation strategy,user-based, collaborative filtering, business, big data, E-commerce, customer behavior&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big data have many applications in scientific research and business, from those in the hardware perspective like Higgs Discovery to the software perspective like E-commence. However, with the passage of time, online shopping and E-commerce have become one of the most popular events for citizens' lives and society. Millions of goods are now sold online the customers all over the world. With the 5G technology&amp;rsquo;s implementation, this trend is now inevitable. These activities will create millions of data about customer&amp;rsquo;s behaviors like how they value products, how they purchase or sell the products, and how they review the goods purchased would have a tremendous contribution for corporations to analyze. These data can not only help convince the current strategies of E-commerce on the right track, but a potential way to see which step E-commerce can make up for attracting more customers to buy the goods. At the same time, these data can also be implemented as a way for recommendation strategies for E-commerce. It will help customers find the next products they like in a short period by implementing machine learning technology on Big Data. The corporations also can enjoy the increase of sales and attractions by recommendation strategies. A better recommendation strategy on E-commerce is now the new trend for massive data scientists and researchers’ target. Therefore, this field is now one of the most popular research areas in the data science fields.&lt;/p>
&lt;p>In this final project, An User-Based Collaborative Filtering Strategy will be implemented to get a taste of the recommendation strategy based on Customer&amp;rsquo;s Gift card purchase records and the item they also viewed and bought. The algorithm&amp;rsquo;s logic is the following: A used record indicates that customer who bought product A and also view/buy products B&amp;amp;C. When a new customer comes and shows his interest in B&amp;amp;C, product A would be recommended. This logic is addressed based on the daily-experience of customer behaviors on their E-commerce experience.&lt;/p>
&lt;h2 id="2-background">2. Background&lt;/h2>
&lt;p>Recommendation Strategy is a quite popular research area in recent years with a strong real-world influence. It is largely used in E-commerce platforms like Taobao, Amazon, etc. Therefore, It is obvious that there are plenty of recommendation strategies have been done. Though every E-commerce recommendation algorithm may be different from each other, the most popular technique for recommendation systems is called Collaborative Filtering. It is a technique that can filter out items that a user might like based on reactions by similar users. During this technique, the memory-based method is considered in this report since it uses a dataset to calculate the prediction using statistical techniques. This strategy will be able to fulfill in the local environment with a proper dataset. There are two kinds of memory-based methods available in the market: &lt;em>User-Based Collaborative Filtering&lt;/em>, &lt;em>Item-Based Collaborative Filtering&lt;/em> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This project will only focus on the User-Based Collaborative Filtering Strategy since Item-Based Collaborative Filtering requires a customer review rate for evaluation. The customer review rate for evaluation is not in the dataset available in the market. Therefore, Item-Based Collaborative Filtering unlikely to be implemented, and the User-Based Collaborative Filtering Strategy is considered.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>The dataset for this study is called &lt;em>Amazon Review Data&lt;/em> &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Particularly, since the dataset is now reached billions of amount, the subcategory gift card will be used as an example since the overall customer record is 1547 and the amount of data retrieved is currently in the right amount of training. This fact can help to perform User-Based Collaborative Filtering in a controlled timeline.&lt;/p>
&lt;h2 id="4-data-preprocessing-and-cleaning">4. Data Preprocessing and cleaning&lt;/h2>
&lt;p>The first step will be data collection and data cleaning. The raw data-set is imported directly from data-set contributors' online storage &lt;em>meta_Gift_Cards.json.gz&lt;/em> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> to Google Colab notebook. The raw database retrieved directly from the website will be shown in &lt;strong>Table 1&lt;/strong>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Attribute&lt;/th>
&lt;th style="text-align:center">Description&lt;/th>
&lt;th style="text-align:center">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">category&lt;/td>
&lt;td style="text-align:center">The category of the record&lt;/td>
&lt;td style="text-align:center">[&amp;quot;Gift Cards&amp;quot;, &amp;ldquo;Gift Cards&amp;rdquo;]\&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">tech1&lt;/td>
&lt;td style="text-align:center">tech relate to it&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">description&lt;/td>
&lt;td style="text-align:center">The description of the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;Gift card for the purchase of goods&amp;hellip;&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">fit&lt;/td>
&lt;td style="text-align:center">fit for its record&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">title&lt;/td>
&lt;td style="text-align:center">title for the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;Serendipity 3 $100.00 Gift Card&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">&lt;strong>also_buy&lt;/strong>&lt;/td>
&lt;td style="text-align:center">the product also bought&lt;/td>
&lt;td style="text-align:center">[&amp;quot;B005ESMEBQ&amp;quot;]\&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">image&lt;/td>
&lt;td style="text-align:center">image of the gift card&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">tech2&lt;/td>
&lt;td style="text-align:center">tech relate to it&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">brand&lt;/td>
&lt;td style="text-align:center">brand of the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;Amazon&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">feature&lt;/td>
&lt;td style="text-align:center">feature of the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;Amazon.com Gift cards never expire&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">rank&lt;/td>
&lt;td style="text-align:center">rank of the product&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">&lt;strong>also_view&lt;/strong>&lt;/td>
&lt;td style="text-align:center">the product also view&lt;/td>
&lt;td style="text-align:center">[&amp;quot;BT00DC6QU4&amp;quot;]\&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">details&lt;/td>
&lt;td style="text-align:center">detail for the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;3.4 x 2.1 inches ; 1.44 ounces&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">main_cat&lt;/td>
&lt;td style="text-align:center">main category of the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;Grocery&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">similar_item&lt;/td>
&lt;td style="text-align:center">similar_item of the product&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">date&lt;/td>
&lt;td style="text-align:center">date of the product assigned&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">price&lt;/td>
&lt;td style="text-align:center">price of the product&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">&lt;strong>asin&lt;/strong>&lt;/td>
&lt;td style="text-align:center">product asin code&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;B001BKEWF2&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 1:&lt;/strong> The description for the dataset&lt;/p>
&lt;p>Since the attributes &lt;em>category&lt;/em>, &lt;em>main_cat&lt;/em> are the same for the whole dataset, they will not be valid training labels. The attributes &lt;em>tech1&lt;/em>, &lt;em>fit&lt;/em>, &lt;em>tech2&lt;/em>, &lt;em>rank&lt;/em>, &lt;em>similar_item&lt;/em>, &lt;em>date&lt;/em>, &lt;em>price&lt;/em> have no/ extremely less filled in. That made them also invalid for being training labels. The attributes &lt;em>image&lt;/em>, &lt;em>description&lt;/em> and &lt;em>feature&lt;/em> is unique per item and hard to find the similarity in numeric purpose and then hard to be used as labels. Therefore, only attributes &lt;strong>also_buy&lt;/strong>, &lt;strong>also_view&lt;/strong>, &lt;strong>asin&lt;/strong> are trained as attributes and labels in this algorithm. &lt;strong>Figure 1&lt;/strong> is a shortcut for the raw database.&lt;/p>
&lt;pre>&lt;code>THE RAW DATABASE
The size of DATABASE : 1547
0
0 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
1 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
2 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
3 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
4 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
... ...
1542 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
1543 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
1544 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
1545 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
1546 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
[1547 rows x 1 columns]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 1:&lt;/strong> The raw database&lt;/p>
&lt;p>For the training purpose, all asins that appeared in the dataset, either from &lt;em>also_buy &amp;amp; also_view&lt;/em> list or * asin*, have to be reformatted from alphabet character to numeric character. For example, the original label for a particular item may be called **B001BKEWF2**. It will now be reformatted to a numeric number as 0. In that case, it can be a better fit-in the next step training method and easy to track. This step will be essential since it will help the also_view and also_buy dataset to be reformatted and make sure they are reformed in the track without overlapping each other. Therefore, a reformat_asin function is called for reformatting all the asins in the dataset and is performed as a dictionary. A shortcut for the *Asin Dictionary* is shown in **Figure 2**.&lt;/p>
&lt;pre>&lt;code>The 4561 Lines of Reformatted ASIN reference dictionary as following.
{'B001BKEWF2': 0, 'B001GXRQW0': 1, 'B001H53QE4': 2, 'B001H53QEO': 3, 'B001KMWN2K': 4, 'B001M1UVQO': 5,
'B001M1UVZA': 6, 'B001M5GKHE': 7, 'B002BSHDJK': 8, 'B002DN7XS4': 9, 'B002H9RN0C': 10, 'B002MS7BPA': 11,
'B002NZXF9S': 12, 'B002O018DM': 13, 'B002O0536U': 14, 'B002OOBESC': 15, 'B002PY04EG': 16, 'B002QFXC7U': 17,
'B002QTM0Y2': 18, 'B002QTPZUI': 19, 'B002SC9DRO': 20, 'B002UKLD7M': 21, 'B002VFYGC0': 22, 'B002VG4AR0': 23,
'B002VG4BRO': 24, 'B002W8YL6W': 25, 'B002XNLC04': 26, 'B002XNOVDE': 27, 'B002YEWXZ0': 28, 'B002YEWXMI': 29,
'B003755QI6': 30, 'B003CMYYGY': 31, 'B003NALDC8': 32, 'B003XNIBTS': 33, 'B003ZYIKDM': 34, 'B00414Y7Y6': 35,
'B0046IIHMK': 36, 'B004BVCHDC': 37, 'B004CG61UQ': 38, 'B004CZRZKW': 39, 'B004D01QJ2': 40, 'B004KNWWPE': 41,
'B004KNWWP4': 42, 'B004KNWWR2': 43, 'B004KNWWRC': 44, 'B004KNWWT0': 45, 'B004KNWWRW': 46, 'B004KNWWQ8': 47,
'B004KNWWNG': 48, 'B004KNWWPO': 49, 'B004KNWWXQ': 50, 'B004KNWWUE': 51, 'B004KNWWYU': 52, 'B004KNWWWC': 53,
'B004KNWX3A': 54, 'B004KNWX1W': 55, 'B004KNWWZE': 56, 'B004KNWWSQ': 57, 'B004KNWX4Y': 58, 'B004KNWX12': 59,
'B004KNWX3U': 60, 'B004KNWX62': 61, 'B004KNWX2Q': 62, 'B004KNWX6C': 63...}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 2:&lt;/strong> The ASIN dictionary&lt;/p>
&lt;p>Then the data contained in the each record&amp;rsquo;s attributes: &lt;strong>also_view&lt;/strong> &amp;amp; &lt;strong>also_buy&lt;/strong> will be reformated as &lt;strong>Figure 3&lt;/strong> and &lt;strong>Figure 4&lt;/strong>. &lt;strong>Figure 3&lt;/strong> is about the also_view item in reformatted numeric numbers based on each item customer purchased. &lt;strong>Figure 4&lt;/strong> is about the also_buy item in reformatted numeric numbers based on each item customer purchased.&lt;/p>
&lt;pre>&lt;code>also_view List: The first 10 lines
Item 0 : []
Item 1 : [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,
2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]
Item 2 : [2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 922, 2036, 283,
2037, 2038, 2001, 2000, 2013, 2039, 2040, 2007, 2041, 2042, 2009, 1233, 2043,
2014, 234, 2044, 2012, 2005, 2045, 2046, 2002, 2047, 378, 2048, 1382, 2008,
2004, 2011, 2049, 2050, 2051, 2052, 2003, 2053, 2054, 2018, 2055, 2056]
Item 3 : []
Item 4 : []
Item 5 : []
Item 6 : []
Item 7 : []
Item 8 : []
Item 9 : []
Item 10 : [2057, 2058, 2059]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 3:&lt;/strong> The also_view list&lt;/p>
&lt;pre>&lt;code>also_buy List: The first 20 lines
Item 0 : []
Item 1 : []
Item 2 : [2026, 2028, 2027, 2049, 1382, 2037, 2012, 2023]
Item 3 : []
Item 4 : []
Item 5 : []
Item 6 : []
Item 7 : []
Item 8 : [4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233,
4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243,
4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252]
Item 9 : []
Item 10 : []
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 4:&lt;/strong> The also_buy list&lt;/p>
&lt;p>While the also_buy list and also_view list is addressed. It is also important to know how many times a particular item appeared in other items' also view list and also buy list. These dictionaries will help to calculate the recommendation rate later. &lt;strong>Figure 5&lt;/strong> and &lt;strong>Figure 6&lt;/strong> is an example for how many times item 2000 appeared in other item&amp;rsquo;s also_view and also_buy lists.&lt;/p>
&lt;pre>&lt;code>also_view dictionary: use Item 2000 as an example
Item 2000 : [1, 2, 11, 12, 51, 60, 63, 65, 66, 67, 85, 86, 90, 94, 99, 100, 101, 103, 107, 108, 113, 116, 123, 126, 127, 129, 130, 141, 142, 143, 145, 146, 147, 148, 194, 199, 200, 204, 217, 221, 225, 229, 230, 231, 232, 233, 234, 235, 251, 253, 254, 260, 264, 268, 269, 270, 271, 280, 284, 285, 286, 287, 288, 294, 295, 296, 298, 299, 305, 306, 307, 308, 309, 313, 319, 327, 328, 338, 339, 344, 346, 348, 355, 356, 360, 371, 372, 377, 380, 389, 394, 406, 407, 410, 415, 440, 456, 469, 480, 490, 494, 495, 496, 502, 505, 509, 511, 512, 514, 517, 519, 520, 527, 530, 548, 591, 595, 600, 608, 609, 621, 631, 633, 670, 671, 672, 673, 675, 681, 689, 691, 695, 697, 707, 708, 709, 719, 783, 792, 793, 796, 797, 801, 803, 804, 807, 810, 816, 817, 818, 819, 836, 840, 842, 856, 892, 902, 913, 914, 917, 921, 955, 968, 972, 974, 975, 979, 981, 990, 991, 997, 998, 999, 1000, 1001, 1003, 1005, 1006, 1007, 1010, 1011, 1014, 1015, 1017, 1018, 1023, 1024, 1026, 1027, 1028, 1031, 1032, 1035, 1037, 1038, 1039, 1040, 1042, 1043, 1050, 1069, 1070, 1084, 1114, 1115, 1116, 1117, 1119, 1143, 1153, 1171, 1175, 1192, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1207, 1208, 1213, 1217, 1218, 1220, 1222, 1233, 1236, 1238, 1242, 1244, 1245, 1246, 1249, 1251, 1258, 1268, 1270, 1280, 1285, 1289, 1290, 1292, 1295, 1315, 1318, 1319, 1324, 1328, 1330, 1333, 1336, 1341, 1345, 1346, 1347, 1348, 1352, 1359, 1361, 1365, 1366, 1373, 1378, 1384, 1389, 1394, 1395, 1396, 1403, 1405, 1406, 1407, 1414, 1415, 1417, 1418, 1419, 1420, 1423, 1424, 1426, 1427, 1430, 1431, 1432, 1433, 1434, 1437, 1443, 1453, 1454, 1455, 1457, 1458, 1462, 1463, 1464, 1467, 1468, 1469, 1470, 1472, 1474, 1475, 1477, 1478, 1480, 1481, 1482, 1486, 1488, 1492, 1496, 1497, 1498, 1499, 1500, 1501, 1504, 1505, 1506, 1508, 1509, 1512, 1513, 1514, 1515, 1523, 1530, 1533, 1537, 1539, 1546]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 5:&lt;/strong> The also_view dictionary&lt;/p>
&lt;pre>&lt;code>also_buy dictionary: use Item 2000 as an example
Item 2000 : [217, 231, 235, 236, 277, 284, 285, 286, 287, 306, 307, 308, 327,
359, 476, 482, 505, 583, 609, 719, 891, 922, 963, 1065, 1328, 1359,
1384, 1399, 1482, 1483, 1490, 1496, 1497, 1499, 1509, 1512, 1540]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 6:&lt;/strong> The also_buy dictionary&lt;/p>
&lt;h2 id="5-recommendation-rate-and-similarity-calculation">5. Recommendation Rate and Similarity Calculation&lt;/h2>
&lt;p>While all the dictionaries and attributes-label relationship are prepared in Part4, the recommendation rate calculation is addressed in this part. There are two types of similarity methods in this algorithm: &lt;strong>Cosine Similarity&lt;/strong> and &lt;strong>Euclidean Distance Similarity&lt;/strong> that perform the similarity calculation. Before calculating the similarity, the first step would be phrasing the recommendation rate for each item to another item. The &lt;strong>Figure 8&lt;/strong> is a shortcut for the recommendation rate matrix. It will use the logic in &lt;strong>Figure 7&lt;/strong>.&lt;/p>
&lt;pre>&lt;code>for item in the asin list:
for asin in the also_view dictionary:
if asin is founded in also_view dictionary[item] list:
score for this item increase 2
each item in the also_view_dict[asin]'s score will be also increase 2
for asin in the also_view dictionary:
if asin is founded in also_view dictionary[item] list:
score for this item increase 10
each item in the also_view_dict[asin]'s score will be also increase 10
for other scores which is currently 0, assigned the average value for it
return the overall matrix for the further step
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 7:&lt;/strong> The sudocode for giving the recommendation rate for the likelyhood of the next purchase item based the current purchase&lt;/p>
&lt;pre>&lt;code>Item 0 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 1 : [13.0, 52, 28, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 2, 2, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0 ...]
Item 2 : [29.5, 28, 182, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 4, 2, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5 ...]
Item 3 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 4 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 5 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 6 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 7 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 8 : [14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 290, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5 ...]
Item 9 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 10 : [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 6, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5 ...]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 8:&lt;/strong> The shortcut for recommenation rate matrix&lt;/p>
&lt;p>The first similarity method implemented is &lt;em>Cosine Similarity&lt;/em> &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. It will use the cosine of the angle between vectors(see &lt;strong>Figure 9&lt;/strong>) to address the similarity between different items. By implementing this method with &lt;em>sklearn.metrics.pairwise&lt;/em> package, it will rephrase the whole recommendation similarity as &lt;strong>table 2&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/images/cosine-similarity.png" alt="image info">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> The cosine similarity&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">item&lt;/th>
&lt;th style="text-align:center">0&lt;/th>
&lt;th style="text-align:center">1.&lt;/th>
&lt;th style="text-align:center">2&lt;/th>
&lt;th style="text-align:center">3&lt;/th>
&lt;th style="text-align:center">4&lt;/th>
&lt;th style="text-align:center">5&lt;/th>
&lt;th style="text-align:center">6&lt;/th>
&lt;th style="text-align:center">7.&lt;/th>
&lt;th style="text-align:center">8.&lt;/th>
&lt;th style="text-align:center">&amp;hellip;1547&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">&amp;hellip;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.928569&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.873242&lt;/td>
&lt;td style="text-align:center">&amp;hellip;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.928569&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">&amp;hellip;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>table 2:&lt;/strong> The shortcut for using consine similarity to address the recommendation result&lt;/p>
&lt;p>The second similarity method implemented is &lt;em>Euclidean Distance Similarity&lt;/em>&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. It will use Euclidean Distance to calculate the distance between each items as a way to calculate similarity (see &lt;strong>Figure 10&lt;/strong>). By implementing this method with &lt;em>scipy.spatial.distance_matrix&lt;/em> package, it will rephrase the whole recommendation similarity. &lt;strong>Figure 11&lt;/strong> is an example with the item 1.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/images/euclidean.png" alt="image info">&lt;/p>
&lt;p>&lt;strong>Figure 10&lt;/strong> The Euclidean Distance calculation&lt;/p>
&lt;pre>&lt;code>Item 1 : [1005.70671669 0. 1370.09142031 1005.70671669 1005.70671669
1005.70671669 1005.70671669 1005.70671669 710.89169358 1005.70671669
905.23339532 862.88933242 971.0745337 1005.70671669 1005.70671669
1005.70671669 1005.70671669 1005.70671669 1005.70671669 1005.70671669...]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 11:&lt;/strong> The Euclidean Distance similarity example of item 1&lt;/p>
&lt;h2 id="6-accuracy">6. Accuracy&lt;/h2>
&lt;p>The accuracy for the consine_similarity and euclidean distance similarity with the number of items already purchased is shown as &lt;strong>Figure 12&lt;/strong>. Here the blue line represented the cosine similarity, and the red line represented the euclidean distance similarity. As presented, the more item joined as purchased, the less likely both similarity algorithms accurately locate the next item the customer may want to purchase next. However, overall the consine_similarity performed better accuracy compared to Euclidean Distance similarity. In &lt;strong>Figure 13&lt;/strong>, both accurate number and both wrong number is addressed. Both wrong numbers changed dramatically after more already-purchased items joined. This fact convinces the prior statement: this algorithm works better when the given object is &lt;em>1&lt;/em> but can&amp;rsquo;t handle many purchased item.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/images/CosVSEuc.png" alt="image info">&lt;/p>
&lt;p>&lt;strong>Figure 12:&lt;/strong> The Cosine similarity and Euclidean Distance Accuracy Comparison&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/images/bothrightandwrong.png" alt="image info">&lt;/p>
&lt;p>&lt;strong>Figure 13:&lt;/strong> The bothright and bothwrong accuracy comparison&lt;/p>
&lt;h2 id="7-benchmark">7. Benchmark&lt;/h2>
&lt;p>The Benchmark for each step for the project is stated in &lt;strong>Figure 14&lt;/strong> The overall Time spent is affordable. The accuracy calculation part(57s) and the Euclidean Distance algorithm implementation(74s) have taken the majority of time for the running. The Accuracy time consumed would be considered proper since it will randomly assign one to ten items and perform recommendation items based on it. The time spent is necessary and should be considered normal. The Euclidean Distance algorithm would be considered making sense since it is trying to perform the difference in two 1547X1547 matrixs.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Name&lt;/th>
&lt;th style="text-align:center">Status&lt;/th>
&lt;th style="text-align:center">Time&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">Data Online Downloading Process&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">1.028&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Raw Database&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.529&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Database Reformatting process&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.587&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Recommendation Rate Calculation&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">1.197&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Consine_Similarity&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.835&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Euclidean distance&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">73.895&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Recommendation strategy showcase-Cosine_Similarity&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.003&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Recommendation strategy showcase-Euclidean distance&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.004&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Accuracy&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">57.119&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Showcase-Cosine_Similarity&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.003&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Showcase-Euclidean distance&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.003&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 14:&lt;/strong> Benchmark&lt;/p>
&lt;p>The time comparison for Cosine Similarity and Euclidean Distance Time Comparison is addressed in &lt;strong>Figure 15&lt;/strong> As stated, the euclidean distance algorithm has taken much more time than cosine similarity. Therefore, the cosine similarity should be considered as efficient in these two similarities.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/images/timecompare.png" alt="image info">&lt;/p>
&lt;p>&lt;strong>Figure 15:&lt;/strong> The Cosine Similarity and Euclidean Distance Time Comparison&lt;/p>
&lt;h2 id="8-conclusion">8. Conclusion&lt;/h2>
&lt;p>This project &lt;em>Big_tata_Application_in_E_commense.ipynb&lt;/em> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> is attempted to get a taste of the recommendation strategy based on &lt;em>User-Based Collaborative Filtering&lt;/em>. Based on this attemption, the two similarity methods: &lt;strong>Cosine Similarity&lt;/strong> and &lt;strong>Euclidean Distance&lt;/strong> are addressed. After analyzing accuracy and time consumption for each method, Cosine Similarity performed better in both the accuracy and implementation time. Therefore the cosine similarity method is recommended to use in the recommendation algorithm strategies.
This project should be aware of Limitations. Since the rating attribute is missing in the dataset, the recommendation rate was assigned by the author. Therefore, in real-world implementation, both methods' accuracy can be expected to be higher than in this project. Besides, the cross-section recommendation strategies are not implemented. This project is only focused on the gift card section recommendations. With the multiple aspects of goods customer purchases addressed, both methods' accuracy can also be expected to be higher.&lt;/p>
&lt;h2 id="9-acknowledgements">9. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="10-references">10. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Build a Recommendation Engine With Collaborative Filtering. Ajitsaria, A. 2020
&lt;a href="https://realpython.com/build-recommendation-engine-collaborative-filtering/">https://realpython.com/build-recommendation-engine-collaborative-filtering/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Justifying recommendations using distantly-labeled reviews and fined-grained aspects. Jianmo Ni, Jiacheng Li, Julian McAuley. Empirical Methods in Natural Language Processing (EMNLP), 2019 &lt;a href="http://jmcauley.ucsd.edu/data/amazon/">http://jmcauley.ucsd.edu/data/amazon/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>meta_Gift_Cards.json.gz &lt;a href="http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Gift_Cards.json.gz">http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Gift_Cards.json.gz&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Recommendation Systems : User-based Collaborative Filtering using N Nearest Neighbors. Ashay Pathak. 2019
&lt;a href="https://medium.com/sfu-cspmp/recommendation-systems-user-based-collaborative-filtering-using-n-nearest-neighbors-bf7361dc24e0">https://medium.com/sfu-cspmp/recommendation-systems-user-based-collaborative-filtering-using-n-nearest-neighbors-bf7361dc24e0&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Similarity and Distance Metrics for Data Science and Machine Learning. Gonzalo Ferreiro Volpi. 2019
&lt;a href="https://medium.com/dataseries/similarity-and-distance-metrics-for-data-science-and-machine-learning-e5121b3956f8">https://medium.com/dataseries/similarity-and-distance-metrics-for-data-science-and-machine-learning-e5121b3956f8&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Big_tata_Application_in_E_commense.ipynb &lt;a href="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/Big_tata_Application_in_E_commense.ipynb">https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/Big_tata_Application_in_E_commense.ipynb&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Residential Power Usage Prediction</title><link>/report/fa20-523-314/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-314/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-314/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-314/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Siny P. Raphel, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-314/">fa20-523-314&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-314/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>We are living in a technology-driven world. Innovations make human life easier. As science advances, the usage of electrical and electronic gadgets are leaping. This leads to the shoot up of power consumption. Weather plays an important role in power usage. Even the outbreak of Covid-19 has impacted daily power utilization. Similarly, many factors influence the use of electricity-driven appliances at homes. Monitoring these factors and consolidating them will result in a humungous amount of data. But analyzing this data will help to keep track of power consumption. This system provides a prediction of usage of electric power at residences in the future and will enable people to plan ahead of time and not be surprised by the monthly electricity bill.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-reason-to-choose-this-dataset">2. Reason to choose this dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-datasets">3. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-data-preprocessing">4. Data preprocessing&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-data-download-and-load">4.1 Data download and load&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-data-descriptive-analysis">4.2 Data descriptive analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-preprocessing-data">4.3 Preprocessing data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#44-merge-datasets">4.4 Merge datasets&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-exploratory-data-analysis">5. Exploratory Data Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-modeling">6. Modeling&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-split-data">6.1 Split Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-pipelines">6.2 Pipelines&lt;/a>&lt;/li>
&lt;li>&lt;a href="#63-metrics">6.3 Metrics&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#631-mean-squared-errormse">6.3.1 Mean squared error(MSE)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#632-root-mean-squared-errorrmse">6.3.2 Root mean squared error(RMSE)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#633-r-squaredr2-score">6.3.3 R-Squared(R2) Score&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#64-baseline-linear-regression-model">6.4 Baseline Linear Regression model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#65-other-regression-models">6.5 Other regression models&lt;/a>&lt;/li>
&lt;li>&lt;a href="#66-results">6.6 Results&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> power usage, big data, regression&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Electricity is an inevitable part of our day-to-day life. The residential power sector consumes about one-fifth of the total energy in the U.S. economy&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Most of the appliances in a household use electricity for its working. The usage of electricity in a residence depends on the standard of living of the country, weather conditions, family size, type of residence, etc&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Most of the houses in the USA are equipped with lightings and refrigerators using electric power. The usage of air conditioners is also increasing. From Figure 1, we can see that the top three categories for energy consumption are air conditioning, space heating, water heating as of 2015.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Residential electricity consumption by end use, 2015&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Climate change is one of the biggest challenges in our current time. As a result, temperatures are rising. Therefore, to analyze energy consumption, understanding weather variations are critical&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. As the temperature rises, the use of air conditioners is also rising. As shown in Figure 1, air conditioning is the primary source of power consumption in households. The weather change has also resulted in a drop in temperatures and variation in humidity. These results in secondary power consumers.&lt;/p>
&lt;p>Even though weather plays an important role in power usage, factors like household income, age of the residents, family type, etc also influence consumption. During the holidays' many people tend to spend time outside which reduces power utilization at their homes. Similarly, during the weekend, since most people have work off, the appliances will be frequently consumed compared to weekdays when they go to work. Our world is currently facing an epidemic. Most of the countries had months of lockdown periods. Schools and many workplaces were closed. People were not allowed to go out and so they were stuck in their homes. As a result, power expending reduced drastically everywhere other than residences. But during the lockdown period, the energy consumption of residences spiked.&lt;/p>
&lt;p>Most of the electric service providers like Duke, Dominion provide customers their consumption data so that customers are aware of their usages. Some providers give predictions on their future usages so that they are prepared.&lt;/p>
&lt;h2 id="2-reason-to-choose-this-dataset">2. Reason to choose this dataset&lt;/h2>
&lt;p>There were many datasets on residential power usage analysis in Kaggle itself. But most of them were three or four years old. This dataset has the recent data of power consumptions together with weather data of each day. Since the pandemic hit the world in 2019-2020, the availability of recent data is considered to be significant for the analysis.&lt;/p>
&lt;p>This dataset is chosen because,&lt;/p>
&lt;ol>
&lt;li>It has the latest power usage data - till August 2020.&lt;/li>
&lt;li>It has marked covid lockdown, vacations, weekdays and weekends which is a challenge for the prediction.&lt;/li>
&lt;/ol>
&lt;h2 id="3-datasets">3. Datasets&lt;/h2>
&lt;p>This project is based on the dataset, &lt;em>Residential Power Usage 3 years data&lt;/em> in Kaggle datasets&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The dataset contains data of hourly power consumption of a 2 storied house in Houston, Texas from 01-06-2016 to August 2020 and also weather conditions of each day like temperatures, humidity wind etc of that area. Each day is marked whether it is a weekday, weekend, vacation or COVID-19 lockdown.&lt;/p>
&lt;p>The project is intending to build a model to predict the future power consumption of a house with similar environments from the available data. Python&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> is used for the development and since the expected output is a continuous variable, linear regression is considered for the baseline model. Later the performance of the base model is compared to one or two other models like tuned linear regression, gradient boosting, Light Gbm, or random forest.&lt;/p>
&lt;p>Data is spread across two csv files.&lt;/p>
&lt;ul>
&lt;li>power_usage_2016_to_2020.csv&lt;/li>
&lt;/ul>
&lt;p>This file depicts the hourly electricity usage of the house for three years, from 2016 to 2020. It contains basic details like startdate with hour, the value of power consumption in kwh, day of the week and notes. It has 4 features and 35953 instances.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/fig-1.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> First five rows of power_usage_2016_to_2020 data&lt;/p>
&lt;p>Figure 2 provides a snapshot of the first few rows of the data. Day of the week is an integer value with 0 being Monday. The column &lt;em>notes&lt;/em> layout details like whether that day was weekend, weekday, covid lockdown or vacation, as shown in Figure 3.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/fig-2.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Details in notes column&lt;/p>
&lt;ul>
&lt;li>weather_2016_2020_daily.csv&lt;/li>
&lt;/ul>
&lt;p>The second file or the weather file imparts the weather conditions of that particular area on each day. It has 19 features and 1553 instances. Figure 4 is the snapshot of the first few rows and columns of this file.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/fig-3.png" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> First few rows of weather_2016_2020_daily data&lt;/p>
&lt;p>Each feature in this data has different units and the units of the features are given in Table 1.&lt;/p>
&lt;p>&lt;strong>Table 1:&lt;/strong> Feature units&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature names&lt;/th>
&lt;th>Units&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Temperature&lt;/td>
&lt;td>F deg&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dew Point&lt;/td>
&lt;td>F deg&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Humidity&lt;/td>
&lt;td>%age&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Wind&lt;/td>
&lt;td>mph&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pressure&lt;/td>
&lt;td>Hg&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Precipitation&lt;/td>
&lt;td>inch&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Weather file has additional features like &lt;em>date&lt;/em> and &lt;em>day&lt;/em> of the date.&lt;/p>
&lt;h2 id="4-data-preprocessing">4. Data preprocessing&lt;/h2>
&lt;p>The data has to be preprocessed before modelling for predictions.&lt;/p>
&lt;h3 id="41-data-download-and-load">4.1 Data download and load&lt;/h3>
&lt;p>The data in this project is directly downloaded from &lt;em>Kaggle&lt;/em>. The downloaded file is then unzipped and loaded to two dataframes using python codes. For more detailed explanation and codes for download and load of data, see &lt;a href="https://github.com/cybertraining-dsc/fa20-523-314/blob/main/project/code/residential_power_usage_prediction.ipynb">python code&lt;/a> &lt;em>Download datasets&lt;/em> and &lt;em>Load datasets&lt;/em> sections.&lt;/p>
&lt;h3 id="42-data-descriptive-analysis">4.2 Data descriptive analysis&lt;/h3>
&lt;p>The data loaded has to be analyzed properly before it can be preprocessed. An analysis is made on the existence of missing values, the range of each feature, etc. On analysis, it is determined that there are no missing values and the date format in both tables is different. The &lt;em>StartDate&lt;/em> feature of the power_usage dataset and &lt;em>Date&lt;/em> feature of the weather dataset is to be used as a key to merge the two datasets. But the format of both features is different. StartDate feature is the combination of date and hour. Whereas, &lt;em>Date&lt;/em> feature of weather is just the date. Hence, these issues will have to be taken care of before merging data.&lt;/p>
&lt;h3 id="43-preprocessing-data">4.3 Preprocessing data&lt;/h3>
&lt;p>In this step, the column name &lt;em>Values (kWh)&lt;/em> is renamed to &lt;em>Value&lt;/em> and also date format issue is addressed. Firstly, StartDate column is split into Date and Hour columns. Since the StartDate column is in Pandas Period type, the function strftime() is used for converting to the required format.&lt;/p>
&lt;h3 id="44-merge-datasets">4.4 Merge datasets&lt;/h3>
&lt;p>For proper analysis of data, it is critical that the analyst should be able to analyze the relationships of each feature concerning the target feature(Value in kWh in this project). Therefore, both power_usage and weather tables are merged with respect to the Date column. The resulting table has a total of 35952 instances and 22 features.&lt;/p>
&lt;h2 id="5-exploratory-data-analysis">5. Exploratory Data Analysis&lt;/h2>
&lt;p>Here we analyze different features, their relationship with each other, and with the target.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/dow.png" alt="Figure 5">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Average power usage by day of the week&lt;/p>
&lt;p>In Figure 5, the average power usage by the day of the week is plotted&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. It is analyzed that Saturday and Friday have the most usage compared to other days of the week. Since the day of the week represents values Sunday-Saturday, we can consider it as a categorical feature.
Similarly, from Figure 6, there is a huge dip in power usage during vacation. Other three occasions like covid lockdown, weekend and weekdays have almost the same power usage, even though consumption during weekends outweigh.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/tod.png" alt="Figure 6">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Average power usage by type of the day&lt;/p>
&lt;p>In Figure 7, we compare the monthly power consumption for three years - 2018, 2019, 2020&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. The overall power usage in 2019 is less compared to 2018. But in 2020 may be due to Covid-lockdown the power consumption shoots. Also, power consumption peaks in the months of June, July, and August.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/monthly_power.png" alt="Figure 7">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Average power usage per month for three years&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/corr_plot.png" alt="Figure 8">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Correlation plot between features&lt;/p>
&lt;p>The correlation plot in Figure 8, depicts the inter-correlation between features. We can see that features like temperature, dew and pressure has a high correlation to our target feature. Also, different temperatures and dew features are inter-correlated. Therefore, all the intercorrelated features except for temp_avg can be dropped during feature selection.&lt;/p>
&lt;h2 id="6-modeling">6. Modeling&lt;/h2>
&lt;p>Modeling of the data includes splitting data into train and test, include cross-validation, create pipelines, select metrics for measuring performance, run data in regression models, and discuss results.&lt;/p>
&lt;h3 id="61-split-data">6.1 Split Data&lt;/h3>
&lt;p>For measuring the accuracy of the model, the main data is split into train and test. 20% of data is selected as test data and the remaining 80% is the train data. The proportion of notes(vacation, weekday, weekend, and covid lockdown) are different. Therefore, we stratify the data according to the notes column. After the split, train data has 28761 rows and test data has 7191 rows.&lt;/p>
&lt;h3 id="62-pipelines">6.2 Pipelines&lt;/h3>
&lt;p>Categorical variables and numeric variables are separated and processed in pipelines separately.
Categorical features are one hot encoded before feeding to the model. Similarly, numerical features are standardized before modeling. Later these two pipelines are joined and modeled used Linear regression and other models.&lt;/p>
&lt;h3 id="63-metrics">6.3 Metrics&lt;/h3>
&lt;p>Our target is a continuous variable and hence we implement regression models for prediction. To determine how accurate a regression model is, we use the following metrics.&lt;/p>
&lt;h4 id="631-mean-squared-errormse">6.3.1 Mean squared error(MSE)&lt;/h4>
&lt;p>MSE is the average of squares of error. The larger the MSE score, the larger the errors are. Models with lower values of MSE is considered to perform well. But, since MSE is the squared value, the scale of the target variable and MSE will be different. Therefore, we go for RMSE values.&lt;/p>
&lt;h4 id="632-root-mean-squared-errorrmse">6.3.2 Root mean squared error(RMSE)&lt;/h4>
&lt;p>RMSE is the square root of MSE scores. The square root is introduced to make the scale of the errors to be the same as the scale of targets. Similar to MSE, the lower scores for RMSE means better model performance. Therefore, in this project, the models with lower RMSE values will be monitored&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="633-r-squaredr2-score">6.3.3 R-Squared(R2) Score&lt;/h4>
&lt;p>R2 score is the goodness-of-fit measure. It&amp;rsquo;s a statistical measure that ranges between 0 and 1. R2 score helps the analyst to understand how similar the fitted line is to the data it is fitted to. The closer it is to one, the more likely the model predicts its variance. Similarly, if the score is zero, the model doesn&amp;rsquo;t predict any variance.
In this project, the R2 score of the test data is calculated. The model with the highest R2 scores will be considered&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="64-baseline-linear-regression-model">6.4 Baseline Linear Regression model&lt;/h3>
&lt;p>We use linear regression as our baseline model. For the baseline model, we are not hyperparameter tuning. For the baseline model, the train RMSE score was 0.6783, and R2 for the test set was 0.4460. These values are then compared to other regression models with hyperparameter tuning.&lt;/p>
&lt;h3 id="65-other-regression-models">6.5 Other regression models&lt;/h3>
&lt;p>After developing a baseline model, we are developing four other regression models and comparing the results. We implement feature selection and hyperparameter tuning. As we analyzed in exploratory data analysis, some features have strong inter-correlation and these features are dropped. The parameters for the regression models are hyper tuned and modeled in GridsearchCV of sklearn package.&lt;/p>
&lt;p>The models used for prediction are:&lt;/p>
&lt;ul>
&lt;li>Linear regression with hyperparameter tuning&lt;/li>
&lt;li>Gradient boosting&lt;/li>
&lt;li>XGBoost&lt;/li>
&lt;li>Light GBM&lt;/li>
&lt;/ul>
&lt;p>Similar to the baseline model, the metrics like train RMSE, test RMSE, and test R2 scores are calculated.&lt;/p>
&lt;h3 id="66-results">6.6 Results&lt;/h3>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/result.png" alt="Figure 9">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> Performance of all the regression models&lt;/p>
&lt;p>Figure 9 documents the performance of all the regression models used.&lt;/p>
&lt;p>cloudmesh.common benchmark and stopwatch framework are used to monitor and record the time taken for each step in this project&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. Time taken for critical steps like downloading data, loading data, preprocessing data, training and predictions of each model are recorded. The StopWatch recordings are shown in Table 2. StopWatch recordings played an important role in the selection of the best model. Benchmark also provides a detailed report on the system or device information as shown in Table 2.&lt;/p>
&lt;p>&lt;strong>Table 2:&lt;/strong> Benchmark results&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attribute&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BUG_REPORT_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://bugs.launchpad.net/ubuntu/%22">https://bugs.launchpad.net/ubuntu/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_DESCRIPTION&lt;/td>
&lt;td>&amp;ldquo;Ubuntu 18.04.5 LTS&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_ID&lt;/td>
&lt;td>Ubuntu&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_RELEASE&lt;/td>
&lt;td>18.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HOME_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://www.ubuntu.com/%22">https://www.ubuntu.com/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ID&lt;/td>
&lt;td>ubuntu&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ID_LIKE&lt;/td>
&lt;td>debian&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NAME&lt;/td>
&lt;td>&amp;ldquo;Ubuntu&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PRETTY_NAME&lt;/td>
&lt;td>&amp;ldquo;Ubuntu 18.04.5 LTS&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PRIVACY_POLICY_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy%22">https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SUPPORT_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://help.ubuntu.com/%22">https://help.ubuntu.com/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UBUNTU_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION&lt;/td>
&lt;td>&amp;ldquo;18.04.5 LTS (Bionic Beaver)&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION_ID&lt;/td>
&lt;td>&amp;ldquo;18.04&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu_count&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.active&lt;/td>
&lt;td>1.0 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.available&lt;/td>
&lt;td>11.2 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.free&lt;/td>
&lt;td>8.5 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.inactive&lt;/td>
&lt;td>2.6 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.percent&lt;/td>
&lt;td>11.6 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.total&lt;/td>
&lt;td>12.7 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.used&lt;/td>
&lt;td>1.9 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>platform.version&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python&lt;/td>
&lt;td>3.6.9 (default, Oct 8 2020, 12:12:24)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>[GCC 8.4.0]&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.pip&lt;/td>
&lt;td>19.3.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.version&lt;/td>
&lt;td>3.6.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sys.platform&lt;/td>
&lt;td>linux&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.machine&lt;/td>
&lt;td>x86_64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.node&lt;/td>
&lt;td>a1f46a7ed3c2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.processor&lt;/td>
&lt;td>x86_64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.release&lt;/td>
&lt;td>4.19.112+&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.system&lt;/td>
&lt;td>Linux&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.version&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>user&lt;/td>
&lt;td>collab&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Status&lt;/th>
&lt;th>Time&lt;/th>
&lt;th>Sum&lt;/th>
&lt;th>Start&lt;/th>
&lt;th>tag&lt;/th>
&lt;th>Node&lt;/th>
&lt;th>User&lt;/th>
&lt;th>OS&lt;/th>
&lt;th>Version&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Data download&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>2.652&lt;/td>
&lt;td>2.652&lt;/td>
&lt;td>2020-11-30 12:43:50&lt;/td>
&lt;td>&lt;/td>
&lt;td>a1f46a7ed3c2&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Data load&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.074&lt;/td>
&lt;td>0.074&lt;/td>
&lt;td>2020-11-30 12:43:53&lt;/td>
&lt;td>&lt;/td>
&lt;td>a1f46a7ed3c2&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Data preprocessing&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>67.618&lt;/td>
&lt;td>67.618&lt;/td>
&lt;td>2020-11-30 12:43:53&lt;/td>
&lt;td>&lt;/td>
&lt;td>a1f46a7ed3c2&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Baseline Linear Regression&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>2.814&lt;/td>
&lt;td>2.814&lt;/td>
&lt;td>2020-11-30 12:45:03&lt;/td>
&lt;td>&lt;/td>
&lt;td>a1f46a7ed3c2&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Linear Regression&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>5.581&lt;/td>
&lt;td>5.581&lt;/td>
&lt;td>2020-11-30 12:45:06&lt;/td>
&lt;td>&lt;/td>
&lt;td>a1f46a7ed3c2&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gradient Boosting&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>244.868&lt;/td>
&lt;td>244.868&lt;/td>
&lt;td>2020-11-30 12:45:12&lt;/td>
&lt;td>&lt;/td>
&lt;td>a1f46a7ed3c2&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>XGBoost&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>2946.7&lt;/td>
&lt;td>2946.7&lt;/td>
&lt;td>2020-11-30 12:49:16&lt;/td>
&lt;td>&lt;/td>
&lt;td>a1f46a7ed3c2&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Light GBM&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>770.967&lt;/td>
&lt;td>770.967&lt;/td>
&lt;td>2020-11-30 13:38:23&lt;/td>
&lt;td>&lt;/td>
&lt;td>a1f46a7ed3c2&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For the baseline model, the RMSE values were high and R2 scores were small compared to all other regression models. The hyperparameter tuned linear regression model scores are better compared to the baseline model. But the other three models outweigh both linear models. XGBoost has the lowest RMSE and highest R2 score of all other models. But the time taken for execution is too long. Therefore, XGBoost is computationally expensive which leads us to ignore its scores. Gradient boosting and Light GBM have similar scores and hence the time taken for execution has to be considered as the deciding factor here. Gradient boosting completed 135 fits in 244.868 seconds whereas LightGBM took around 770.967 seconds for executing 3645 fits and then prediction. Since per fit execution time for Light GBM is too small, we consider Light GBM as the best model for predicting daily power usage of a residence with similar background conditions.&lt;/p>
&lt;p>The RMSE scores for Light GBM are .2896 for train and .2910 for the test. The R2 score for the test set is .6526.&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>As the importance of electricity is increasing, the need to know how or where the power usage increase will be a lifesaver for the electricity consumers. In this project, the daily power consumption of a house is analyzed and modeled for a prediction of electricity usage for residences with similar environments. The model considered a set of parameters like weather conditions, weekdays, type of days, etc. for prediction. Since the output is power consumption in kWh, we selected regression for modeling and prediction. Experiments are conducted on five regression models. After analyzing the experiment results, we concluded that the performance of the Light GBM model is better and faster compared to all other models.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>The author would like to express special thanks to Dr. Geoffrey Fox, Dr. Gregor von Laszewski, and all the associate instructors of the Big Data Applications course (FA20-BL-ENGR-E534-11530) offered by Indiana University, Bloomington for their continuous guidance and support throughout the project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Jia Li and Richard E. Just, Modeling household energy consumption and adoption of energy efficient technology, Energy Economics, vol. 72, pp. 404-415, 2018.
Available: &lt;a href="https://www.sciencedirect.com/science/article/pii/S0140988318301440#bbb0180">https://www.sciencedirect.com/science/article/pii/S0140988318301440#bbb0180&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Domestic Power Consumption, [Online resource] &lt;a href="https://en.wikipedia.org/wiki/Domestic_energy_consumption">https://en.wikipedia.org/wiki/Domestic_energy_consumption&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource] &lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Yating Li, William A. Pizer, and Libo Wu, Climate change and residential electricity consumption in the Yangtze River Delta, China, Research article, Available: &lt;a href="https://www.pnas.org/content/116/2/472#ref-1">https://www.pnas.org/content/116/2/472#ref-1&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Residential Power Usage dataset, &lt;a href="https://www.kaggle.com/srinuti/residential-power-usage-3years-data-timeseries">https://www.kaggle.com/srinuti/residential-power-usage-3years-data-timeseries&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Residential Power Usage Prediction script, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-314/blob/main/project/code/residential_power_usage_prediction.ipynb">https://github.com/cybertraining-dsc/fa20-523-314/blob/main/project/code/residential_power_usage_prediction.ipynb&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>seaborn: statistical data visualization, &lt;a href="https://seaborn.pydata.org/index.html">https://seaborn.pydata.org/index.html&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Group by: split-apply-combine, &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html">https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Mean Square Error &amp;amp; R2 Score Clearly Explained, [Online resource] &lt;a href="https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/">https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data Applications in the Gaming Industry</title><link>/report/fa20-523-340/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-340/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-340/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-340/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-340/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-340/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Aleksandr Linde, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-340/">fa20-523-340&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-340/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Gaming is one of the fastest growing aspects of the modern entertainment industry. It’s a rapidly evolving market, where trends can change in a near instant, meaning that companies need to be ready for near anything when making decisions that may impact development times, targets and milestones. Companies need to be able to see market trends as they happen, not post factum, which frequently means predicting things based off of freshly incoming data. Big data is also used for development of the games themselves, allowing for new experiences and capabilities. It’s a relatively new use for big data, but as AI capabilities in games are developed further this is becoming a very important method of providing more immersive experiences. Last use case that will be talked about, is monetization in games, as big data has also found a use there as well.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1--introduction">1. Introduction&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#11-market-expansion--segmentation">1.1 Market Expansion &amp;amp; Segmentation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#2-big-data-in-mobile-gaming-spaces">2. Big data in mobile gaming spaces.&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-mobile-game-monetizaton">2.1 Mobile game monetizaton&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-profits-from-monetization">2.2 Profits from Monetization.&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-big-data-and-ai-development-on-in-game-ai-systems">3. Big Data and AI Development on In-Game AI Systems&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-ai-implementation">3.1. AI implementation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-level-design-and-balance-an-unlikely-big-data-application">4. Level Design and Balance, An Unlikely Big Data Application&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-balance">4.1. Balance&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-what-the-future-holds">5. What the future holds&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#acknowledgements">Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-refernces">7. Refernces&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> gaming, big data, product development, computer science, technology, microtransactions, artifician intelligence&lt;/p>
&lt;h2 id="1--introduction">1. Introduction&lt;/h2>
&lt;p>The video game market is one of the fastest growing aspects of the modern entertainment industry, and in 2020 brought in 92 billion USD worldwide out of a total worldwide entertainment market value income of 199.64 billion USD. With global player count reaching 2.7 billion users, more and more people choose to spend some of their leisure time behind a controller or a keyboard &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This phenomenon isn’t exactly a new thing. Originally getting its start in 1972 with the release of the Magnavox Odyssey, the first home console with replaceable cartridges. The scale of this achievement was hardly recognized, as back then if you wanted to play something, the only option was arcades. Arcades were a social experience but being able to play the exact same titles, if slightly downgraded, at home was a breakthrough. These first gen consoles were quite clunky and by modern standards unimpressive, yet they were the vital first step for birthing what we now know today as the video game market. At that point games had been a thing for a around a decade, but they were primarily limited to a group of computer hobbyists, who would exchange copies of homebrew software amongst themselves. In this format it would be impossible to get any sort of mainstream popularity. With time, home consoles had changed this dramatically. Games become a mainstream phenomenon, which means that suddenly the potential player pool is a lot larger, and as a result we see an explosion in the popularity of gaming.&lt;/p>
&lt;h3 id="11-market-expansion--segmentation">1.1 Market Expansion &amp;amp; Segmentation&lt;/h3>
&lt;p>Over the decades, this market has grown into a massive global phenomenon, becoming one of the primary forms of media alongside film, music, and art. Thanks to all of this we have seen 3 distinct market segments emerge.&lt;/p>
&lt;ol>
&lt;li>Personal Computers – (Laptops and desktops)&lt;/li>
&lt;li>Consoles – (Nintendo Switch, 3DS, Xbox, Playstation)&lt;/li>
&lt;li>Mobile Phones – (Anything with the Google Play store and IOS Appstore)&lt;/li>
&lt;/ol>
&lt;p>Each segment has some interesting specifics. Mobile games account for 33% of all app downloads, 74% of all mobile consumer spending and 10% of all raw time spent in apps. By the end of 2019 the amount of people who played mobile games topped 2.4 billion &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. An important reason for this is accessibility, since mobile phones as of today, are the most commonly bought tech item in the world. In many developing nations, mobile phones are a commonplace piece of tech that is owned by the majority of adults &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.As the global population increases in wealth and size, this trend is only set to increase. Meaning that any company that ignores the mobile phone market is loosing out of massive sums of money. The same is true, but for a lesser scale, in personal computers, meaning that as the population grows, the PC market will expand as well. This is less true for consoles but machines from the last console generation sold a combined 221 million units, with a yearly revenue of 48.7 Billion in 2019 &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. Far cry from what mobile games make, but still rather significant, spelling good fortunes for the health of the industry in the coming years. As a result of all this, it is only natural that companies will focus more and more of their attention on the developing world for expansion This is where big data can help significantly,&lt;/p>
&lt;h2 id="2-big-data-in-mobile-gaming-spaces">2. Big data in mobile gaming spaces.&lt;/h2>
&lt;p>A big reason for the integration of big data analytics into mobile games comes from the intelligence edge that it provides you in a competitive market space. Being able to track Key Performance Indicators, or KPI’s for short allows you to rapidly shift strategies to fix growing problems as soon as you see them. For example, customer retention is one of, if not the most important metric in any gaming product, but or mobile gaming this is especially critical since gaming attention spans are getting shorter, and in a market where attention spans are already low from the get-go, hemorrhaging customers can spell doom for any app &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Thus, an important big data application is the CLTV – customer lifetime analysis, which tracks customer retention and average return per customer on a specific platform. Machine learning in this case factors in a lot of different variables that effect just how long the user will end up using your app before putting it down to go do something else. Reducing the churn, or the rate at which you intake and expel customers is thus a key business consideration when bringing a new gaming product to market &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. This is more important for mobile game customers since the ROI per user on mobile platforms is generally lower, thus maximizing user count rapidly becomes an extremely important tactic to get a self-sustaining userbase that can bring in stable profits &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. But how are games monetized in the first place, and how does big data play into it?&lt;/p>
&lt;h3 id="21-mobile-game-monetizaton">2.1 Mobile game monetizaton&lt;/h3>
&lt;p>Most mobile games operate on the freemium model, where the base game is free, but you pay for things such as boosts, upgrades and cosmetics &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Systems such as this still allow you to earn most things in the game normally, yet make it prohibitively difficult to do so, requiring a large time investment. What happens often in this case is that players will spend money to save time and effort that would have normally gone into grinding out these items for free [9]. So why is this effective? Because the initial entry barrier is quite literally nonexistent and the main monetization fees don’t actually cost that much at first, making up mostly 5-10$ purchases&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. This doesn’t seem like much, but this eventually reaches into the sunk cost fallacy where users get so ingrained in an ecosystem and simply don’t want to leave. Mobile monetization platform Tapjoy recently used big data analysis to identify 5 different categories of mobile users &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. The one that interests us the most is the whales. Whales are called as such due to how despite making up only 10% of a game’s population, they will usually be responsible for 70% of the cash flow from games. Many developers work specifically to design systems that aren’t exactly fun but work more to trap players like this within the ecosystem. While we may not agree with this rather predatory practice, its another big data application to be cognizant of.
It works off the same principle that gambling does, via enticing potential rewards that don’t actually have publicly avaliable information about your actual chance to win. When the positive effect happens, it’s usually minor, but still works like a Skinner box trigger where the user gets the positive feedback that further keeps them in the churn loop &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Is it scummy? Many players think so but using big data in this case allows us to hyper target these users. Big data analytics generate user reports that show directly what users are most susceptible, how to reach them and most importantly how to stick them into the endless loop where they keep dumping more and more money into a game that many don’t really even love anymore &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup> . For companies this is great, since it earns you a customer who is guaranteed to stay and dump money in a marketplace where getting the average user to pay even 5$ for anything is already a feat that many cant handle. Using big data lets developers and companies be 10 steps ahead of potential users. By the time they realize they are addicted its far too late. Similar systems are making their way into our desktop and console spaces as well.&lt;/p>
&lt;h3 id="22-profits-from-monetization">2.2 Profits from Monetization.&lt;/h3>
&lt;p>In fact, microtransactions that function this way can now be found on every platform and genre, all due to how insanely profitable it is. This year, gaming industry valuations rose 30% because of the absurd amounts of money that get pulled in via microtransactions &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. All of this, possible only due to the massively increasing use of big data analytical tools. Why make a good game, when you can hyper tailor the monetization so that players are guaranteed to stop caring about how good your game actually is when they get sufficiently sucked in enough. This trend is only increasing since its predicted that by 2023, 99% of all game downloads will be free to play with various forms of microtransaction based monetization &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. However, its not all doom and gloom. Big data can also be used for some other really interesting applications when it comes to developing the game itself, and not just the predatory monetization methodology.&lt;/p>
&lt;h2 id="3-big-data-and-ai-development-on-in-game-ai-systems">3. Big Data and AI Development on In-Game AI Systems&lt;/h2>
&lt;p>Last summer, the world of online poker had quite the shock when a machine learning algorithm beat 4 seasoned poker vets &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>. But poker is actually a fairly simple game, so why is this important? It’s a big deal due to how most games feature AI in one form or another. When one plays strategy games, they can usually immediately tell if they are playing versus an AI or vs an actual player due to how current AI tech still isn’t nearly as good as a regular player. However, in simple games like poker, you can make systems that almost perfectly copy human behavior. All that is required is that they are trained on the users of the game, and then become nearly indistinguishable from a regular player. This opens up massive new possibilities, because soon we will be able to mimic whole players so that even games that are functionally dead due to lower player counts can still have users enjoy content that was made for multiplayer and such. Plus, just having smarter AI for non-player characters and enemies would be a nice touch. Currently most games that have AI opponents function on a system that is called a Finite State Machine &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>. Systems like this have a strict instruction set that they can’t really deviate from, nor make new strategies.This causes everythign to feel scripted and dumb, yet this is also a fairly lightweight method of ai control. FSM can be refined into what is called a Monte Carlo Search Tree (MCST) algorithm, where computers will on their own, make decision trees based on the reward value of the tree endpoint. MCST’s can become massive, so in order to cut down on the sheer amount of processing power that is needed, developers will make the AI randomly select a few paths, which will then in turn be implemented as actions. This cut down version adds the randomness that players expect from other players, but also removes a lot of rigidity of traditional AI systems &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>. Using machine learning models on real player actions allows MCST models to be really close to how an actual player acts in specific situations. However, training machine learning models on players also has another, really interesting purpose – dual AI systems.&lt;/p>
&lt;h3 id="31-ai-implementation">3.1. AI implementation&lt;/h3>
&lt;p>Some games, feature AI that is divided into two separate algorithms, the director, and the controller. Director AI, has only one objective, make the game experience as enjoyable as possible. It is a macro level passive AI that bases game triggers and events off of player action. For example, causing random noises when it detects player stress via their control inputs&lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>. This means that the system can detect whether it needs to up the ante on what is actually happening in game, introduce new enemies, change environmental effects etc. This molds the experience into a completely unique system that learns from every player that has ever played the game. All of this data goes into crafting emerging experiences that can’t be replicated via a rigid AI system. But this is only one piece of the puzzle as the other component of this system is the controller AI. Controller AI is the sidekick that the director AI uses to help immerse the player. We will explain how it works in detail, but what is important is that the controller AI is ultimately subordinate to the director.
In a normal AI system, the algorithm knows everything, can see everything and will pretend it has no idea about what the player is doing, yet is ultimately aware of their actions. This system is the easiest to make, but also can seem to players like the AI is cheating or being exceedingly stupid at times. Let’s use the example of an alien hunting down a player. Normally, the AI would just head to the players location and just see them once they enter detection range. What two tier systems do is limit the information flow to the controller AI from the director AI. The director sees all, but it limits what the controller can visualize. Instead of saying, &lt;em>Go to area, find player in location, attack player&lt;/em>, what the controller gets as input is – &lt;em>Go to area and look for player&lt;/em>. The director ai can set the area as anything. But what matters is that the end alien cannot actually see everything that’s happening. What it does is learn from the player in previous encounters. Early on in the game, it might just do a cursory pass around an area and leave, while later on, it might have found the player in a specific location and thus will now check for them in places like it. The alien learns as it goes, and the beauty of this system is that if you select a higher difficulty, then the game can just draw upon cloud data to fill in that early learning steps, dropping in smarter, more intelligent enemies much earlier in the game &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>&lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>. This is only possible via big data analysis. Any other solution would mean either massive amounts of very rigid code, or very blatant information cheating. Best example of this is Alien: Isolation a 2014 title by Creative Assembly, that impressed the gaming world with just how scary the AI implementation was. In our opinion this is the best example of a two tier system so far. Systems like this are only possible with big data implementation and the proper ML algorithms. As games advance to more and more realistic worlds, this approach, in our opinion is going to become the norm, since it allows for really flexible systems that are engaging to play around with. However, this isn’t the only way developers can make systems fairer and fun for the end user. Another area where big data is gaining lots of traction is level design.&lt;/p>
&lt;h2 id="4-level-design-and-balance-an-unlikely-big-data-application">4. Level Design and Balance, An Unlikely Big Data Application&lt;/h2>
&lt;p>When crafting virtual worlds its always difficult to strike a balance between breadth and scope. A common approach is to hand craft levels in games that do not require that much breadth. This approach has great benefits in terms of the attention to detail and depth of narrative, yet crafting each level by hand takes a while. Big data in this case helps with testing, since previously playtesting had players run the map time after time. Now, you can run hundreds of models in addition to the players, compare their paths, and find the best solution for what would increase player enjoyment. However, the main drawback of this approach is its great cost for the developer. Much moreso than just using a script to autogenerate terrain. Some developers will focus entirely on auto-generation as a means of level design instead, which can provide near infinite possibilities for content (see Minecraft), but has the danger of being exceedingly bland to the end user. Big data analytics are a way of alleviating this via looking at what terrain players prefer more, and thus adjusting level generation parameters accordingly. Thus, removing some of the randomness that such worlds depend on. However, this isn’t the main use of such analytics. A big reason why developers collect massive amounts of data about levels is actually balance issues. This is especially true for Esports titles that dominate the current games markets. Titles where minute advantages in map design get exploited to their fullest extent &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>. Using data collected from tens of thousands of matches, we can see what paths are most taken, what are optimal firing angles (most esports titles are shooters) and any detail that can give players a leg up over the competition. This allows maps to be fine tuned to produce the most memorable player experiences, feel hand crafted and also be as balanced as possible.&lt;/p>
&lt;h3 id="41-balance">4.1. Balance&lt;/h3>
&lt;p>Balance isn’t just a competitive thing however, multiplayer games live and die due to balance, as unbalanced gameplay drives away players, leaving only people who are ok with it, which in turn drives away new players &lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>. Ultimately this increases player churn, and you end up with a dead game &lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup>. Normally you would just use player feedback, but if you have mountains of raw data, ML and big data can also allow you to really fine tune specific aspects of balance. This isn’t only limited to maps, but also skills, abilities and puzzles. Balance has always been a really fine line between enjoyment and fairness yet it’s the developer’s job to ensure that no one is left out in the cold on purpose. Some genre’s are more balance intensive and thus require more data to make things fair even when taking into randomness into action.&lt;/p>
&lt;p>A great example are MOBAs, Multiplayer Online Battle Arenas, where being down even 1 player can cause a team to get crushed in a 4v5 matchup. A single person leaving completely changes the dynamic of the game, and without big data its hard to compensate for events like that, since they are at their core, massively unbalanced. But being able to account for literally any situation, allows developers to craft systems that can handle disbalance better in these cases &lt;sup id="fnref:21">&lt;a href="#fn:21" class="footnote-ref" role="doc-noteref">21&lt;/a>&lt;/sup>.
Anything that includes player vs player is very difficult to balance for as well. A good use for big data in this case would be truly fair matchmaking. Currently most systems of this sort use only things like win rate and total playtime, in order to pair up players. However, if we start watching for minute flags on how well players actually play, we can make systems that are specifically made to balance players as much as possible by looking at the minute details of how they actually play.&lt;/p>
&lt;h2 id="5-what-the-future-holds">5. What the future holds&lt;/h2>
&lt;p>One thing we can definitely count on is the sheer amount of data that is going to be generated from now on by gaming. With always online experiences being the norm, machines send data reports back to the main system which matches them with other players in the same instance to allow playing in the same world. As more and more players enter the community, we will run into the issue of where an absurdly massive amount of data is being generated, and not all of it is positive. Its going to get harder to get the full picture since now looking over all the data would make the task of sifting through it extremely difficult.
What is most exciting is the developments of self-learning AI based on the mountains of this newfound data. Currently unless you use two tier AI systems, there isn’t a way to make AI believable, and having bad AI can potentially ruin a game for most players. With more advancements in reliable AI systems based on potent data analysis, there exists the potential for AI that is near indistinguishable from players &lt;sup id="fnref:22">&lt;a href="#fn:22" class="footnote-ref" role="doc-noteref">22&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>Gaming has gone from a very niche and simple pastime to one of the biggest entertainment markets in the world. From mobile phones, to supercharged desktop setups, the field is expansive to the point where there is something for anyone who wants to have blow a couple of hours behind a screen. The speed at which the field is developing is truly astonishing, and a good portion of it is being driven by the previously discussed Big Data use cases. By being able to use all this data for developmental purposes, game companies and publishers can craft truly memorable and interesting experiences.It has been demonstrated that big data analytics is having some very profound effects on the video game markets from pretty much every side. The end goal of this developemnt is systems that can predict nearly any game situation and adjuist parameters accordingly to maximise player enjoyment. It means, more advanced AI systems that feel as lifelike as humanly possible that can populate virtual worlds. It means systems that can properly react to player inputs in order to create dynamic worlds that aren&amp;rsquo;t just a static canvas that a player is thrust into upon booting up the game. This may seem far off, especially with how a lot of development teams dont exactly use Big Data in the right way, but the best examples are getting quite close in its implementation.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>I would like to thank Dr Gregor von Laszewski for helping me despite me taking a long time with quite a few assignments. I want to thank the AI team for their massive help with any issues that arose in during the class period, as well as the excellent avaliablity of their help whenever it was needed. Would like to thank Dr. Geoffery Fox for his very informative and thorough class.&lt;/p>
&lt;h2 id="7-refernces">7. Refernces&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>The Average Gamer: How the Demographics Have Shifted. (n.d.). Retrieved December 08, 2020, from &lt;a href="https://www.gamesparks.com/blog/the-average-gamer-how-the-demographics-have-shifted/">https://www.gamesparks.com/blog/the-average-gamer-how-the-demographics-have-shifted/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Nakamura, Y. (2019, January 23). Peak Video Game? Retrieved December 08, 2020, from &lt;a href="https://www.bloomberg.com/news/articles/2019-01-23/peak-video-game-top-analyst-sees-industry-slumping-in-2019">https://www.bloomberg.com/news/articles/2019-01-23/peak-video-game-top-analyst-sees-industry-slumping-in-2019&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Kaplan, O. (2019, August 22). Mobile gaming is a $68.5 billion global business, and investors are buying in. Retrieved December 08, 2020, from &lt;a href="https://techcrunch.com/2019/08/22/mobile-gaming-mints-money/">https://techcrunch.com/2019/08/22/mobile-gaming-mints-money/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Silver, L., Smith, A., Johnson, C., Jiang, J., Anderson, M., &amp;amp; Rainie, L. (2020, August 25). 1. Use of smartphones and social media is common across most emerging economies. Retrieved December 08, 2020, from &lt;a href="https://www.pewresearch.org/internet/2019/03/07/use-of-smartphones-and-social-media-is-common-across-most-emerging-economies/">https://www.pewresearch.org/internet/2019/03/07/use-of-smartphones-and-social-media-is-common-across-most-emerging-economies/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Ali, A. (2020, November 10). The State of the Multi-Billion Dollar Console Gaming Market. Retrieved December 08, 2020, from &lt;a href="https://www.visualcapitalist.com/multi-billion-dollar-console-gaming-market/">https://www.visualcapitalist.com/multi-billion-dollar-console-gaming-market/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Filippo, A. (2019, December 17). Our attention spans are changing, and so must game design. Retrieved December 08, 2020, from &lt;a href="https://www.polygon.com/2019/12/17/20928761/game-design-subscriptions-attention">https://www.polygon.com/2019/12/17/20928761/game-design-subscriptions-attention&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Addepto. (2019, March 07). Benefits of Big Data Analytics in the Mobile Gaming Industry. Retrieved December 08, 2020, from &lt;a href="https://medium.com/datadriveninvestor/benefits-of-big-data-analytics-in-the-mobile-gaming-industry-2b4747b90878">https://medium.com/datadriveninvestor/benefits-of-big-data-analytics-in-the-mobile-gaming-industry-2b4747b90878&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Rands, D., &amp;amp; Rands, K. (2018, January 26). How big data is disrupting the gaming industry. Retrieved December 08, 2020, from &lt;a href="https://www.cio.com/article/3251172/how-big-data-is-disrupting-the-gaming-industry.html">https://www.cio.com/article/3251172/how-big-data-is-disrupting-the-gaming-industry.html&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Matrofailo, I. (2015, December 21). Retention and LTV as Core Metrics to Measure Mobile Game Performance. Retrieved December 08, 2020, from &lt;a href="https://medium.com/@imatrof/retention-and-ltv-as-core-metrics-to-measure-mobile-game-performance-89229e70f710">https://medium.com/@imatrof/retention-and-ltv-as-core-metrics-to-measure-mobile-game-performance-89229e70f710&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Batt, S. (2018, October 04). What Is a &amp;ldquo;Whale&amp;rdquo; In Mobile Gaming? Retrieved December 08, 2020, from &lt;a href="https://www.maketecheasier.com/what-is-whale-in-mobile-gaming/">https://www.maketecheasier.com/what-is-whale-in-mobile-gaming/&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Shaul, B. (2016, March 01). Infographic: &amp;lsquo;Whales&amp;rsquo; Account for 70% of In-App Purchase Revenue. Retrieved December 08, 2020, from &lt;a href="https://www.adweek.com/digital/infographic-whales-account-for-70-of-in-app-purchase-revenue/">https://www.adweek.com/digital/infographic-whales-account-for-70-of-in-app-purchase-revenue/&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Perez, D. (2012, January 13). Skinner&amp;rsquo;s Box and Video Games: How to Create Addictive Games - LevelSkip - Video Games. Retrieved December 08, 2020, from &lt;a href="https://levelskip.com/how-to/Skinners-Box-and-Video-Games">https://levelskip.com/how-to/Skinners-Box-and-Video-Games&lt;/a>&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Muench Frederickm (2014, March 18), The New Skinner Box: We and Mobile Analytics, December 7th 2020, &lt;a href="https://www.psychologytoday.com/us/blog/more-tech-support/201403/the-new-skinner-box-web-and-mobile-analytics">https://www.psychologytoday.com/us/blog/more-tech-support/201403/the-new-skinner-box-web-and-mobile-analytics&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Gardner, M. (2020, September 19). Report: Gaming Industry Value To Rise 30%–With Thanks To Microtransactions. Retrieved December 08, 2020, from &lt;a href="https://www.forbes.com/sites/mattgardner1/2020/09/19/gaming-industry-value-200-billion-fortnite-microtransactions/?sh=3374fce32bb4">https://www.forbes.com/sites/mattgardner1/2020/09/19/gaming-industry-value-200-billion-fortnite-microtransactions/?sh=3374fce32bb4&lt;/a>&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>Gardner, M. (2020, June 11). What&amp;rsquo;s The Future Of Gaming? Industry Professors Tell Us What To Expect. Retrieved December 08, 2020, from &lt;a href="https://www.forbes.com/sites/mattgardner1/2020/06/11/whats-the-future-of-gaming-industry-professors-tell-us-what-to-expect/">https://www.forbes.com/sites/mattgardner1/2020/06/11/whats-the-future-of-gaming-industry-professors-tell-us-what-to-expect/&lt;/a>&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16" role="doc-endnote">
&lt;p>Maass, L. (2019, July 01). Artificial Intelligence in Video Games. Retrieved December 08, 2020, from &lt;a href="https://towardsdatascience.com/artificial-intelligence-in-video-games-3e2566d59c22">https://towardsdatascience.com/artificial-intelligence-in-video-games-3e2566d59c22&lt;/a>&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17" role="doc-endnote">
&lt;p>Burford, G. (2016, April 26). Alien Isolation&amp;rsquo;s Artificial Intelligence Was Good&amp;hellip;Too Good. Retrieved December 08, 2020, from &lt;a href="https://kotaku.com/alien-isolations-artificial-intelligence-was-good-too-1714227179">https://kotaku.com/alien-isolations-artificial-intelligence-was-good-too-1714227179&lt;/a>&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18" role="doc-endnote">
&lt;p>Ozyazgan, E. (2019, December 14). The Data Science Boom in Esports. Retrieved December 08, 2020, from &lt;a href="https://towardsdatascience.com/the-data-science-boom-in-esports-8cf9a59fd573/">https://towardsdatascience.com/the-data-science-boom-in-esports-8cf9a59fd573/&lt;/a>&amp;#160;&lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19" role="doc-endnote">
&lt;p>Cormack, L. (2018, June 29). Balancing game data with player data - DR Studios/505 Games. Retrieved December 08, 2020, from &lt;a href="https://deltadna.com/blog/balancing-game-data-player-data/">https://deltadna.com/blog/balancing-game-data-player-data/&lt;/a>&amp;#160;&lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20" role="doc-endnote">
&lt;p>Sergeev, A. (2019, July 15). Analytics of Map Design: Use Big Data to Build Levels. Retrieved December 08, 2020, from &lt;a href="https://80.lv/articles/analytics-of-map-design-use-big-data-to-build-levels/">https://80.lv/articles/analytics-of-map-design-use-big-data-to-build-levels/&lt;/a>&amp;#160;&lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:21" role="doc-endnote">
&lt;p>Site Admin. (2017, March 18). Retrieved December 08, 2020, from &lt;a href="http://dmtolpeko.com/2017/03/18/moba-games-analytics-platform-balance-details/">http://dmtolpeko.com/2017/03/18/moba-games-analytics-platform-balance-details/&lt;/a>&amp;#160;&lt;a href="#fnref:21" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:22" role="doc-endnote">
&lt;p>Is AI in Video Games the Future of Gaming? (2020, November 21). Retrieved December 08, 2020, from &lt;a href="https://www.gamedesigning.org/gaming/artificial-intelligence/">https://www.gamedesigning.org/gaming/artificial-intelligence/&lt;/a>&amp;#160;&lt;a href="#fnref:22" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Forecasting Natural Gas Demand/Supply</title><link>/report/sp21-599-356/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/sp21-599-356/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-356/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-356/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Baekeun Park, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/">sp21-599-356&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Code:
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/blob/main/project/code/Forecasting_NG_Demand_Supply.ipynb">Forecasting_NG_Demand_Supply.ipynb&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Natural Gas(NG) is one of the valuable ones among the other energy resources. It is used as a heating source for homes and businesses through city gas companies and utilized as a raw material for power plants to generate electricity. Through this, it can be seen that various purposes of NG demand arise in the different fields. In addition, it is essential to identify accurate demand for NG as there is growing volatility in energy demand depending on the direction of the government&amp;rsquo;s environmental policy.&lt;/p>
&lt;p>This project focuses on building the model of forecasting the NG demand and supply amount of South Korea, which relies on imports for much of its energy sources. Datasets for training include various fields such as weather and prices of other energy resources, which are open-source.
Also, those are trained by using deep learning methods such as the multi-layer perceptron(MLP) with long short-term memory(LSTM), using Tensorflow. In addition, a combination of the dataset from various factors is created by using pandas for training scenario-wise, and the results are compared by changing the variables and analyzed by different viewpoints.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-related-work">2. Related Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-datasets">3. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#41-min-max-scaling">4.1. Min-Max scaling&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-training">4.2. Training&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-evaluation">4.3. Evaluation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#44-prediction">4.4. Prediction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-result">5. Result&lt;/a>&lt;/li>
&lt;li>&lt;a href="#51-scenario-oneregional-dataset">5.1 Scenario one(regional dataset)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-scenario-tworegional-climate-dataset">5.2 Scenario two(regional climate dataset)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-scenario-threeregional-temperature-dataset">5.3 Scenario three(regional temperature dataset)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#54-scenario-fourapplying-timesteps">5.4 Scenario four(applying timesteps)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#55-scenario-fivenational-dataset">5.5 Scenario five(national dataset)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#56-overall-results">5.6 Overall results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmarks">6. Benchmarks&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-source-code">9. Source code&lt;/a>&lt;/li>
&lt;li>&lt;a href="#10-references">10. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Natural Gas, supply, forecasting, South Korea, MLP with LSTM, Tensorflow, various dataset.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>South Korea relies on imports for 92.8 percent of its energy resources as of the first half of 2020 &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Among the energy resources, the Korea Gas Corporation(KOGAS) imports Liquified Natural Gas(LNG) from around the world and supplies it to power generation plants, gas-utility companies, and city gas companies throughout the country &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. It produces and supplies NG in order to ensure a stable gas supply for the nation. Moreover, it operates LNG storage tanks at LNG acquisition bases, storing LNG during the season when city gas demand is low and replenish LNG during winter when demand is higher than supply &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The wholesale charges consist of raw material costs (LNG introduction and incidental costs) and gas supply costs &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Therefore, the forecasting NG demand/supply will help establish an optimized mid-to-long-term plan for the introduction of LNG and stable NG supply and economic effects.&lt;/p>
&lt;p>The factors which influence NG demand include weather, economic conditions, and petroleum prices. The winter weather strongly influences NG demand, and the hot summer weather can increase electric power demand for NG. In addition, some large-volume fuel consumers such as power plants and iron, steel, and paper mills can switch between NG, coal, and petroleum, depending on the cost of each fuel &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Therefore, some indicators related to weather, economic conditions, and the price of other energy resources can be used for this project.&lt;/p>
&lt;h2 id="2-related-work">2. Related Work&lt;/h2>
&lt;p>Khotanzad and Elragal (1999) proposed a two-stage system with the first stage containing a combination of artificial neural network(ANN) for prediction of daily NG consumption &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, and Khotanzad et al. (2000) combined eight different algorithms to improve the performance of forecasters &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Mustafa Akpinar et al. (2016) used daily NG consumption data to forecast the NG demand by ABC-based ANN &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. Also, Athanasios Anagnostis et al. (2019) conducted daily NG demand prediction by a comparative analysis between ANN and LSTM &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Unlike those methods, MLP with LSTM is applied for this project, and external factors affecting NG demand are changed and compared.&lt;/p>
&lt;h2 id="3-datasets">3. Datasets&lt;/h2>
&lt;p>As described, weather datasets like temperature and precipitation, price datasets of other energy resources like crude oil and coal, and economic indicators like exchange rate are used in this project for forecasting NG demand and supply.&lt;/p>
&lt;p>There is an NG supply dataset &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup> from a public data portal in South Korea. It includes four years from 2016 to 2019 of regional monthly NG supply in the nine different cities of South Korea. In addition, climate data such as temperature and precipitation &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup> for the same period can be obtained from the Korea Meteorological Administration. Similarly, data on the price of four types of crude oil &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup> and various types of coal price datasets per month &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup> are also available through corresponding agencies. Finally, the Won-Dollar exchange rate dataset &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup> with the same period is used.&lt;/p>
&lt;p>As mentioned above, each dataset has monthly information and also has average values instead of the NG supply dataset. It is regionally separated or combined according to the test scenario. For example, the NG supply dataset has nine different cities. One column of cities is split from the original dataset and merged with another regional dataset like temperature or precipitation. On the other hand, each regional value&amp;rsquo;s summation is utilized in a scenario where a national dataset is needed.&lt;/p>
&lt;p>The dataset is applied differently for each scenario. In scenario one, all datasets such as crude oil price, coal price, exchange rate, and regional temperature and precipitation are merged with regional dataset, especially Seoul. For scenario two, all climate datasets are used with the regional dataset. Only temperature dataset is utilized with regional dataset in scenario three. In addition, in scenario four, all cases are the same as in scenario one, but the timesteps are changed to two months. Finally, the national dataset is used for scenario five.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/each_factors.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> External factors affecting natural gas&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;h2 id="41-min-max-scaling">4.1. Min-Max scaling&lt;/h2>
&lt;p>In this project, all datasets are rescaled between 0 and 1 by Min-Max scaling, one of the most common normalization methods. If there is a feature with anonymous data, The maximum value(max(x)) of data is converted to 1, and the minimum value(min(x)) of data is converted to 0. The other values between the maximum value and the minimum value get converted to x', between 0 and 1.&lt;/p>
&lt;img src="https://render.githubusercontent.com/render/math?math=%5CLarge%20x'%20%3D%20%5Cfrac%7Bx-min(x)%7D%7Bmax(x)-min(x)%7D">
&lt;h2 id="42-training">4.2. Training&lt;/h2>
&lt;p>For forecasting the NG supply amount from the time series dataset, MLP with LSTM network model is designed by using Tensorflow. The first and second LSTM layers have 100 units, and a total of 3 layers of MLP follow it. Each MLP layer has 100 neurons instead of the final layer, where its neuron is 1. In addition, dropout was designated to prevent overfitting of data, the Adam is used as an optimizer, and the Rectified Linear Unit(ReLU) as an activation function.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/structure_of_network.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Structure of network model&lt;/p>
&lt;h2 id="43-evaluation">4.3. Evaluation&lt;/h2>
&lt;p>Mean Absolute Error(MAE) and Root Mean Squared Error(RMSE) are applied for this time series dataset to evaluate this network model. The MAE measures the average magnitude of the errors and is presented by the formula as following, where n is the number of errors, &lt;img src="https://render.githubusercontent.com/render/math?math=y_i"> is the &lt;img src="https://render.githubusercontent.com/render/math?math=i%5E%7Bth%7D"> true value, and &lt;img src="https://render.githubusercontent.com/render/math?math=%5Chat%7By_i%7D"> is the &lt;img src="https://render.githubusercontent.com/render/math?math=i%5E%7Bth%7D"> predicted value.&lt;/p>
&lt;img src="https://render.githubusercontent.com/render/math?math=%5CLarge%20MAE%20%3D%20%5Cfrac%7B%5CSigma_%7Bi%3D1%7D%5En%7Cy_i-%5Chat%7By_i%7D%7C%7D%7Bn%7D">
&lt;p>Also, The RMSE is used for observing the differences between the actual dataset and prediction values. The following is the formula of RMSE, and each value of this is the same for MAE.&lt;/p>
&lt;img src="https://render.githubusercontent.com/render/math?math=%5CLarge%20RMSE%20%3D%20%5Csqrt%7B%5Cfrac%7B%5CSigma_%7Bi%3D1%7D%5En(y_i-%5Chat%7By_i%7D)%5E2%7D%7Bn%7D%7D">
&lt;h2 id="44-prediction">4.4. Prediction&lt;/h2>
&lt;p>Since the datasets used for the training process are normalized between 0 and 1, they get converted to a range of the ground truth values again. From these rescaled datasets, it is possible to obtain the RMSE and compare the differences between the actual value and the predicted value.&lt;/p>
&lt;h2 id="5-result">5. Result&lt;/h2>
&lt;p>In all scenarios, main variables such as dropout, learning rate, and epochs are fixed under the same conditions and are 0.1, 0.0005, and 100 in order. In scenarios one, two, three, and five, the training set is applied as twelve months, and in scenario four, next month&amp;rsquo;s prediction comes from the previous two months dataset. For comparative analysis, the results are obtained by changing the size of the training set from twelve months to twenty-four months, and the effect is described. Each scenario shows individual results and is comprehensively compared at the end of this part.&lt;/p>
&lt;h2 id="51-scenario-oneregional-dataset">5.1 Scenario one(regional dataset)&lt;/h2>
&lt;p>The final MAE of the train set is around 0.05, and the one of the test set is around 0.19. Also, the RMSE between actual data and predicted data is around 227018. The predictive graph tends to deviate a lot at the beginning of the part, but it shows a relatively similar shape at the end of the graph.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/Error_For_SenarioOne.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Loss for scenario one&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/Prediction_for_SenarioOne.png" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Prediction results for scenario one&lt;/p>
&lt;h2 id="52-scenario-tworegional-climate-dataset">5.2 Scenario two(regional climate dataset)&lt;/h2>
&lt;p>The final MAE of the train set is around 0.10, and the one of the test set is around 0.14. Also, the RMSE is around 185205. Although the predictive graph still differs compared to the actual graph, it shows similar trends in shape.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/Error_For_SenarioTwo.png" alt="Figure 5">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Loss for scenario two&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/Prediction_for_SenarioTwo.png" alt="Figure 6">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Prediction results for scenario two&lt;/p>
&lt;h2 id="53-scenario-threeregional-temperature-dataset">5.3 Scenario three(regional temperature dataset)&lt;/h2>
&lt;p>The final MAE of the train set is around 0.13, and the one of the test set is around 0.14. Also, the RMSE is around 207585. While the tendency to follow high and low seems similar, but changes in the middle seem to be misleading.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/Error_For_SenarioThree.png" alt="Figure 7">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Loss for scenario three&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/Prediction_for_SenarioThree.png" alt="Figure 8">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Prediction results for scenario three&lt;/p>
&lt;h2 id="54-scenario-fourapplying-timesteps">5.4 Scenario four(applying timesteps)&lt;/h2>
&lt;p>The final MAE of the train set is around 0.06, and the one of the test set is around 0.30. Also, the RMSE is around 340843. Out of all scenarios, the predictive graph shows to have the most differences. However, in the last part, there is a somewhat akin tendency.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/Error_For_SenarioFour.png" alt="Figure 9">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> Loss for scenario four&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/Prediction_for_SenarioFour.png" alt="Figure 10">&lt;/p>
&lt;p>&lt;strong>Figure 10:&lt;/strong> Prediction results for scenario four&lt;/p>
&lt;h2 id="55-scenario-fivenational-dataset">5.5 Scenario five(national dataset)&lt;/h2>
&lt;p>The final MAE of the train set is around 0.03 and the one of test set is around 0.14. Also, the RMSE between real data and predicted data is around 587340. Tremendous RMSE value results, but direct comparisons are not possible because the baseline volume is different from other scenarios. Although the predictive graph shows discrepancy, it tends to be similar to the results in scenario two.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/Error_For_SenarioFive.png" alt="Figure 11">&lt;/p>
&lt;p>&lt;strong>Figure 11:&lt;/strong> Loss for scenario five&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/Prediction_for_SenarioFive.png" alt="Figure 12">&lt;/p>
&lt;p>&lt;strong>Figure 12:&lt;/strong> Prediction results for scenario five&lt;/p>
&lt;h2 id="56-overall-results">5.6 Overall results&lt;/h2>
&lt;p>Out of the five scenarios in total, the second and third have smaller RMSE than others, and the graphs also show relatively similar results. The first and fourth show differences in the beginning and similar trends in the last part. However, it is noteworthy that the gap at the beginning of them is very large, but it tends to shrink together at the point of decline and stretch together at the point of increase.&lt;/p>
&lt;p>In the first and fifth scenarios, all data are identical except that they differ in regional scale in temperature and precipitation. It is also the same that twelve months of data are used as the training set. From the subtle differences in the shape of the resulting graph, it can be seen that the national average data cannot represent the situation in a particular region, and the amount of NG supply differs depending on the circumstances in the region.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/compared_prediction.png" alt="Figure 13">&lt;/p>
&lt;p>&lt;strong>Figure 13:&lt;/strong> Total prediction results: 12 months training set&lt;/p>
&lt;p>After changing the training set from twelve months to twenty-four months, the results are more clearly visible. The second and third prediction graphs have a more similar shape and the RMSE value decreases than the previous setting. The results of other scenarios show that the overall shape has improved; contrarily, the shape of the rapidly changing middle part is better in the previous condition.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/compared_prediction_2.png" alt="Figure 14">&lt;/p>
&lt;p>&lt;strong>Figure 14:&lt;/strong> Total prediction results: 24 months training set&lt;/p>
&lt;h2 id="6-benchmarks">6. Benchmarks&lt;/h2>
&lt;p>For a benchmark, the Cloudmesh StopWatch and Benchmark &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup> is used to measure the program&amp;rsquo;s performance. The time spent on data load, data preprocessing, network model compile, training, and the prediction was separately measured, and the overall time for execution of all scenarios is around 77 seconds. It can be seen that The training time for the fourth scenario is the longest, and the one for the fifth scenario is the shortest.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/sp21-599-356/main/project/images/benchmarks.png" alt="Figure 15">&lt;/p>
&lt;p>&lt;strong>Figure 15:&lt;/strong> Benchmarks&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>From the results of this project, it can be seen that simplifying factors that have a significant impact shows better efficiency than combining various factors. For example, NG consumption tends to increase for heating in cold weather. In addition, there is much precipitation in warm or hot weather; on the contrary, there is relatively little precipitation in the cold weather. It can be seen that these seasonal elements show relatively high consistency for affecting prediction when those are used as training datasets. Also, the predictions are derived more effectively when the seasonal datasets are combined.&lt;/p>
&lt;p>However, in training set with a duration of twelve months, the last part of the scenario tends to match the actual data despite using the dataset combined with various factors that appears to be seasonally unrelated. Furthermore, when the training set is doubled on the same dataset, it can be seen that the differences between the actual and prediction graph are decreased than the result of a smaller training set. Based on this, it can be expected that the results could vary if a large amount of dataset with a more extended period is used and the ratio of the training set is appropriately adjusted.&lt;/p>
&lt;p>South Korea imports a large amount of its energy resources. Also, the plan for energy demand and supply is being made and operated through nation-led policies. Ironically, the government&amp;rsquo;s plan also shows a sharp change in direction with recent environmental issues, and the volatility of demand in the energy market is increasing than before. Therefore, methodologies for accurate forecasting of energy demand will need to be complemented and developed constantly to prepare for and overcome this variability.&lt;/p>
&lt;p>In this project, Forecasting NG demand and supply was carried out using various data factors such as weather and price that is relatively easily obtained than the datasets which are complex economic indicators or classified as confidential. Nevertheless, state-of-the-art deep learning methods show that it has the flexibility and potential to forecast NG demand through the tendency of the results that indicate a relatively consistent with the actual data. From this point of view, it is thought that the research on NG in South Korea should be conducted in an advanced form by utilizing various data and more specialized analysis.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor von Laszewski for his invaluable feedback, continued assistance, and suggestions on this paper, and Dr. Geoffrey Fox for sharing his expertise in Deep Learning and Artificial Intelligence applications throughout this Deep Learning Application: AI-First Engineering course offered in the Spring 2021 semester at Indiana University, Bloomington.&lt;/p>
&lt;h2 id="9-source-code">9. Source code&lt;/h2>
&lt;p>The source code for all experiments and results can be found here as &lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/blob/main/project/code/Forecasting_NG_Demand_Supply.ipynb">ipynb link&lt;/a> and as &lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/blob/main/project/code/Forecasting_NG_Demand_Supply.pdf">pdf link&lt;/a>.&lt;/p>
&lt;h2 id="10-references">10. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>2020 Monthly Energy Statistics, [Online resource]
&lt;a href="http://www.keei.re.kr/keei/download/MES2009.pdf">http://www.keei.re.kr/keei/download/MES2009.pdf&lt;/a>, Sep. 2020&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>KOGAS profile, [Online resource]
&lt;a href="https://www.kogas.or.kr:9450/eng/contents.do?key=1498">https://www.kogas.or.kr:9450/eng/contents.do?key=1498&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>LNG production phase, [Online resource]
&lt;a href="https://www.kogas.or.kr:9450/portal/contents.do?key=2014">https://www.kogas.or.kr:9450/portal/contents.do?key=2014&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>NG wholesale charges, [Online resource]
&lt;a href="https://www.kogas.or.kr:9450/portal/contents.do?key=2026">https://www.kogas.or.kr:9450/portal/contents.do?key=2026&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Natural gas explained, [Online resource],
&lt;a href="https://www.eia.gov/energyexplained/natural-gas/factors-affecting-natural-gas-prices.php">https://www.eia.gov/energyexplained/natural-gas/factors-affecting-natural-gas-prices.php&lt;/a>, Aug, 2020&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>A. Khotanzad and H. Elragal, &amp;ldquo;Natural gas load forecasting with combination of adaptive neural networks,&amp;rdquo; IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339), 1999, pp. 4069-4072 vol.6, doi: 10.1109/IJCNN.1999.830812.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>A. Khotanzad, H. Elragal and T. . -L. Lu, &amp;ldquo;Combination of artificial neural-network forecasters for prediction of natural gas consumption,&amp;rdquo; in IEEE Transactions on Neural Networks, vol. 11, no. 2, pp. 464-473, March 2000, doi: 10.1109/72.839015.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>M. Akpinar, M. F. Adak and N. Yumusak, &amp;ldquo;Forecasting natural gas consumption with hybrid neural networks — Artificial bee colony,&amp;rdquo; 2016 2nd International Conference on Intelligent Energy and Power Systems (IEPS), 2016, pp. 1-6, doi: 10.1109/IEPS.2016.7521852.&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>A. Anagnostis, E. Papageorgiou, V. Dafopoulos and D. Bochtis, &amp;ldquo;Applying Long Short-Term Memory Networks for natural gas demand prediction,&amp;rdquo; 2019 10th International Conference on Information, Intelligence, Systems and Applications (IISA), 2019, pp. 1-7, doi: 10.1109/IISA.2019.8900746.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>NG supply dataset, [Online resource],
&lt;a href="https://www.data.go.kr/data/15049904/fileData.do">https://www.data.go.kr/data/15049904/fileData.do&lt;/a>, Apr, 2020&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Regional climate dataset, [Online resource]
&lt;a href="https://data.kma.go.kr/climate/RankState/selectRankStatisticsDivisionList.do?pgmNo=179">https://data.kma.go.kr/climate/RankState/selectRankStatisticsDivisionList.do?pgmNo=179&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Crude oil orice dataset, [Online resource]
&lt;a href="https://www.petronet.co.kr/main2.jsp">https://www.petronet.co.kr/main2.jsp&lt;/a>&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Bituminous coal price dataset, [Online resource]
&lt;a href="https://www.kores.net/komis/price/mineralprice/ironoreenergy/pricetrend/baseMetals.do?mc_seq=3030003&amp;amp;mnrl_pc_mc_seq=506">https://www.kores.net/komis/price/mineralprice/ironoreenergy/pricetrend/baseMetals.do?mc_seq=3030003&amp;amp;mnrl_pc_mc_seq=506&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Won-Dollar exchange rate dateset, [Online resource]
&lt;a href="http://ecos.bok.or.kr/flex/EasySearch.jsp?langGubun=K&amp;amp;topCode=022Y013">http://ecos.bok.or.kr/flex/EasySearch.jsp?langGubun=K&amp;amp;topCode=022Y013&lt;/a>&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data on Gesture Recognition and Machine Learning</title><link>/report/fa20-523-315/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-315/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-315/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-315/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-315/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-315/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Sunny Xu, Peiran Zhao, Kris Zhang, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-315/">fa20-523-315&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-315/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Since our technology is more and more advanced as time goes by, traditional human-computer interaction has become increasingly difficult to meet people&amp;rsquo;s demands. In this digital era, people need faster and more efficient methods to obtain information and data. Traditional and single input and output devices are not fast and convenient enough, it also requires users to learn their own methods of use, which is extremely inefficient and completely a waste of time. Therefore, artificial intelligence comes out, and its rise has followed the changeover times, and it satisfied people&amp;rsquo;s needs. At the same time, gesture is one of the most important way for human to deliver information. It is simple, efficient, convenient, and universally acceptable. Therefore, gesture recognition has become an emerging field in intelligent human-computer interaction field, with great potential and future.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background">2. Background&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-gesture-recognition">3. Gesture Recognition&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-hand-gesture">3.1 Hand Gesture&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#311-hand-gesture-recognition-and-big-data">3.1.1 Hand Gesture Recognition and Big Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#312-principles-of-hand-gesture-recognition">3.1.2 Principles of Hand Gesture Recognition&lt;/a>&lt;/li>
&lt;li>&lt;a href="#313-gesture-segmentation-and-algorithm-the-biggest-difficulty-of-gesture-recognition">3.1.3 Gesture Segmentation and Algorithm, The Biggest Difficulty of Gesture Recognition.&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#32-body-gesture">3.2 Body Gesture&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#321-introduction-to-body-gesture">3.2.1 Introduction to Body Gesture&lt;/a>&lt;/li>
&lt;li>&lt;a href="#322-body-gesture-and-big-data">3.2.2 Body Gesture and Big Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#323-random-forest-algorithm-in-body-gesture-recognition">3.2.3 Random Forest Algorithm in Body Gesture Recognition&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#33-face-gesture">3.3 Face Gesture&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#331-introduction-to-face-gesture-facial-expression">3.3.1 Introduction to Face Gesture (Facial Expression)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#332-sense-organs-on-the-face">3.3.2 Sense Organs on The Face&lt;/a>&lt;/li>
&lt;li>&lt;a href="#333-facial-expression-and-big-data">3.3.3 Facial Expression and Big Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#334-the-problem-with-detecting-emotion-for-technology-nowadays">3.3.4 The Problem with Detecting Emotion for Technology Nowadays&lt;/a>&lt;/li>
&lt;li>&lt;a href="#335-classification-algorithms">3.3.5 Classification Algorithms&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-conclusion">4. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-references">5. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> gesture recognition, human, technology, big data, artificial intelligence, body language, facial expression&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Technology is probably one of the most attracting things for people nowadays. Whether it is the new iPhone coming out or some random new technology that is bring into our life. It is a matter of fact that technology has become one of the essential parts of our life and our society. Simply, our life will change a lot without technology. As of today, since technology is improving so fast, there are many things that can be related to AI and machine learning. A lot of the ordinary things around our life becomes data. And the reason why they become data is because there is a need for them in having better technology to improve our life. For example, language was stored into data to produce technology like translator to provide convenience for people that does not speak the language. Another example is that roads were stored into data to produce GPS to guide direction for people. Nowadays, people values communication and interaction between others. Since gesture recognition is one of the most important ways to understand people and know their emotion, it becomes a popular field of study for many scientists. There are multiply field of study in gesture recognition and each require a lot of amount of time to know them well. For the report, we do research about hand gesture, body gesture and facial expression. Of course, there will be a lot of other fields related to gesture recognition, for example, like animal gestures. They all can be stored into data and get study in the research by scientists. Many people might have question about how gesture recognition are has anything to do with technology. They simply do not think that they can be related, but in fact, they are related. Companies like Intel and Microsoft have already created so many studies for new technology in that field. For example, Intel proposed combining facial recognition with device recognition to authenticate users. Studying gestures recognition will often reveal what the think. For example, when someone is lying, their eye will tend to look around and they tend to touch their nose with their hand, etc. So, studying gesture recognition will not only help people understand much more about human beings and it can also help our technology grow. For example, in AI and machine learning, studying gestures recognition will make or improve AI and machine learning to better understand humans and be more human-like.&lt;/p>
&lt;h2 id="2-background">2. Background&lt;/h2>
&lt;p>Nowadays, people are doing more and more high-tech research, which also makes various high-tech products appear in society. For people, electricity is as important as water and air. Can&amp;rsquo;t imagine life without electricity. We can realize that technology is changing everything about people from all aspects. People living in the high-tech era are also forced to learn and understand the usage of various high-tech products. As a representative of high technology, artificial intelligence has also attracted widespread attention from society. Due to the emergence of artificial intelligence, people have also begun to realize that maintaining human characteristics is also an important aspect of high technology.&lt;/p>
&lt;p>People&amp;rsquo;s living environment is inseparable from high technology. As for the use of human body information, almost every high-tech has different usage &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. For example, face recognition is used in many places to check-in. This kind of technology enables the machine to store the information of the human face and determine whether it is indeed the right person by judging the five senses. We are most familiar with using this technology in airports, customs, and companies to check in at work. Not only that, but the smartphones we use every day are also unlocked through this face recognition technology. Another example is the game console that we are very familiar with. Game consoles such as Xbox and PS already have methods for identifying people&amp;rsquo;s bodies. They can identify several key points of people through the images received by their own cameras, thus inputting this line of action into the world of the game.&lt;/p>
&lt;p>Many researchers are now studying other applications of human movements, gestures, and facial expressions. One of the most influential ones is that Google’s scientists have developed a new computer vision method for hand perception. Google researchers identified the movement of a hand through twenty-one 3D points on the hand. Research Engineers Valentin Bazarevsky and Fan Zhang stated that &amp;ldquo;The ability to perceive the shape and motion of hands can be a vital component in improving the user experience across a variety of technological domains and platforms &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&amp;rdquo; This model can currently identify many common cultural features. gesture. They have done experiments. When people play a game of &amp;ldquo;rock, paper, scissors&amp;rdquo; in front of the camera, this model can also judge everyone&amp;rsquo;s win or loss by recognizing gestures.&lt;/p>
&lt;p>More than that, many artificial intelligences can now understand people&amp;rsquo;s feelings and intentions by identifying people&amp;rsquo;s facial expressions. This also allows us to know how big a database is behind this to support the operation of these studies. But collecting these data about gesture recognition is not easy. Many times we need to worry about not only whether the data we input is correct, but also whether the target identified by artificial intelligence is clear and the output information is accurate.&lt;/p>
&lt;h2 id="3-gesture-recognition">3. Gesture Recognition&lt;/h2>
&lt;p>Gesture recognition is mainly divided into two categories, one is based on external device recognition, the specific application is data gloves, wearing it on user&amp;rsquo;s hand, to obtain and analysis information through sensors. This method has obvious shortcomings, though it is accurate and has excellent response speed, but it is costly and is not good for large-scale promotion. The other one is the use of computer vision. People do not need to wear gloves. As its name implies, this method collects and analyzes information through a computer. It is convenient, comfortable, and not so limited based on external device identification. In contrast, it has greater potential and is more in line with the trend of the times. Of course, this method needs more effective and accurate algorithms to support, because the gestures made by different people at different times, in different environments and at different angles also represent different meanings. So, if we want more accurate information feedback. Then the advancement of algorithms and technology is inevitable. The development of gesture recognition is also the development of artificial intelligence, a process of the development of various algorithms from data gloves to the development of computer vision-based optical technology plays a role in promoting it.&lt;/p>
&lt;h3 id="31-hand-gesture">3.1 Hand Gesture&lt;/h3>
&lt;h4 id="311-hand-gesture-recognition-and-big-data">3.1.1 Hand Gesture Recognition and Big Data&lt;/h4>
&lt;p>Hand gesture recognition is commonly used in gesture recognition because fingers are the most flexible and it is able to create different angles that will represent different meanings. The hand gesture itself is also an easy but efficient way for us human beings to communicate and send messages to each other. The existence of hand gestures can be considered easy but powerful. However, if we are using the application of hand gesture recognition, it is a much more complicated process. In real life, we can just ben our finger or simply make a fist so that other people will understand our message. But when using hand gesture recognition there are many processes that are being involved. Hand gesture is commonly used in geesture recoginitaion As we did our research and based on our life experiences, hand gesture recognition is a very hot topic and has all the potential to be the next wave. Hand gesture recognition has recently achieved big success in many fields. The advancement and development of hand gesture recognition is also the development of other technology such as the advancement of computer chips, the advancement of algorithms, the advancement of machine learning even advancement of deep learning, and the advancement of cameras from 2D to 3D. The most important part of hand gesture recognition is big data and machine learning. Because of the development of big data and machine learning, data scientists are able to have better datasets, build a more accurate and successful model and be able to process the information and predict the most accurate results. Hand gesture recognition is a significant link in Gesture recognition.However gesture recognition is also not only about hand gesture recognition, it also includes other body parts such as facial expression recognition and body gesture recognition. With the help of the whole system of different gesture recognitions, the data can be recorded and processed by AIs. The results or predictions can be used currently or later on for different purposes in different areas &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="312-principles-of-hand-gesture-recognition">3.1.2 Principles of Hand Gesture Recognition&lt;/h4>
&lt;p>Hand gesture recognition is a complicated process involving many steps. And in order to get the most accurate result, it will need a large amount of quality data and a scientific model with high performance. Hand gesture recognition is also at a developing stage simply because there are so many possible factors that can influence the result. Possible factors include skin color, background color, hand gesture angle, and Bending angle, etc. To simplify the process of gesture recognition, AIs will use 3-D cameras to capture images. After that, the data of the image will be collected and processed by programs and built models. And lastly, AIs will be able to use that model to get an accurate result in order to have a corresponding response or predict future actions. To explain all processes of hand gesture recognition in detail, it includes graphic gathering, retreatment, skin color segmentation, hand gesture segmentation, and finally hand gesture recognition. Hand Gesture Recognition can not achieve the best accuracy without all any of these steps. Within all these steps, skin color segmentation is the most crucial step in order to increase accuracy and this process will be explained in the next session &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="313-gesture-segmentation-and-algorithm-the-biggest-difficulty-of-gesture-recognition">3.1.3 Gesture Segmentation and Algorithm, The Biggest Difficulty of Gesture Recognition.&lt;/h4>
&lt;p>If someone actually asks us a question which is what kind of recognition is going to have the maximum potential in the future? We will have Hand Gesture recognition as my answer without a doubt. Because in my opinion, Hand Gesture Recognition is really the next wave, as our technology is getting better and better, it will be a much easier and more efficient type of recognition that could possibly change our lives. If you compare Hand Gesture Recognition with Voice Recognition, you will see the biggest difference because everyone is using Hand Gesture all over the world in different ways while Voice is limited to people that are unable to make a sound and sound is a much more complicated type of data that in my opinion is not efficient enough to deliver a message, at least with lots of evidence indicating it is not easier than Hand Gesture Recognition. However, it doesn’t mean hand gesture doesn’t have any limit. Instead, Hand Gesture Recognition is influenced by the color in many ways including skin colors and the colors of the background. But skin color is also a great characteristic of recognition at the same time. So if we could overcome this shortcoming or obstacle, the biggest disadvantage of Hand Gesture Recognition could also become its biggest advantage since skin color has so many amazing characteristics that could be used as a huge benefit for Hand Gesture Recognition. Firstly, skin color is a unique attribute which means it has a similar meaning all over the world. For example, Asian people mostly have yellow skins, Western people mostly have white skins while African American people mostly have black skins. People might form different regions from all over the world but since their skins are similar in many ways, they are most likely to have at least similar hand gesture meanings according to different scientific studies. However, you might ask another realistic question which is what about many people who have similar skin colors but are coming from different groups of people who have a completely different cultural background which results in different Hand Gestures and people who have similar Hands Gesture but have much different skin colors. These are all realistic Hand Gesture Recognition problems and these are the problems that Hand Gesture Recognition already solved or is going to solve in the future. Firstly, for people who have similar skin colors but are coming from different groups of people who have a completely different cultural background, this is when skin color comes to play its role. Even though those people have similar skin color, their skin color can’t be exactly the same. Most of the time, it will be either darker or lighter and we might say it’s all yellow or white, but the machine will see it as its data format so even if it is all white, the type of white is still completely different. And this is when gesture segmentation or more accurately skin color segmentation makes a difference. Overall, us human read skin colors as the simple color we have learned from different textbooks but the computer or machine see the different color in the different color spaces and the data they receive and going to process will be much more accurate. In addition to that, scientists will need to do more in-depth research and studies in order to get the most accurate result. And for people who have similar Hands Gesture but have many different skin colors, scientists will need to collect more accurate data not only about the color and about the size, angles, etc. This more detailed information will help the machine read Hand Gesture more accurately in order to get the most beneficial feedback. The background color will undoubtedly provide lots of useless information and potentially negatively influence the result. In this way, Hand Gesture Recognition has developed its own color and gesture recognition algorithm and method to remove the most useless data or color and leave the valid ones. Lighting in different background settings will have a huge influence to and in most ways, it will negatively influence the result too. There are five most important steps in Hand Gesture Recognition which are Graphic Gathering, Pretreatment, Skin Color Segmentation, Gesture Segmentation, and lastly Gesture Recognition. All these different steps are all very crucial in order to get the most accurate feedback or result. It is pretty similar to the most data treatment process especially the first two steps where you first build a model, gather different types of data, clean the data after that, and use skin color segmentation and gesture segmentation before the last Gesture Recognition process &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="32-body-gesture">3.2 Body Gesture&lt;/h3>
&lt;h4 id="321-introduction-to-body-gesture">3.2.1 Introduction to Body Gesture&lt;/h4>
&lt;p>Body gestures, which can also be called body language, refer to humans expressing their ideas through the coordinated activities of various body parts. In our lives, body language is ubiquitous. It is like a bridge for our human communication. Through body language expression, it is often easier for us to understand what the other person wants to express. At the same time, we can express ourselves better. The profession of an actor is a good example of body language. This is a compulsory course for every actor because actors can only use their performances to let us know what they are expressing &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. At this time, body language becomes extremely important. Different characters have different body movements in different situations, and actors need to make the right body language at a specific time to let the audience know their inner feelings. Yes, the most important point of body language is to convey mood through movement.&lt;/p>
&lt;p>In many cases, certain actions will make people feel emotions. For us who communicate with all kinds of people every day, there are also many body languages that we are more familiar with. For example, when a person hangs his head, it means that he is unhappy, walking back and forth is a sign of a person&amp;rsquo;s anxiety, and body shaking is caused by nervousness, etc.&lt;/p>
&lt;h4 id="322-body-gesture-and-big-data">3.2.2 Body Gesture and Big Data&lt;/h4>
&lt;p>As a piece of big data, body language requires data collected by studying human movements. Scientists found that when a person wants to convey a complete message, body language accounts for half. And because body language belongs to a person&amp;rsquo;s actions subconsciously, it is rarely deceptive. All of your nonverbal behaviors—the gestures you make, your posture, your tone of voice, how many eyes contact you make—send strong messages &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. In many cases, these unconscious messages from our bodies allow the people who communicate with us to feel our intentions. Even when we stop talking, these messages will not stop. This also explains why scientists want to collect data to let artificial intelligence understand human behavior. In order for artificial intelligence to understand human mood or intention from people&amp;rsquo;s body postures and actions, scientists have collected a lot of human body actions that show intentions in different situations through research. The music gesture artificial intelligence developed by MIT-IBM Watson AI Lab is a good example &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. The music gesture artificial intelligence developed by MIT-IBM Watson AI Lab can enable artificial intelligence to judge and isolate the sounds of individual instruments through body and gesture movements. This success is undoubtedly created by the big data of the entire body and gestures. The research room collects a large number of human structure actions to provide artificial intelligence with a large amount of information so that the artificial intelligence can judge what melody the musician is playing through body gestures and key points of the face. This can improve its ability to distinguish and separate sounds when artificial intelligence listens to the entire piece of music.&lt;/p>
&lt;p>Most of the artificial intelligence&amp;rsquo;s analysis of the human body requires facial expressions and body movements. This recognition cannot be achieved only by calculation. What is needed is the collection of the meaning of different body movements of the human body by a large database. The more situations are collected, the more accurate the analysis of human emotions and intentions by artificial intelligence will be. The easiest way is to include more. Just like humans, broadening your horizons is a way to better understand the world. The way of recording actions is not complicated. Just set several key movable joints of the human body to several points, and then connect the red dots with lines to get the approximate shape of the human body. At this time, the actions made by the human body will be included in the artificial intelligence. In the recording process, the upper body and lower body can be recorded separately. In order to avoid in some cases, the existence of obstructions will cause artificial intelligence to fail to recognize correctly.&lt;/p>
&lt;h4 id="323-random-forest-algorithm-in-body-gesture-recognition">3.2.3 Random Forest Algorithm in Body Gesture Recognition&lt;/h4>
&lt;p>Body gesture recognition is pretty useful but pretty hard to achieve because of its limitations and harsh requirements. Without the development of all kinds of 3D cameras, body gesture recognition is just an unrealistic dream. In order to get important and precise data for the body gesture recognition to process, different angles, light, background all needs to be captured &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. For body gestures, the biggest difficulty is that if you only capture data in the front, it will not give you the correct information and result in most of the time. In this way, you will need precise data from different angles. A Korean team has done an experiment using three 3D cameras and three stereo cameras to capture images and record data from different angles. The data were recorded in a large database that includes captured data both from outside and inside. One of the most popular algorithms used in body gesture recognition is the random forest algorithm. It is very famous and useful in all types of machine learning projects. It is a type of supervised learning algorithm. Because there are all types of data are needed to be a record and process. The random forest algorithm is perfect for that, the biggest advantage of this algorithm is that it can let each individual tree mainly focus on one part or one characteristic of body gesture data because of this algorithm’s ability to combine all weak classifiers into a strong one &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. It is simple but so powerful and efficient. In addition to that, it works really well with body gesture recognition. With the algorithm and advanced cameras, precise data could be collected and AIs will be able to get useful information at different levels.&lt;/p>
&lt;h3 id="33-face-gesture">3.3 Face Gesture&lt;/h3>
&lt;h4 id="331-introduction-to-face-gesture-facial-expression">3.3.1 Introduction to Face Gesture (Facial Expression)&lt;/h4>
&lt;p>Body language is one of the ways that we can express ourselves without saying any words. It has been suggested that body language may account for between 60 to 65% of all communication &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. According to expert, body language is used every day for us to communicate with each other. During our communication, we not only use words but also use body gestures, hand gestures and most importantly, we use facial expression most. During communication with different people, our face communicate different thoughts, idea, and emotion and the reason why we use facial expression more than any other body gestures is that when we have certain emotion, it is express in our face automatically. Facial expression is often not under our control. That is why people often say that the word that come out of mouth cannot always be true, but their facial expression will reveal what those people are thinking about. So, what is facial expression exactly? According to Jason Matthew Harley, Facial expressions are configurations of different micromotor movements in the face that are used to infer a person’s discrete emotional state &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Some example of common facial expression will be: Happiness, Sadness, Anger, Surprise, Disgust, Fear, etc &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Each facial expression will have some deep meaning behind it. For example, A simple facial expression like smiling can be translated into a sign of approval, or it can be translated into a sign of friendly. If we put all those emotion into big data, it will help us to understand ourselves much better.&lt;/p>
&lt;h4 id="332-sense-organs-on-the-face">3.3.2 Sense Organs on The Face&lt;/h4>
&lt;p>The facial expression expresses our emotion during the communication by micro movement of our sense organs. The most used organs are the eyes and mouth and sometimes, the eyebrows.&lt;/p>
&lt;h5 id="3321-eye">3.3.2.1 Eye&lt;/h5>
&lt;p>The eyes are one of the most important communication tools in our ways of communication with each other. When we communicate with each other, the eye contact will be inevitable. The signal in your eye will tell people what you are think.
Eye gaze is a sample of paying attention when communicating with others. When you are talking to a person and if his eye is directly on you and both of you keep having eye contact. In this situation, this mean that he is interested in what you say and is paying attention to what you say. On the other hand, if the action of breaking eye contact happens very frequently, it means that he is not interested, distracted, or not paying attention to you and what you are saying.&lt;/p>
&lt;p>Blinking is another eye signal that is very often and will happen in communicating with other people. When talking to other people, blinking is very usual and will happen every time when you are going to communicate with different people. But the frequency of blanking can give away what are you feeling right now. People often blink more rapidly when they are feeling distressed or uncomfortable. Infrequent blinking may indicate that a person is intentionally trying to control his or her eye movements &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. For example, when A person is lying, he might try to control his blinking frequency to make other people feel like he is calm and saying the truth. In order to persuade other people that he is calm and telling the truth, he will need to blink less frequently.&lt;/p>
&lt;p>Pupil size is a very important facial expression. Pupil size can be a very subtle nonverbal communication signal. While light levels in the environment control pupil dilation, sometimes emotions can also cause small changes in pupil size &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. For example, when you are surprised by something, your pupil size will become noticeably larger than before. When having a communication, dilated eyes can also mean that the person is interesting in the communication.&lt;/p>
&lt;h5 id="3322-mouth">3.3.2.2 Mouth&lt;/h5>
&lt;p>Mouth expression and movement will also be a huge part in communicating with other and reading body language. The easiest example will be smiling. A micro movement of your mouth and lip will give signal to others about what do you think or how are you feeling.
When you tighten your lips, it means that you either distaste, disapprove or distrust other people when having a conversation.
When you bite your lips, it means that you are worried, anxious, or stressed.
When someone tries to hide certain emotional reaction, they tend to cover their mouth in order not to display any facial expression through lip movement. For example, when you are laughing.
The simple movement of turning up or down of the lip will also indicate what a person is feeling. When the mouth is slightly turn up, it might mean that the person is either feeling happy or optimistic. On the other hand, a slightly down-turned mouth can be an indicator of sadness, disapproval, or even an outright grimace &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="333-facial-expression-and-big-data">3.3.3 Facial Expression and Big Data&lt;/h4>
&lt;p>Nowadays, since technology is so advance, everything around us can be turn into data and everything can be related to data. Facial expression is often study by different scientist in research because it allows us to understand more about human and communication between different people. One of the relatively new and promising trends in using facial expressions to classify learners' emotions is the development and use of software programs the automate the process of coding using advanced machine learning technologies. For example, FaceReader is a commercially available facial recognition program that uses an active appearance model to model participant faces and identifies their facial expression. The program further utilizes an artificial neural network, with seven outputs to classify learner’s emotions &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. Also, facial expression can be analyzed in other software programs like the Computer Expression Recognition Toolbox. Emotion is a huge study field in the technology field, and facial expression is one of the best ways to study and analyze people&amp;rsquo;s emotion. Emotion technology is becoming huge right now and will be even more popular in the future according to MIT Technology Review, Emotion recognition – or using technology to analyze facial expressions and infer feelings-is, by one estimate, set to be a $25 billion business by 2023 &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. So back to the topic about big data and facial expression. Why are those things related? It is because, first everything is data around us. Your facial expression can be stored into data for other to learn and detect too. One of the examples is that, in 2003, The US Transportation Security Administration started training humans to spot potential terrorists by reading their facial expression. And by that, scientist believe that if human can do that, with data and AI technology, robot can detect facial expression more accurate than human.&lt;/p>
&lt;h4 id="334-the-problem-with-detecting-emotion-for-technology-nowadays">3.3.4 The Problem with Detecting Emotion for Technology Nowadays&lt;/h4>
&lt;p>Even though facial expression can reveal people&amp;rsquo;s emotion and what they think, but there has been &amp;ldquo;growing pushback&amp;rdquo; against the statement. A group of scientists brought together a research after reviewing more than 1,000 paper on emotion detection. After the research, the conclusion of it is hard to use facial expressions alone to accurately tell how someone is feeling is made. Human&amp;rsquo;s mind is very hard to predict. People do not always cry when they feel down and smile when they feel happy. The facial expression can not always reveal the true feeling the person is feeling. Not only that, because there is not enough data for facial expression, people will often mistakenly categorize other&amp;rsquo;s facial expression. For example, Kairos, which is a facial biometrics company, promise retailers that it can use a emotion recognition technology to figure out how their customers are feeling. But when they are labeling the data to feed the algorithm, one big problem reveals. An observer might read a facial expression as &amp;ldquo;surprised,&amp;rdquo; but without asking the original person, it is hard to know what the real emotion was &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. So the problems with technology that involves around facial expression are first, there is not enough data. Second is that facial expression sometimes can not be always true.&lt;/p>
&lt;h4 id="335-classification-algorithms">3.3.5 Classification Algorithms&lt;/h4>
&lt;p>Nowadays, since technology is growing so fast, there are a lot of interaction between humans and computer. Facial expression plays an essential role in social interaction with other people. It is not arguably one of the best ways to understand human. &amp;ldquo;It is reported that facial expression constitutes 55% of the effect of a communicated message while language and voice constitute 7% and 38% respectively. With the rapid development of computer vision and artificial intelligence, facial expression recognition becomes the key technology of advanced human computer interaction &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>.&amp;rdquo; This quote from the research shows that facial expression is one of the main tools that we are using to communicate with other people and interact with computer. So being able to recognize and identify the facial expression becomes relatively important. The main objective for facial expression recognitions is to use its conveying information automatically and correctly. As a result, feature extraction is very important to the facial expression recognition process. The process needs to be smooth and without any mistakes. So, algorithms are needed in the processes. Classification analysis is an important component of facial recognition, it is mainly used to find data distribution that is valuable and at the same time, find data models in the potential data. At present it has further study of the database, data mining, statistics, and other fields &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. In addition to that, one of the major obstacles and limitation of facial expression recognition is face detection. To detect the face, you will need to locate the faces in an image or a photograph. This is where scientists applicate classification algorithm, machine learning and deep learning. Recently, convolutional neural network model has become so successful that facial recognition is the next top wave &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-conclusion">4. Conclusion&lt;/h2>
&lt;p>With the development of artificial intelligence, human-computer interaction, big data, and machine learning even deep learning is getting more mature. Gesture Recognition including Hand Gesture Recognition, Body Gesture Recognition, and Face Gesture Recognition has finally come true into a real-life application and already achieved huge success in many areas. But it still has much more potential in all possible areas that could change people&amp;rsquo;s lives drastically in a good way. Gestures are the simplest and the most natural language of all human beings. It sends the clearest message for communicating between people, and even human and computers. Because of the more powerful cameras, better big data technology, and more efficient and effective algorithms from deep learning, Scientists are able to use color and the Gesture Segmentation method to remove useless color data in order to maximize the accuracy of the result. As we are doing our research, we also find out Hand Gesture Recognition is not the only Recognition in this area, Body Gesture Recognition and Face Gesture Recognition or facial expression are also very important, they can also deliver messages in the simplest way. They are also very effective when building relationships between humans and machines. Face Gesture or facial expression could not only deliver messages but even deliver emotions. Micromovements of facial expressions studied by different scientists could be very useful in predicting the emotions of humans. Body Gesture Recognition is also helpful as we did our research with the body gesture data scientists collected from different musicians with different instruments. They are able to predict the melodies or even the songs played by that musician. This is mind-blowing because with this type of technology and applications we are able to achieve more and use it in many possible fields. With all these combined, scientists could build a very successful and mature Gesture Recognition model to get the most accurate result or prediction. According to the research and our own analysis, we come up with a conclusion that Gesture Recognition will be the next hot trendy topic and are applicable in many possible areas including Security, AI, economics, manufacture, the game industry, and even medical services. With Gesture Recognition being applied, scientists are able to develop much smarter AIs and machines that can interact with humans more efficiently and more fluently. AIs will be able to receive and understand messages from humans more easily and will able to function better. This is also a great message for many handicapped people. With Hand Gesture Recognition being used, their life will also be easier and happier and that’s definitely something we are want to see because the overall goal of all the technologies is to make people&amp;rsquo;s life easier and bring the greatest amount of happiness to the greatest amount of people. However, the technology we have right now is not advanced enough yet, in order to get a more accurate result, we still need to develop better cameras, better algorithms, and better models. But we all believe that this era is the big data era, and everything could happen as big data and deep learning technology get more and more advanced and mature. We believe in and look forward to the beautiful future of Gesture Recognition. And we also think people should really pay more and closer attention to this field since Gesture Recognition is the next wave.&lt;/p>
&lt;h2 id="5-references">5. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Srilatha, Poluka, and Tiruveedhula Saranya. &amp;ldquo;Advancements in Gesture Recognition Technology.&amp;rdquo; IOSR Journal of VLSI and Signal Processing, vol. 4, no. 4, 2014, pp. 01–07, iosrjournals.org/iosr-jvlsi/papers/vol4-issue4/Version-1/A04410107.pdf, 10.9790/4200-04410107. Accessed 25 Oct. 2020.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Bazarevdsky, V., &amp;amp; Zhang, F. (2019, August 19). On-device, real-time hand tracking with MediaPipe. Google AI Blog. &lt;a href="https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html">https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>F. Zhan, &amp;ldquo;Hand Gesture Recognition with Convolution Neural Networks,&amp;rdquo; 2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI), Los Angeles, CA, USA, 2019, pp. 295-298, doi: 10.1109/IRI.2019.00054.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Di Zhang, DZ.(2019) Research on Hand Gesture Recognition Technology Based on Machine Learning, Nanjing University of Posts and Telecommunications.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>A. Choudhury, A. K. Talukdar and K. K. Sarma, &amp;ldquo;A novel hand segmentation method for multiple-hand gesture recognition system under complex background,&amp;rdquo; 2014 International Conference on Signal Processing and Integrated Networks (SPIN), Noida, 2014, pp. 136-140, doi: 10.1109/SPIN.2014.6776936.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Cherry, K. (2019, September 28). How to Read Body Language and Facial Expressions. Verywell Mind. Retrieved November 8, 2020, from &lt;a href="https://www.verywellmind.com/understand-body-language-and-facial-expressions-4147228">https://www.verywellmind.com/understand-body-language-and-facial-expressions-4147228&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Segal, J., Smith, M., Robinson, L., &amp;amp; Boose, G. (2020, October). Nonverbal Communication and Body Language. HelpGuide.org. &lt;a href="https://www.helpguide.org/articles/relationships-communication/nonverbal-communication.htm">https://www.helpguide.org/articles/relationships-communication/nonverbal-communication.htm&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Martineau, K. (2020, June 25). Identifying a melody by studying a musician’s body language. MIT News | Massachusetts Institute of Technology. &lt;a href="https://news.mit.edu/2020/music-gesture-artificial-intelligence-identifies-melody-by-musician-body-language-0625">https://news.mit.edu/2020/music-gesture-artificial-intelligence-identifies-melody-by-musician-body-language-0625&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>N. Normani et al., &amp;ldquo;A machine learning approach for gesture recognition with a lensless smart sensor system,&amp;rdquo; 2018 IEEE 15th International Conference on Wearable and Implantable Body Sensor Networks (BSN), Las Vegas, NV, 2018, pp. 136-139, doi: 10.1109/BSN.2018.8329677.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Bon-Woo Hwang, Sungmin Kim and Seong-Whan Lee, &amp;ldquo;A full-body gesture database for automatic gesture recognition,&amp;rdquo; 7th International Conference on Automatic Face and Gesture Recognition (FGR06), Southampton, 2006, pp. 243-248, doi: 10.1109/FGR.2006.8.&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Harley, J. M. (2016). Facial Expression. ScienceDirect. &lt;a href="https://www.sciencedirect.com/topics/computer-science/facial-expression">https://www.sciencedirect.com/topics/computer-science/facial-expression&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Chen, A. (2019, July 26). Computers can’t tell if you’re happy when you smile. MIT Technology Review. &lt;a href="https://www.technologyreview.com/2019/07/26/238782/emotion-recognition-technology-artifical-intelligence-inaccurate-psychology/">https://www.technologyreview.com/2019/07/26/238782/emotion-recognition-technology-artifical-intelligence-inaccurate-psychology/&lt;/a>&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Ou, J. (2012). Classification algorithms research on facial expression recognition. Retrieved from &lt;a href="https://www.sciencedirect.com/science/article/pii/S1875389212006438">https://www.sciencedirect.com/science/article/pii/S1875389212006438&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Brownlee, J. (2020, August 24). How to perform face detection with deep learning. Retrieved from &lt;a href="https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/">https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/&lt;/a>&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data in the Healthcare Industry</title><link>/report/fa20-523-352/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-352/report/report/</guid><description>
&lt;p>Cristian Villanueva, Christina Colon&lt;/p>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-352/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-352/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-352/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-352/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-352/">fa20-523-352&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-352/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Healthcare is an organized provision of medical practices provided to individuals or a community. Over centuries the application of innovative healthcare has been needed increasingly as humans expand their life span and become more aware of better preventative care practices. The application of Big Data within the industry of Healthcare is of the utmost importance in order to quantify the effects of wide scale efficient and safe solutions. Pharmaceutical and Bio Data Research companies can use big data to intake large facets of patient record data and use this collected data to iterate how preventative care can be implemented before diseases actually present themselves in stages that are beyond the point of potential recovery. Data collected in laboratory settings and statistics collected from medical and state institutions of healthcare facilitate time, money, and life saving initiatives as deep learning can in certain instances perform better than the average doctor at detecting malignant cells. Big data within healthcare has proven great results for the advancement and diverse application of informed reasoning towards medical solutions.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1--introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-patient-records">2. Patient Records&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-ehr-application-detecting-error-and-reducing-costs">2.1 EHR Application: Detecting Error and Reducing Costs&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-ai-models-in-cancer-detection">3. AI Models in Cancer Detection&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-early-detection-big-data-applications">3.1 Early Detection Big Data Applications&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-detecting-cervical-cancer">3.2 Detecting Cervical Cancer&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-artificial-intelligence-in-cardiovascular-disease">4. Artificial intelligence in Cardiovascular Disease&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-deep-learning-techniques-for-genomics">5. Deep Learning Techniques for Genomics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-discussion">6. Discussion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgements">8. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> EHR, Healthcare, diagnosis, application, treatment, AI, network, records&lt;/p>
&lt;h2 id="1--introduction">1. Introduction&lt;/h2>
&lt;p>Healthcare is a multi-dimensional system established with the aim of the prevention, diagnosis, and treatment of health-related issues or impairments in human beings&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The many dimensions of Healthcare can be characterized by the influx of information coming and going from each level as there are multiple different applications of Healthcare. These applications can include but are not limited to vaccines, surgeries, x-rays, medicines/treatments. Big data plays a pivotal role in Healthcare diagnostics, predictions, and accelerated results/outcomes of these applications. Big Data has the ability to save millions of dollars through automating 40% of radiologist’s tasks, saving time on potential treatments through digital patients, and by providing improved outcomes&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. With higher accuracy rates of diagnosis and advanced AI is able to transform hypothetical analysis into data driven diagnosis and treatment strategies.&lt;/p>
&lt;h2 id="2-patient-records">2. Patient Records&lt;/h2>
&lt;p>EHR stands for &amp;lsquo;electronic health records&amp;rsquo; and is a digital version of a patient’s paper chart.The Healthcare industry utilizes EHR for maintaining records of everything related in their institutions. EHR are real-time, patient centred records that make information available instantly and securely to authorized users&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. EHR is capable of holding even more information as it is possible to include such information such as medical history, diagnoses, medications, treatment plans, immunization dates, allergies, radiology images, and laboratory and test results. According to Definitive Healthcare data from 2020, more than 89 percent of all hospitals have implemented inpatient or ambulatory EHR systems &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. A network of information surrounding a patient&amp;rsquo;s health record and medical data allows for the research and production of such progressive advancement in treatment. To underline the potential of the resources, more than 110 million EHRs around the continents were inspected for genetic disease research&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This is the capability of EHRs as it holds information capable of diagnosing, preventing and treating other patients for early detection of an ailment or disease. Through the application of neural networks in Deep Learning models, EHR’s could be compiled and analyzed to identify inconspicuous indicators of disease development of patients in early stages far before a human doctor would be able to make a clear diagnosis. The application has the ability to work far ahead for preventive measures as well as the allocation of resources to make sure that patients are paying for the care at minimum costs, the appropriate method of medical intervention is applied, and physicians’ workload can become less strenuous.&lt;/p>
&lt;h3 id="21-ehr-application-detecting-error-and-reducing-costs">2.1 EHR Application: Detecting Error and Reducing Costs&lt;/h3>
&lt;p>In order to understand the impact that Big Data such as EHRs has on the Healthcare industry an example of research is presented in the form of collection before and after implementation of EHR. The research study collected data for the period of 1 year before EHR (pre-EHR) and 1 year after EHR (post-EHR) implementation. What was noticed in the analyzes of the data was in the area of &amp;lsquo;Medication errors and near misses&amp;rsquo; the research stated &amp;lsquo;medication errors per 1000 hospital days decreased 14.0%-from 17.9% in the pre-EHR period to 15.4% in the 9 months after CPOE implementation&amp;rsquo;&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The research determined that with implementation of EHR with (CPOE) computerized provider order entry was able to reduce the costs of treatment and improvised upon the safety of their patients. Participants of the study mentioned that there was an increase in speed when it came to pharmacy, laboratory and radiology orders. The research also stated &amp;lsquo;our study demonstrated an 18% reduction in laboratory testing&amp;rsquo;. The study touched upon the rapidness that EHR can add to a process of treatment when orders are validated much quicker and hospitals and patients save money from the rapid diagnosis and treatment. This cuts out the middle-man of deliberate testing and examinations upon patients so they don’t have to cover the costs or undergo wasteful testing from their own EHR and other extensive EHR that it utilizes for comparison. Examples of models used in this example study include data mining through phenotyping and natural language processing. In this way data mining allows large sets of patient data to be aggregated in order to make inferences over a population or theories regarding how a disease will progress in any given patient. Phenotyping categorizes features of patients' health and their DNA and ultimately their overall health. Association rule data mining helps automated systems in their predictions in order to predict behavioral and health outcomes of patients’ circumstances.&lt;/p>
&lt;h2 id="3-ai-models-in-cancer-detection">3. AI Models in Cancer Detection&lt;/h2>
&lt;p>AI is modifying early detection of cancer as models are capable of being more accurate and precise with the analysis of mass and cell images. The difficulty of diagnosing cancer is because of the possibilities of either the mass being benign or malignant. The amount of time overlooking the cell nuclei and its features to either determine if it is malignant or benign can be staggering for oncologists. Utilizing the information of what&amp;rsquo;s known about cancer can train AI to be calibrated to scour through several images and screenings of cell nuclei to find the key indicators. These key indicators can also be whittled down even further as there is AI to determine which indicators have the highest correlation with malignant cancer. As a dataset from Kaggle consisting of 569 cases of malignant and benign breast cancer, it represented 357 cases of benign and 212 of malignant. With that information there were initially 33 features that may have indicated malignancy in these cases. The 33 features were reduced to 10 features as not all of them equally displayed the same level of contribution to the diagnosis. Across the 10 features there were 5 features that demonstrated the highest correlation to the malignancy. Several models were adapted to find the highest accuracy and precision. This form of AI detection improves upon the efficacy of early cancer detection.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-352/raw/main/report/images/aimodels.PNG" alt="AI Models Demonstrate Accuracy &amp; Precision">&lt;/p>
&lt;p>&lt;strong>Figure 1.&lt;/strong> Demonstrates how AI in this study used images to cross-analyze features of a patient&amp;rsquo;s results to verify what model is the most accurate and precise to determine which model can best serve a physician in their diagnostic report.&lt;/p>
&lt;h3 id="31-early-detection-big-data-applications">3.1 Early Detection Big Data Applications&lt;/h3>
&lt;p>&amp;lsquo;An ounce of prevention is worth more than a pound of a cure&amp;rsquo; is a common philosophy held by medical professionals. The meaning behind this ideology is found in that if one can prevent a disease from ever taking its final form through performing small routine tasks and check ups, a plethora of harm and suffering from trying to recage a disease can be avoided. Many medical solutions for diseases such as cancer or degenerative brain diseases rely on the idea that outside medical intervention will strengthen the patient enough for the human body to heal itself through existing biological principles &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. For example, vaccines work by injecting dead cells into a patient so that its antibodies can be learned and immunity can be built up by white blood cells naturally. Intervening before one is infected must be completed for these measures to be effective. If preventative care such as routine screenings on individuals with family history of diseases or those with general genetic predispositions then the power truly lies in having the discernment knowledge to catch the disease early. In many diseases once a patient is presenting symptoms, it is too late or survival/recovery probability percentages are slashed. This places immense pressures on patients themselves to work to have access to routine screenings and even more pressure on physicians to intake these patients and make preliminary diagnosis with little more than a visual analysis of the patient. Big data automates these tasks and gives physicians an incredible advantage and discernment as to what is truly happening within a patient’s circumstance.&lt;/p>
&lt;h3 id="32-detecting-cervical-cancer">3.2 Detecting Cervical Cancer&lt;/h3>
&lt;p>Cervical cancer in the past was one of the most common causes of cancer death for women in the United States. However preventive care in the form of pap test has been able to drop the death rate significantly. In the pap test images are taken of the women’s cervix to identify any changes that might indicate cancer is going to form. Cervical cancer has a much higher death rate without early detection as a cure is easier to take full effect in the early stages. Artificial Intelligence performs an algorithm and gives the computer the ability to act and reason based on a set of known information. Machine learning implements more data and allows the computer to work iteratively and make predictions and make decisions based on the massive amount of data provided. In this way, machines have had the ability to detect cervical cancer with greater precision and accuracy in some cases than gynecologists &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Imaging of cervical screenings targeted by a convolution neural network is the key to unlocking correlations behind the large sum of images. By implementing further reasoning into the data set, the CNN is able to classify enhanced recognition of cancer as or before it forms. This study using this method of machine learning has been able to perform with 90-96% accuracy and save lives. The CNN is able to identify the colors, shapes, sizes, edges and other features pertaining to cancerous cells.&lt;/p>
&lt;p>This is ground breaking for women in underdeveloped countries like India and Nigeria where the death rate for cervical cancer is much higher than the United States due to lack of access to routine pap smears. Women could get results on their cervical cancer status even if they do not get a pap smear every 3 years as recommended by doctors. For example if a woman in Nigeria has her first pap smear at the age of 40 when the recommended age to start pap smears is 21 she has gone unchecked for nearly 20 years and the early detection window is narrowed. However, if she is one of the 20% of women who get cervical cancer over the age of 65, a deep learning analysis of her pap smear at 40 could save her life and roadblock potential suffering. Early detection is key and big data optimizes early detection windows by providing a deeper analysis in the preventive care stages. From here doctors are able to implement the best care plan available on a case by case basis.&lt;/p>
&lt;h2 id="4-artificial-intelligence-in-cardiovascular-disease">4. Artificial intelligence in Cardiovascular Disease&lt;/h2>
&lt;p>AI in cardiovascular disease models are innovating disease detection by segmenting different types of analysis together for more efficient and accurate results. Being that cardiovascular diseases typically agitate/involve the heart and lungs there are numerous dynamics surrounding why a person is experiencing certain symptoms or at risk for development of a more critical diagnosis. Immense amount of labor is included in the diagnosis and treatment of individuals with cardiovascular disease on behalf of general physicians, specialists, nurses, and several other medical professionals. Artificial intelligence has the capability to add a layer of ease and accuracy that is involved in analyzing a patient&amp;rsquo;s status or risk for cardiovascular disease. AI is able to overcome the challenges of low quality pixelated images from analyzes and draw clearer and more accurate conclusions at a stage where more prevention strategies can be implemented. AI in this sense is able to analyze the systems of the human body as a whole as opposed to a doctor which might have several appointments with a patient to determine results from evaluations on lungs, heart, etc. By segmenting x-rays from numerous patients AI is able to learn and grow its data set to produce increasingly accurate and precise results[^8].
By using a combination of recurrent neural networks and convolutional neural networks artificial intelligence is able to go beyond what currently exists in terms of medical analysis and provide optimum results for patients in need. Recurrent neural networks function by building upon past data in order to create new output in series. They work hand in hand with Convolutional Neural networks which focus on analyzing advanced imagery based on qualitative data and can weigh biases on potential prescriptive outcomes.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-352/raw/main/report/images/cardioai.PNG" alt="AI Learning Wireframe">[^10]&lt;/p>
&lt;p>&lt;strong>Figure 2.&lt;/strong> Demonstrates a wireframe of how data is computed to draw relevant conclusions from thousands of images and pinpoint exact predictions of diagnosis. Risk analysis is crucial for heart attack prevention and understanding how suspeectable a person is to heart failure. Being that heart attacks can lead to strokes due to loss of blood and oxygen to the brain, these imaging tools serve as an invaluable life saving mechanism to help bring prevention to the forefront of these medical emergencies.&lt;/p>
&lt;h2 id="5-deep-learning-techniques-for-genomics">5. Deep Learning Techniques for Genomics&lt;/h2>
&lt;p>A digital patient is the idea that a patient’s health record can be compiled with live and exact biometrics for the purpose of testing. Through this method medical professionals will have the ability to propose new solutions to patients and monitor potential effects of operations or medicines over a period of time in a condensed/rapid results format. Essentially if a patient would be able to see how their body reacts to medical procedures before they are performed. The digital copy of a patient would receive simulated trial treatments to better understand what would happen over a period of time if the solution was adopted. For example, a patient would be able to verify with their physician what type of diuretics, beta inhibitors, or angiotensin receptor blocker medication would be the most effective solution to their hypertension regulatory needs[^11]. Physicians would be able to mitigate the risks and side effects associated with a certain solution given a patients expected behavior in response to what has been uploaded to the model.
In order to produce deep learning results, models must be implemented by indicating genetic markers by which computational methods can traverse the genetics strands and draw relevant conclusions. In this way data can be processed to propose changes to disease carrying chains of DNA or fortify immune based responses in those who are immunocompromised[^9].&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-352/raw/main/report/images/genomics.PNG" alt="Genomics Illustration">&lt;/p>
&lt;p>&lt;strong>Figure 3.&lt;/strong> Illustrates how genes are analyzed through data collection methods such as EHR, personal biometric data, and family history in order to track what type of disease poses a threat and how to prevent, predict, and treat disease at the molecular level. Producing accurate methods of treatments, medications as well as predictions without having to put the patient through any trials.&lt;/p>
&lt;h2 id="6-discussion">6. Discussion&lt;/h2>
&lt;p>In considering the numerous innovations made possible by Big Data one can expect major impacts on society as we know it. Access to these types of data solutions should be made accessible to all those who are in need. Collectively an effort must be made to promote equitable access to life saving artificial intelligence discussed in this report. Processing power and lack of resources stand as a barrier to widespread access to proper testing. However, governments and industries in the private sector must work together to avoid monopolies and price gouging limitations to such valuable data and computing models.
With further investment into deep learning models error margins can be narrowed and risk percentages and be slimmed pertaining to prescriptive analysis in specific use cases. The more access to information and examples are available, the better and more advanced a deep learning system can become. With the addition of electronic health records and past analysis artificial intelligence has the power to exponentially revolutionize the healthcare industry. By providing patients with services that could save their lives there is more incentive to stay involved in personal health as computation is optimized targeting patients for more results focused visits to the doctor. Doctors themselves are able to be relieved of a portion of the workload and foster a greater work life balance through cutting down on testing time and having more time to interact with patients for educational informative appointments. Legally medical professionals will be able to use prediction errors as alternative signals to further analyze a patient and justify treatment measures. Using data visualization of potential outcomes via a specific treatment method will empower patients and doctors to choose the pathway with the most favorable outcome.Convolutional Neural Networks within deep learning is one of the if not the most essential form of algorithm for AI in healthcare. CNN allows images to be input in a way that allows for learnable weights and biases to be calculated for and differentiate and match aspects of images that would go unknown to the human eye. Through identifying the edges, shape, size, color, amount of scarring CNN is able to identify cancerous and non-cancerous cells into five categories: normal, mild, moderate, severe, and carcinoma. Accuracy in this space is above 95% and creates a new opportunity space for medical professionals to provide their patients with a high level of accuracy and timely action planning for treatment and recovery[^13]. Beyond human healthcare CNN modeling has the potential to transfer into the realm of veterinary medicine, agricultural engineering, and sustainable environment initiatives to detect invasive species and similar disease development. Dogs or cats with cancer or heart worm could be analyzed in order to determine that with their heredity/breed and life span what are the chances and timeline for disease development. Crop production could be amplified with the processing of plant genomes in combination with soil to foresee what combination will produce the most abundant and profitable harvest. Lastly, ecosystems distrubed by global warming have the capability of being studied with CNN in order to factor in changes to the environment and what solutions could be on the horizon. With enough sample collection the power of CNN has the capability of securing a brighter future for tomorrow.&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>Healthcare is an essential resource to living a long life and without it we can see our lifespan slashed nearly in half or even more for those who are hindered by hereditary ailments. Healthcare has been around as long as medicine and such other treatments have been around and that was centuries ago. The field has expanded well beyond what could’ve been expected for any medical professional or institution. Where the information and resources are available to save and care for the life before them even when a lack of training can hinder them the resources are present. It&amp;rsquo;s come to be such an accomplishment to mesh the medical practices of many medical professionals and Big Data to develop the largest compendium of medical practices in the world. By the allowance of such an asset many are able to collaborate with new findings and reinforcing old findings as these prevalent results allow physicians to work without faltering over inconclusive findings. The goal for this area of Big Data is to continue making the EHR system more secure and friendly towards medical professionals in different areas of practice as well as allowing easy access for patients who seek out their own medical history. The more advancements in this area of Healthcare can be applicable to other fields that must reference the compendium that maintains individuals and their history going forward. Such a structure will continue to aid generations of physicians and patients alike and can aid technological advancements along the way.&lt;/p>
&lt;h2 id="8-acknowledgements">8. Acknowledgements&lt;/h2>
&lt;p>We would like to thank Dr Gregor von Laszweski for allowing us to complete this report despite the delays there was as well as the lack of communication. We would also like to thank the AI team for their commitment to assisting in this class as even through a pandemic they continued to help the students complete the course. We would also like to thank Dr. Geoffrey Fox for teaching the course and making the class as informative as possible given his experience with the field of Big Data.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>[^8] Arslan, M., Owais, M., &amp;amp; Mahmood, T. Artificial Intelligence-Based Diagnosis of Cardiac and Related Diseases (2020, March 23). Retrieved December 13, 2020 from &lt;a href="https://www.mdpi.com/2077-0383/9/3/871/htm">https://www.mdpi.com/2077-0383/9/3/871/htm&lt;/a>&lt;/p>
&lt;p>[^9] Eraslan, G., Avsec, Z., Gagneur, J., &amp;amp; Theis, Fabian J.. Deep learning: new computational modelling techniques for genomics. (2019, April 10). Retrieved December 14, 2020 from &lt;a href="https://www.nature.com/articles/s41576-019-0122-6">https://www.nature.com/articles/s41576-019-0122-6&lt;/a>&lt;/p>
&lt;p>[^10] 1Regina. AI to Detect Cancer. (2019, November 22). Retrieved December 14, 2020 from &lt;a href="https://towardsdatascience.com/ai-for-cancer-detection-cadb583ae1c5">https://towardsdatascience.com/ai-for-cancer-detection-cadb583ae1c5&lt;/a>&lt;/p>
&lt;p>[^11] Koumakis, L. Deep learning models in genomics; are we there yet? (2020). Retrieved December 14, 2020 from &lt;a href="https://www.sciencedirect.com/science/article/pii/S2001037020303068">https://www.sciencedirect.com/science/article/pii/S2001037020303068&lt;/a>&lt;/p>
&lt;p>[^12] Ross, M.K., Wei, W., &amp;amp; Ohno-Machado, L., &amp;lsquo;Big Data&amp;rsquo; and the Electronic Health Record (2014, August 15). Retrieved 15, 2020 from &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4287068/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4287068/&lt;/a>&lt;/p>
&lt;p>[^13] P, Shanthi. B., Faruqi, F., K, Hareesha, K., &amp;amp; Kudva, R., Deep Convolution Neural Network for Malignancy Detection and Classification in Microscopic Uterine Cervex Cell Images (2019, November 1). Retrieved December 15, 2020 from &lt;a href="https://pubmed.ncbi.nlm.nih.gov/31759371/">https://pubmed.ncbi.nlm.nih.gov/31759371/&lt;/a>&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Laney, D., AD. Mauro, M., Gubbi, J., Doyle-Lindrud, S., Gillum, R., Reiser, S., . . . Reardon, S. Big data in healthcare: Management, analysis and future prospects (2019, June 19). Retrieved December 10, 2020, from &lt;a href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0217-0">https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0217-0&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>What is an electronic health record (EHR)? (2019, September 10). Retrieved December 10, 2020 from &lt;a href="https://www.healthit.gov/faq/what-electronic-health-record-ehr">https://www.healthit.gov/faq/what-electronic-health-record-ehr&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Moriarty, A. Does Hospital EHR Adoption Actually Improve Data Sharing? (2020, October 23) Retrieved December 10, 2020 from &lt;a href="https://blog.definitivehc.com/hospital-ehr-adoption">https://blog.definitivehc.com/hospital-ehr-adoption&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Cruciana, Paula A. The Implications of Big Data in Healthcare (2019, November 21) Retrieved December 11, 2020 from &lt;a href="https://ieeexplore.ieee.org/document/8970084">https://ieeexplore.ieee.org/document/8970084&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Zlabek, Jonathan A. Early cost and safety benefits of an inpatient electronic health record (2011, February 2) Retrieved December 10, 2020 from &lt;a href="https://academic.oup.com/jamia/article/18/2/169/802487">https://academic.oup.com/jamia/article/18/2/169/802487&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Artificial Intelligence-Oppurtunities in Cancer Research. (2020, August 31). Retrieved December 11, 2020 from &lt;a href="https://www.cancer.gov/research/areas/diagnosis/artificial-intelligence">https://www.cancer.gov/research/areas/diagnosis/artificial-intelligence&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Zhang, R., Simon, G., &amp;amp; Yu, F. Advancing Alzheimer&amp;rsquo;s research: A review of big data promises. (2017, June 4) Retrieved December 11, 2020 from &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5590222/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5590222/&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analysis of Various Machine Learning Classification Techniques in Detecting Heart Disease</title><link>/report/fa20-523-309/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-309/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-309/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-309/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Ethan Nguyen, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-309">fa20-523-309&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>As cardiovascular diseases are the number 1 cause
of death in the United States, the study of the factors and early detection and treatment could improve quality of life and lifespans. From investigating how the variety of factors related to cardiovascular health relate to a general trend, it has resulted in general guidelines to reduce the risk of experiencing a cardiovascular disease. However, this is a rudimentary way of preventative care that allows for those who do not fall into these risk categories to fall through. By applying machine learning, one could develop a flexible solution to actively monitor, find trends, and flag patients at risk to be treated immediately. Solving not only the risk categories but has the potential to be expanded to annual checkup data revolutionizing health care.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-dataset-cleaning">2.1 Dataset Cleaning&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-dataset-analysis">2.2 Dataset Analysis&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-machine-learning-algorithms-and-implementation">3. Machine Learning Algorithms and Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-scikit-learn-and-algorithm-types">3.1 Scikit-Learn and Algorithm Types&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-classification-algorithms">3.2 Classification Algorithms&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#321-support-vector-machines">3.2.1 Support Vector Machines&lt;/a>&lt;/li>
&lt;li>&lt;a href="#322-k-nearest-neighbors">3.2.2 K-Nearest Neighbors&lt;/a>&lt;/li>
&lt;li>&lt;a href="#323-gaussian-naive-bayes">3.2.3 Gaussian Naive Bayes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#324-decision-trees">3.2.4 Decision Trees&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#33-clustering-algorithms">3.3 Clustering Algorithms&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#331-k-means">3.3.1 K-Means&lt;/a>&lt;/li>
&lt;li>&lt;a href="#332-mean-shift">3.3.2 Mean-shift&lt;/a>&lt;/li>
&lt;li>&lt;a href="#333-spectral-clustering">3.3.3 Spectral Clustering&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#34-implementation">3.4 Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#341-dataset-preprocessing">3.4.1 Dataset Preprocessing&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-results--discussion">4. Results &amp;amp; Discussion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-algorithm-metrics">4.1 Algorithm Metrics&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#411-support-vector-machines">4.1.1 Support Vector Machines&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-k-nearest-neighbors">4.1.2 K-Nearest Neighbors&lt;/a>&lt;/li>
&lt;li>&lt;a href="#413-gaussian-naive-bayes">4.1.3 Gaussian Naive Bayes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#414-decision-trees">4.1.4 Decision Trees&lt;/a>&lt;/li>
&lt;li>&lt;a href="#415-k-means">4.1.5 K-Means&lt;/a>&lt;/li>
&lt;li>&lt;a href="#416-mean-shift">4.1.6 Mean-shift&lt;/a>&lt;/li>
&lt;li>&lt;a href="#417-spectral-clustering">4.1.7 Spectral Clustering&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#42-system-information">4.2 System Information&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-discussion">4.3 Discussion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgements">6. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#references">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> health, healthcare, cardiovascular disease, data analysis&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Since cardiovascular diseases are the number 1 cause of death in the United States, early prevention could help in extending one’s life span and possibly quality of life &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Since there are cases where patients do not show any signs of cardiovascular trouble until an event occurs, having an algorithm predict from their medical history would help in picking up on early warning signs a physician may overlook. Or could also reveal additional risk factors and patterns for research on prevention and treatment. In turn this would be a great tool to apply in preventive care, which is the type of healthcare policy that focuses in diagnosing and preventing health issues that would otherwise require specialized treatment or is not treatable &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This also has the potential to trickle down and increase the quality of life and lifespan of populations at a reduced cost as catching issues early most likely results in cheaper treatments &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This project will take a high-level overview of common, widely available classification algorithms and analyze their effectiveness for this specific use case. Notable ones include, Gaussian Naive Bayes, K-Nearest Neighbors, and Support Vector Machines. Additionally, two data sets that contain common features will be used to increase the training and test pool for evaluation. As well as to explore if additional feature types contribute to a better prediction. The goal of this project being a gateway to further research in data preprocessing, tuning, or development of specialized algorithms as well as further ideas on what data could be provided.&lt;/p>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.kaggle.com/johnsmith88/heart-disease-dataset">https://www.kaggle.com/johnsmith88/heart-disease-dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kaggle.com/sulianova/cardiovascular-disease-dataset">https://www.kaggle.com/sulianova/cardiovascular-disease-dataset&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The range of creation dates are 1988 and 2019 respectively with different features of which 4 are common between. This does bring up a small hiccup in preprocessing to consider. Namely the possibility of changing diet and culture trends resulting in significantly different trends/patterns within the same age group. As well as possible differences in measurement accuracy. However this large gap is within the scope of the project in exploring which features can help provide an accurate prediction.&lt;/p>
&lt;p>This possible phenomenon may be of interest to explore closely if time allows. Whether a trend itself is even present or there is an overarching trend across different cultures and time periods. Or to consider if this difference is significant enough that the data from the various sets needs to be adjusted to normalize the ages to present day.&lt;/p>
&lt;h3 id="21-dataset-cleaning">2.1 Dataset Cleaning&lt;/h3>
&lt;p>The datasets used have already been significantly cleaned from the raw data and has been provided as a csv file. These files were then imported into the python notebook as pandas dataframes for easy manipulation.&lt;/p>
&lt;p>An initial check was made to ensure the integrity of the data matched the description from the source websites. Then some preprocessing was completed to normalize the common features between the datasets. These features were gender, age, and cholesterol levels. The first two adjustments were trivial in conversion however, in the case of cholesterol levels, the 2019 set is on a 1-3 scale while the 1988 dataset provided them as real measurements. A conversion of the 1988 dataset was done based on guidelines found online for the age range of the dataset &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="22-dataset-analysis">2.2 Dataset Analysis&lt;/h3>
&lt;p>From this point on, the 1988 dataset will be referred to as &lt;code>dav_set&lt;/code> and 2019 data set will be referred to as &lt;code>sav_set&lt;/code>.&lt;/p>
&lt;p>To provide further insight on what to expect and how a model would be applied, the population of the datasets was analysed first. As depicted in Figure 2.1 the population samples of both datasets of gender vs age show the majority of the data is centered around 60 years of age with a growing slope from 30 onwards.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/agevssex.jpg" alt="Figure 2.1">&lt;/p>
&lt;p>&lt;strong>Figure 2.1&lt;/strong>: Age vs Gender distributions of the dav_set and sav_set.&lt;/p>
&lt;p>This trend appears to signify that the datasets focused solely on an older population or general trend in society of not monitoring heart conditions as closely in the younger generation.&lt;/p>
&lt;p>Moving on to Figure 2.2, we see an interesting trend with a significant growing trend in the sav_set in older population having more cardiovascular issues compared to the dav_set. While this cannot be seen in the dav_set. This may be caused by the additional life expectancy or a change in diet as noted in the introduction.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/agevstarget.jpg" alt="Figure 2.2">&lt;/p>
&lt;p>&lt;strong>Figure 2.2&lt;/strong>: Age vs Target distributions of the dav_set and sav_set.&lt;/p>
&lt;p>In Figure 2.3, the probability of having cardiovascular issues between the sets are interesting. In the dav_set the inequality of higher probability could be attributed to the larger female samples in the dataset. With the sav_set having a more equal probability between the genders.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/gendervsprobability.jpg" alt="Figure 2.3">&lt;/p>
&lt;p>&lt;strong>Figure 2.3&lt;/strong>: Gender vs Probability of cardiovascular issues of the dav_set and sav_set.&lt;/p>
&lt;p>Finally, in Figure 2.4 is the probability vs cholesterol levels. This one is very interesting between the two datasets in terms of trend levels. With the dav_set having a higher risk at normal levels compared to the sav_set. This could be another hint of a societal change across the years or may in fact be due to the low sample size. Especially since the sav_set matches the general consensus of higher cholesterol levels increasing risk of cardiovascular issues &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/cholesterolvsprobability.jpg" alt="Figure 2.4">&lt;/p>
&lt;p>&lt;strong>Figure 2.4&lt;/strong>: Cholesterol levels vs Probability of cardiovascular issues of the dav_set and sav_set.&lt;/p>
&lt;p>To close out this initial analysis is the correlation map of each of the features. From Figure 2.5 and 2.6 it can be concluded that both of these datasets are viable to conduct machine learning as the correlation factor is below the recommended value of 0.8 &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Although we do see the signs of a low sample amount in the dav_set with a higher correlation factor compared to the sav_set.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davsetcorrelation.jpg" alt="Figure 2.5">&lt;/p>
&lt;p>&lt;strong>Figure 2.5&lt;/strong>: dav_set correlation matrix.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savsetcorrelation.jpg" alt="Figure 2.6">&lt;/p>
&lt;p>&lt;strong>Figure 2.6&lt;/strong>: sav_set correlation matrix.&lt;/p>
&lt;h2 id="3-machine-learning-algorithms-and-implementation">3. Machine Learning Algorithms and Implementation&lt;/h2>
&lt;p>With many machine learning algorithms already available and many more in development. Selecting the optimal one for an application can be a challenging balance since each algorithm has both its advantages and disadvantages. As mentioned in the introduction, we will explore applying the most common and established algorithms available to the public.&lt;/p>
&lt;p>Starting off, is selecting a library from the most popular ones available. Namely Keras, Pytorch, Tensorflow, and Scikit-Learn. Upon further investigation it was determined that Scikit-Learn would be used for this project. The reason being Scikit-Learn is a great general machine learning library that also includes pre and post processing functions. While Keras, Pytorch, and Tensorflow are targeted for neural networks and other higher-level deep learning algorithms which are outside of the scope of this project at this time &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="31-scikit-learn-and-algorithm-types">3.1 Scikit-Learn and Algorithm Types&lt;/h3>
&lt;p>Diving further into the Scikit-Learn library, its key strength appears to be the variety of algorithms available that are relatively easy to implement against a dataset. Of those available, they are classified under three different categories based on the approach each takes. They are as follows:&lt;/p>
&lt;ul>
&lt;li>Classification
&lt;ul>
&lt;li>Applied to problems that require identifying the category an object belongs to.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Regression
&lt;ul>
&lt;li>For predicting or modeling continuous values.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Clustering
&lt;ul>
&lt;li>Grouping similar objects into groups.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For this project, we will be investigating the Classification and Clustering algorithms offered by the library due to the nature of our dataset. Since it is a binary answer, the continuous prediction capability of regression algorithms will not fair well. Compared to classification type algorithms which are well suited for determining binary and multi-class classification on datasets &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Along with Clustering algorithms being capable of grouping unlabeled data which is one of the key problem points mentioned in the introduction &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="32-classification-algorithms">3.2 Classification Algorithms&lt;/h3>
&lt;p>The following algorithms were determined to be candidates for this project based on the documentation available on the Scikit-learn for supervised learning &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="321-support-vector-machines">3.2.1 Support Vector Machines&lt;/h4>
&lt;p>This algorithm was chosen because classification is one of the target types and has a decent list of advantages that appear to be applicable to this dataset &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;ul>
&lt;li>Effective in high dimensional spaces as well as if the number dimensions out number samples.&lt;/li>
&lt;li>Is very versatile.&lt;/li>
&lt;/ul>
&lt;h4 id="322-k-nearest-neighbors">3.2.2 K-Nearest Neighbors&lt;/h4>
&lt;p>This algorithm was selected due to being a non-parametric method that has been successful in classification applications &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. From the dataset analysis, it is appears that the decision boundary may be very irregular which is a strong point of this type of method.&lt;/p>
&lt;h4 id="323-gaussian-naive-bayes">3.2.3 Gaussian Naive Bayes&lt;/h4>
&lt;p>Is an implementation of the Naive Bayes theorem that has been targeted for classification. The advantages of this algorithm is its speed and requires a small training set compared to more advanced algorithms &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="324-decision-trees">3.2.4 Decision Trees&lt;/h4>
&lt;p>This algorithm was chosen to investigate another non-parametric method to determine their efficacy against this dataset application. This algorithm also has some advantages over K-Nearest namely &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>.&lt;/p>
&lt;ul>
&lt;li>Simple to interpret and visualize&lt;/li>
&lt;li>Requires little data preparation
&lt;ul>
&lt;li>Handles numerical and categorical data instead of needing to normalize&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Can validate the model and is possible to audit from a liability standpoint.&lt;/li>
&lt;/ul>
&lt;h3 id="33-clustering-algorithms">3.3 Clustering Algorithms&lt;/h3>
&lt;p>The following algorithms were determined to be candidates for this project based on the table of clustering algorithms available on the Scikit-learn &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="331-k-means">3.3.1 K-Means&lt;/h4>
&lt;p>The usecase for this algorithm is general purpose with even and low number of clusters &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Of which the sav_set appears to have with the even distribution across most of the features.&lt;/p>
&lt;h4 id="332-mean-shift">3.3.2 Mean-shift&lt;/h4>
&lt;p>This algorithm was chosen for its strength in dealing with uneven cluster sizes and non-flat geometry &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Though it is not easily scalable the application of our small dataset size might be of interest.&lt;/p>
&lt;h4 id="333-spectral-clustering">3.3.3 Spectral Clustering&lt;/h4>
&lt;p>As an inverse, this algorithm was chosen for its strength with fewer uneven clusters &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. In comparison to Mean-shift, this maybe the better algorithm for this application.&lt;/p>
&lt;h3 id="34-implementation">3.4 Implementation&lt;/h3>
&lt;p>The implementation of these algorithms were done under the direction of the documentation page for each respective algorithm. The jupyter notebook used for this project is available at &lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/data_analysis/ml_algorithms.ipynb">https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/data_analysis/ml_algorithms.ipynb&lt;/a> with each algorithm having a corresponding cell. A benchmarking library is also included to determine the efficiency of each algorithm in processing time. One thing of note is the lack of functions used for the classification compared to the clustering algorithms. The justification for this discrepancy is due to inexperience in creating optimal implementations as well as determining that not being implemented in a function would not have a significant impact on performance. Additionally, graphs representing the test data were included to help visualize the performance of the clustering algorithms utilizing example code from the documentation &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="341-dataset-preprocessing">3.4.1 Dataset Preprocessing&lt;/h4>
&lt;p>Pre-processing of the cleaned datasets for the classification algorithms was done under guidance of the scikit learn documentation &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. Overall, each algorithm was trained and tested with the same split for each run. While the split data could have been passed directly to the algorithms, they were normalized further using the built-in fit_transform function for the best results possible.&lt;/p>
&lt;p>Pre-processing of the cleaned datasets for the clustering algorithms was done under guidance of the scikit learn documentation &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Compared to the classification algorithms, a dimensionality reduction was conducted using Principal component analysis (PCA). This step condenses the multiple features into a 2 feature array which the clustering algorithms were optimized for, increasing the odds for the best results possible. Another note is the dataset split was conducted during execution of the algorithm. Upon further investigation, it was determined that this does not have an effect on the ending results as the randomization was disabled due to setting the same random_state parameter for each call.&lt;/p>
&lt;h2 id="4-results--discussion">4. Results &amp;amp; Discussion&lt;/h2>
&lt;h3 id="41-algorithm-metrics">4.1 Algorithm Metrics&lt;/h3>
&lt;p>The metrics used to determine the viability of each of the algorithms are precision, recall, and f1-score. These are simple metrics based on the values from a confusion matrix which is a visualization of the False and True Positives and Negatives. Precision is essentially how accurate was the algorithm in classifying each data point. This however, is not a good metric to solely base performance as precision does not account for imbalanced distributions within a dataset &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This is where the recall metric comes in which is defined as how many samples were accurately classified by the algorithm. This is a more versatile metric as it can compensate for imbalanced datasets. While it may not be in our case as seen in the dataset analysis where we have a relatively balanced ratio. It still gives great insight on the performance for our application.&lt;/p>
&lt;p>Finally is the f1-score which is the harmonic mean of the precision and recall metric &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. This will be the key metric we will mainly focus on as it strikes a good balance between the two more primitive metrics. Since one may think in medical applications one would want to maximize recall, it is at the cost of precision which ends up in more false predictions which is essentially an overfitting scenario &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. Something that reduces the viability of the model to the application especially since we have a relatively balanced dataset, more customized weighting is not as necessary.&lt;/p>
&lt;p>The metrics for each algorithm implementation are as follows. The training time metric is provided by the cloudmesh.common benchmark library &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="411-support-vector-machines">4.1.1 Support Vector Machines&lt;/h4>
&lt;p>&lt;strong>Table 4.1:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.94&lt;/td>
&lt;td>0.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.95&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.97&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.038 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.2:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.94&lt;/td>
&lt;td>0.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.95&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.97&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>167.897 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="412-k-nearest-neighbors">4.1.2 K-Nearest Neighbors&lt;/h4>
&lt;p>&lt;strong>Table 4.3:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.88&lt;/td>
&lt;td>0.86&lt;/td>
&lt;td>0.87&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.87&lt;/td>
&lt;td>0.90&lt;/td>
&lt;td>0.88&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.025 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.4:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.62&lt;/td>
&lt;td>0.74&lt;/td>
&lt;td>0.67&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.67&lt;/td>
&lt;td>0.54&lt;/td>
&lt;td>0.60&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>10.116 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="413-gaussian-naive-bayes">4.1.3 Gaussian Naive Bayes&lt;/h4>
&lt;p>&lt;strong>Table 4.5:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.88&lt;/td>
&lt;td>0.81&lt;/td>
&lt;td>0.84&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.83&lt;/td>
&lt;td>0.90&lt;/td>
&lt;td>0.86&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.011 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.6:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.90&lt;/td>
&lt;td>0.69&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.72&lt;/td>
&lt;td>0.28&lt;/td>
&lt;td>0.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.057 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="414-decision-trees">4.1.4 Decision Trees&lt;/h4>
&lt;p>&lt;strong>Table 4.7:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.92&lt;/td>
&lt;td>0.97&lt;/td>
&lt;td>0.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.97&lt;/td>
&lt;td>0.93&lt;/td>
&lt;td>0.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.009 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.8:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.71&lt;/td>
&lt;td>0.80&lt;/td>
&lt;td>0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>0.66&lt;/td>
&lt;td>0.71&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.272 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="415-k-means">4.1.5 K-Means&lt;/h4>
&lt;p>&lt;strong>Figure 4.1:&lt;/strong> dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davkmeans.jpg" alt="Figure 4.1">&lt;/p>
&lt;p>&lt;strong>Table 4.9:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.22&lt;/td>
&lt;td>0.29&lt;/td>
&lt;td>0.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.12&lt;/td>
&lt;td>0.09&lt;/td>
&lt;td>0.10&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.376 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 4.2:&lt;/strong> sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savkmeans.jpg" alt="Figure 4.2">&lt;/p>
&lt;p>&lt;strong>Table 4.10:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.51&lt;/td>
&lt;td>0.69&lt;/td>
&lt;td>0.59&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.52&lt;/td>
&lt;td>0.34&lt;/td>
&lt;td>0.41&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>1.429 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="416-mean-shift">4.1.6 Mean-shift&lt;/h4>
&lt;p>&lt;strong>Figure 4.3:&lt;/strong> dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davmeanshift.jpg" alt="Figure 4.3">&lt;/p>
&lt;p>&lt;strong>Table 4.11:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.47&lt;/td>
&lt;td>1.00&lt;/td>
&lt;td>0.64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.461 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 4.4:&lt;/strong> sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savmeanshift.jpg" alt="Figure 4.4">&lt;/p>
&lt;p>&lt;strong>Table 4.12:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.50&lt;/td>
&lt;td>1.00&lt;/td>
&lt;td>0.67&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>193.93 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="417-spectral-clustering">4.1.7 Spectral Clustering&lt;/h4>
&lt;p>&lt;strong>Figure 4.5:&lt;/strong> dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davspectral.jpg" alt="Figure 4.5">&lt;/p>
&lt;p>&lt;strong>Table 4.13:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.86&lt;/td>
&lt;td>0.74&lt;/td>
&lt;td>0.79&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.79&lt;/td>
&lt;td>0.89&lt;/td>
&lt;td>0.84&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.628 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 4.6:&lt;/strong> sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savspectral.jpg" alt="Figure 4.6">&lt;/p>
&lt;p>&lt;strong>Table 4.14:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.57&lt;/td>
&lt;td>0.57&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.56&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>208.822 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="42-system-information">4.2 System Information&lt;/h3>
&lt;p>Google Collab was used to train and evaluate the models selected. The specifications of the system in use is provided by the cloudmesh.common benchmark library and is listed in Table 4.15 &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Table 4.15&lt;/strong>: Training and Evaluation System Specifications&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attribute&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BUG_REPORT_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://bugs.launchpad.net/ubuntu/%22">https://bugs.launchpad.net/ubuntu/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_DESCRIPTION&lt;/td>
&lt;td>&amp;ldquo;Ubuntu 18.04.5 LTS&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_ID&lt;/td>
&lt;td>Ubuntu&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_RELEASE&lt;/td>
&lt;td>18.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HOME_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://www.ubuntu.com/%22">https://www.ubuntu.com/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ID&lt;/td>
&lt;td>ubuntu&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ID_LIKE&lt;/td>
&lt;td>debian&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NAME&lt;/td>
&lt;td>&amp;ldquo;Ubuntu&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PRETTY_NAME&lt;/td>
&lt;td>&amp;ldquo;Ubuntu 18.04.5 LTS&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PRIVACY_POLICY_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy%22">https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SUPPORT_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://help.ubuntu.com/%22">https://help.ubuntu.com/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UBUNTU_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION&lt;/td>
&lt;td>&amp;ldquo;18.04.5 LTS (Bionic Beaver)&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION_ID&lt;/td>
&lt;td>&amp;ldquo;18.04&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu_count&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.active&lt;/td>
&lt;td>698.5 MiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.available&lt;/td>
&lt;td>11.9 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.free&lt;/td>
&lt;td>9.2 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.inactive&lt;/td>
&lt;td>2.6 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.percent&lt;/td>
&lt;td>6.5 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.total&lt;/td>
&lt;td>12.7 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.used&lt;/td>
&lt;td>1.6 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>platform.version&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python&lt;/td>
&lt;td>3.6.9 (default, Oct 8 2020, 12:12:24) [GCC 8.4.0]&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.pip&lt;/td>
&lt;td>19.3.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.version&lt;/td>
&lt;td>3.6.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sys.platform&lt;/td>
&lt;td>linux&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.machine&lt;/td>
&lt;td>x86_64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.node&lt;/td>
&lt;td>bc15b46ebcf6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.processor&lt;/td>
&lt;td>x86_64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.release&lt;/td>
&lt;td>4.19.112+&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.system&lt;/td>
&lt;td>Linux&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.version&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>user&lt;/td>
&lt;td>collab&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="43-discussion">4.3 Discussion&lt;/h3>
&lt;p>In analyzing the resulting metrics in section 4.1, two major trends between the algorithms are apparent.&lt;/p>
&lt;ol>
&lt;li>The classification algorithms perform significantly better than the clustering algorithms.&lt;/li>
&lt;li>Significant signs of overfitting for the dav_set.&lt;/li>
&lt;/ol>
&lt;p>Addressing the first point, it is obvious from the metric performance where on average the classification algorithms were higher than the clustering algorithms. At a lower training time cost as well, which indicates that classification algorithms are well suited for this application than clustering. Especially when looking at the results for Mean-Shift in section 4.1.6 where the algorithm failed to identify any patient with a disease. This also illustrates the discussion on the metrics used to determine performance as the recall was 100% at the cost of missing every patient that would have required treatment illustrated by Figure 4.3 and 4.4. On this topic, comparing the actual data graphs for each of the clustering algorithms and comparing them to the example clustering figures within the scikit documentation, it solidifies that this is not the correct algorithm type for this dataset &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Moving on to the next point, it can be seen that overfitting is occurring for the dav_set in comparing the performance to the sav_set for the same algorithm which can be seen in the corresponding tables in sections 4.1.2, 4.1.3, and 4.1.4. Here the performance gap is at least 20% between the two compared to what one would assume should be relatively close to each other. While this could also illustrate the affect the various features have on the algorithm, it was determined that this is most likely due to the small dataset size having a larger influence than anticipated.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>Reviewing these results, a clear conclusion cannot be accurately be determined due to the considerable amount of variables involved that were not able to be isolated to a desirable level. Namely the compromises that were mentioned in section 2.1 and general dataset availability. However, it was determined that the main goal of this project was accomplished where the Support Vector Machine algorithm was narrowed down as a viable candidate for future work. Due in part to the overall f1-score performance for both datasets, providing confidence that overfitting may not occur. While there is a downside in scalability due to the significant increase in training time between the smaller dav_set and larger sav_set. This could indicate that further research should be focused on either improving this algorithm or creating a new one based on the underlying mechanism.&lt;/p>
&lt;p>In relation to the types of features, it could be interpreted from this project that further efforts require a more expansive and modern dataset to perform to a level suitable for real world applications. As possible factors affecting the performance are in the accuracy and granularity of the measurements and factors available to learn from. This however, is seen to be a difficult challenge due to the nature of privacy laws on health data but, as proposed in the introduction. It would be very interesting to apply this project&amp;rsquo;s findings on more general health data that is retrieved in annual visits.&lt;/p>
&lt;h2 id="6-acknowledgements">6. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their assistance and suggestions with regard to this project.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Centers for Disease Control and Prevention. 2020. Heart Disease Facts | Cdc.Gov. [online] Available at: &lt;a href="https://www.cdc.gov/heartdisease/facts.htm">https://www.cdc.gov/heartdisease/facts.htm&lt;/a> [Accessed 16 November 2020].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Amadeo, K., 2020. Preventive Care: How It Lowers Healthcare Costs In America. [online] The Balance. Available at: &lt;a href="https://www.thebalance.com/preventive-care-how-it-lowers-aca-costs-3306074">https://www.thebalance.com/preventive-care-how-it-lowers-aca-costs-3306074&lt;/a> [Accessed 16 November 2020].&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>WebMD. 2020. Understanding Your Cholesterol Report. [online] Available at: &lt;a href="https://www.webmd.com/cholesterol-management/understanding-your-cholesterol-report">https://www.webmd.com/cholesterol-management/understanding-your-cholesterol-report&lt;/a> [Accessed 21 October 2020].&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>R, V., 2020. Feature Selection — Correlation And P-Value. [online] Medium. Available at: &lt;a href="https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf">https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf&lt;/a> [Accessed 21 October 2020].&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Stack Overflow. 2020. Differences In Scikit Learn, Keras, Or Pytorch. [online] Available at: &lt;a href="https://stackoverflow.com/questions/54527439/differences-in-scikit-learn-keras-or-pytorch">https://stackoverflow.com/questions/54527439/differences-in-scikit-learn-keras-or-pytorch&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.4. Support Vector Machines — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/svm.html#classification">https://scikit-learn.org/stable/modules/svm.html#classification&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 2.3. Clustering — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/clustering.html#clustering">https://scikit-learn.org/stable/modules/clustering.html#clustering&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1. Supervised Learning — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning">https://scikit-learn.org/stable/supervised_learning.html#supervised-learning&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.6. Nearest Neighbors — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/neighbors.html">https://scikit-learn.org/stable/modules/neighbors.html&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.9. Naive Bayes — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/naive_bayes.html">https://scikit-learn.org/stable/modules/naive_bayes.html&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.10. Decision Trees — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/tree.html">https://scikit-learn.org/stable/modules/tree.html&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. A Demo Of K-Means Clustering On The Handwritten Digits Data — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py">https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py&lt;/a> [Accessed 17 November 2020].&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 6.3. Preprocessing Data — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing">https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing&lt;/a> [Accessed 17 November 2020].&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Mianaee, S., 2020. 20 Popular Machine Learning Metrics. Part 1: Classification &amp;amp; Regression Evaluation Metrics. [online] Medium. Available at: &lt;a href="https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce">https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce&lt;/a> [Accessed 10 November 2020].&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Predicting Hotel Reservation Cancellation Rates</title><link>/report/fa20-523-323/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-323/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-323/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-323/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Anthony Tugman, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-323/">fa20-523-323&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-323/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>As a result of the Covid-19 pandemic all segments of the travel industry face financial struggle. The lodging segment, in particular, has had the financial records scrutinized revealing a glaring problem. Since the beginning of 2019, the lodging segment has seen reservation cancellation rates near 40%. At the directive of business and marketing experts, hotels have previously attempted to solve the problem through an increased focus on reservation retention, flexible booking policies, and targeted marketing. These attempts did not produce results, and continue to leave rooms un-rented which is detrimental to the bottom line. This document will explain the creation and testing of a novel process to combat the rising cancellation rate. By analyzing reservation data from a nationwide hotel chain, it is hoped that an algorithm may be developed capable of predicting the likeliness that a traveler is to cancel a reservation. The resulting algorithm will be evaluated for accuracy. If the resulting algorithm has a satisfactory accuracy, it would make clear to the hotel industry that the use of big data is key to solving this problem.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-datasets">3. DataSets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-data-preprocessing">4. Data Preprocessing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-model-creation">5. Model Creation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> travel, finance, hospitality, tourism, data analysis, environment, big data&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big Data is a term that describes the large volume of data that is collected and stored by a business on a day-to-day basis. While the scale of this data is impressive, what is more interesting is what can be done by analyzing the data &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Becoming more commonplace, companies are beginning to use Big Data to gain advantage in competing, innovating, and capturing customers. It is necessary for businesses to collaborate with Data Scientists to expose patterns and other insights that can be gained from inspection of the data. This collaboration is amongst software engineers, hardware engineers, and Data Scientists who develop powerful machine learning algorithms to efficiently analyze the data. Businesses have multiple benefits to gain from effectively utilizing Big Data including cost savings, time reductions, market condition insights, advertising insights, as well as driving innovation and product development &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The lodging industry takes a hit to the bottom line each year as the rate of room reservation cancellations continues to rise. In the instance that a guest cancels a room without adequate notice there are additional expenses the hotel faces to re-market the room as available. If the room is unable to be rented, the hotel loses the revenue &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. At the directive of business and marketing experts, hotels have attempted to solve this problem through an increased focus on reservation retention, flexible booking policies, and targeted marketing campaigns. However, even with these efforts, the reservation cancellation rate continues to rise unchecked reaching upwards of 40% in 2019 &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. By analyzing the Big Data the lodging industry collects on its customers, it would be possible to form a model capable of predicting whether a customer is likely to cancel a reservation. To develop this model, a large data set was sourced that tracked anonymized reservation information for a chain of hotels over a three-year period. Machine learning techniques will be applied to the data set to form a model capable of producing the aforementioned predictions. If the model proves to be statistically significant, recommendations will be made to the lodging industry as to how the results should be interpreted.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>Room reservation completion rates are an important consideration for revenue management in the hotel industry. Unsurprisingly, there have been previous attempts at creating an algorithm capable of predicting room cancellation rates. However, these attempts seem to have taken a more general approach such as predicting cancellations by particular ranges of dates &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Other attempts make broad claims about the accuracy of the resulting algorithm without reliably proving so through statistical analysis &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Most importantly for the scope of this course, these previous attempts use subsets of the full data set available. This has the potential to lead to unpredictability in the performance of the algorithm.&lt;/p>
&lt;p>To differentiate from and improve on the attempts previously mentioned, the algorithm produced as a result of the current effort will form a model capable of predicting if a reservation with cancel based on the reservation as a whole, rather than a subset of dates. Not predict the cancellation rate over general stipulations, but rather for a specific customer. Additionally, a special focus will be on proving that the created algorithm is statistically significant and can accurately be extrapolated to larger subsets of the data. Finally, for initial training and testing, larger subsets of the data sets will be utilized due to the increased processing power available from Google Colab. It is important to note that the first-mentioned previous work &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> appears to use a proprietary data set while the second-mentioned work &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> appears to utilize the same data set that this study will as well.&lt;/p>
&lt;h2 id="3-datasets">3. DataSets&lt;/h2>
&lt;p>Locating a data set appropriate for this task proved to be challenging. Privacy concerns and the desire to keep internal corporate information confidential adds difficulty in locating the necessary data. Ultimately, the following data repository was selected to train and test the reservation cancellation, prediction model:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://www.kaggle.com/jessemostipak/hotel-booking-demand">Hotel Booking Demand&lt;/a> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>The Hotel Booking Demand data set contains approximately 120,000 unique entries with 32 describing attributes. This data comes from a popular data science website, and previous analysis has occurred. This data set was featured as part of a small contest on the hosting website where the goal was to predict the likelihood of a guest making a reservation based on certain attributes while this study will instead attempt to predict the likelihood that a reservation is canceled. The 32 individual attributes of the data will be evaluated for their weight on the outcome of cancellation. Unlike previously mentioned studies, the attribute describing how the booking was made (in person, online) will be utilized. Researchers believe that the increase in reservations booked through online third-party platforms is contributing to the increase in cancellation &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Considering this attribute may have a significant impact on the overall accuracy of the developed predictive algorithm. Finally, the data set provides information on reservations over the span of a three-year period. During the training and testing phase, an appropriate amount of data will be used from each year to account for trends in cancellations that may have occurred over time.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Attribute&lt;/th>
&lt;th style="text-align:center">Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">hotel&lt;/td>
&lt;td style="text-align:center">hotel type (resort or city)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">is_canceled&lt;/td>
&lt;td style="text-align:center">reservation canceled (true/false)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">lead_time&lt;/td>
&lt;td style="text-align:center">number of days between booking and arrival&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">arrival_date_year&lt;/td>
&lt;td style="text-align:center">year of arrival date&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">arrival_date_month&lt;/td>
&lt;td style="text-align:center">month of arrival date&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">arrival_date_week_number&lt;/td>
&lt;td style="text-align:center">week number of year for arrival date&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">arrival_date_day_of_month&lt;/td>
&lt;td style="text-align:center">day of arrival date&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">stays_in_weekend_nights&lt;/td>
&lt;td style="text-align:center">number of weekend nights (Sat. and Sun.) booked&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">stays_in_week_nights&lt;/td>
&lt;td style="text-align:center">number of week nights (Mon. to Fri.) booked&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">adults&lt;/td>
&lt;td style="text-align:center">number of adults&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">children&lt;/td>
&lt;td style="text-align:center">number of children&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">babies&lt;/td>
&lt;td style="text-align:center">number of babies&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">meal&lt;/td>
&lt;td style="text-align:center">meal booked (multiple categories)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">country&lt;/td>
&lt;td style="text-align:center">country of origin&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">market_segment&lt;/td>
&lt;td style="text-align:center">market segment (multiple categories)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">distribution_channel&lt;/td>
&lt;td style="text-align:center">booking distribution channel (multiple categories)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">is_repeated_guest&lt;/td>
&lt;td style="text-align:center">is repeat guest (true/false)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">previous_cancellations&lt;/td>
&lt;td style="text-align:center">number of times customer has canceled previously&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">previous_bookings_not_canceled&lt;/td>
&lt;td style="text-align:center">number of times customer has completed a reservation&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">reserved_room_type&lt;/td>
&lt;td style="text-align:center">reserved room type&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">assigned_room_type&lt;/td>
&lt;td style="text-align:center">assigned room type&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">booking_changes&lt;/td>
&lt;td style="text-align:center">number of changes made to reservation from booking to arrival&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">deposit_type&lt;/td>
&lt;td style="text-align:center">deposit made (true/false)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">agent&lt;/td>
&lt;td style="text-align:center">ID of the travel agent&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">company&lt;/td>
&lt;td style="text-align:center">ID of the company&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">days_in_waiting_list&lt;/td>
&lt;td style="text-align:center">how many days customer took to confirm reservation&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">customer_type&lt;/td>
&lt;td style="text-align:center">type of customer (multiple categories)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">adr&lt;/td>
&lt;td style="text-align:center">average daily rate&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">required_car_parking_space&lt;/td>
&lt;td style="text-align:center">number of parking spaces required for reservation&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">total_of_special_requests&lt;/td>
&lt;td style="text-align:center">number of special requests made&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">reservation_status&lt;/td>
&lt;td style="text-align:center">status of reservation&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">reservation_status_date&lt;/td>
&lt;td style="text-align:center">date status was last updated&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="4-data-preprocessing">4. Data Preprocessing&lt;/h2>
&lt;p>The raw data set &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> is imported directly from Kaggle into a Google Colab notebook &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Data manipulation is handled using Pandas. Pandas was specifically written for data manipulation and analysis, and will make for a simple process preprocessing the data. The raw data set must be prepared before a model can be developed from it. Before preprocessing the data:&lt;/p>
&lt;ul>
&lt;li>shape: (119390, 32)&lt;/li>
&lt;li>duplicate entries: 31,994&lt;/li>
&lt;/ul>
&lt;p>The features of the data set, the categories other than what is being predicted, have varying levels of importance to the predictive model. By inspection, the following features are removed:&lt;/p>
&lt;ul>
&lt;li>country: in a format unsuitable for predictive model, unable to convert&lt;/li>
&lt;li>agent: the ID number of the booking agent will not affect reservation outcome&lt;/li>
&lt;li>babies: no reservation had babies&lt;/li>
&lt;li>children: no reservation had children&lt;/li>
&lt;li>company: the ID number of the booking company will not affect reservation outcome&lt;/li>
&lt;li>reservation_status_date: intermediate status of reservation is irrelevant&lt;/li>
&lt;/ul>
&lt;p>In addition, duplicates are removed. In the case of the features &amp;lsquo;reserved_room_type&amp;rsquo; and &amp;lsquo;assigned_room_type&amp;rsquo; what is of interest is if the guest was given the requested room type. As the room code system has been anonymized and is proprietary to the brand, it is impossible to make inferences other than this. To simplify the number of features, a Boolean comparison is performed on &amp;lsquo;reserved_room_type&amp;rsquo; and &amp;lsquo;assigned_room_type&amp;rsquo;. &amp;lsquo;reserved_room_type&amp;rsquo; and &amp;lsquo;assigned_room_type&amp;rsquo; are deleted while the Boolean results are converted to integer values then placed in a new feature category, &amp;lsquo;room_correct&amp;rsquo;. As it stands, multiple data entries across various features are strings. In order to build a predictive model, the data entries are converted to integers.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#8f5902;font-style:italic">#Convert to numerical values&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;City Hotel&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;HB&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Online TA&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;TA/TO&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;No Deposit&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#39;Transient&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Check-Out&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;0&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;Resort Hotel&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;January&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;BB&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Ofline TA/TO&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;GDS&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#39;Non Refund&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Transient-Party&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">Canceled&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;], &amp;#39;&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;February&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;SC&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Groups&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Refundable&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Group&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#39;No-Show&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;2&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;March&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;FB&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Direct&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Contract&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;3&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;April&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Undefined&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Corporate&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;4&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;May&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Complementary&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;5&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;June&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Aviation&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;6&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;July&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;7&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;August&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;8&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;September&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;9&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;October&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;10&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;November&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;11&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;December&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;12&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>After preprocessing the data:&lt;/p>
&lt;ul>
&lt;li>shape: (84938, 25)&lt;/li>
&lt;li>duplicate entries: 0&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/raw/main/project/images/figure1.png" alt="Data Snapshot">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Snapshot of Data Set after Preprocessing&lt;/p>
&lt;h2 id="5-model-creation">5. Model Creation&lt;/h2>
&lt;p>To form the predictive model a Random Forest Classifier will be used. With the use of the sklearn package, the Random Forest Classifier is simple to implement. The Random Forest Classifier is based on the concept of a decision tree. A decision tree is a series of yes/no question asked about the data which eventually leads to a predicted class or value &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. To start the creation of the model, the data must first be split into a training and testing set. Typically, this is a ratio that must be adjusted to determine which will result in the higher accuracy. Here are the accuracy outcomes for various ratios:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Train/Test Ratio&lt;/th>
&lt;th style="text-align:center">Accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">80/20&lt;/td>
&lt;td style="text-align:center">77.64%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">70/30&lt;/td>
&lt;td style="text-align:center">77.66%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">60/40&lt;/td>
&lt;td style="text-align:center">79.56%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">50/50&lt;/td>
&lt;td style="text-align:center">77.92%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">40/60&lt;/td>
&lt;td style="text-align:center">75.73%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">30/70&lt;/td>
&lt;td style="text-align:center">74.71%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">20/80&lt;/td>
&lt;td style="text-align:center">73.13%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The train/test ratio of 60/40 had the best initial accuracy of 79.56% so this ratio will be used in the creation of the final model. For the initial test, all remaining features will be used to train the reservation cancellation outcome. To determine the accuracy of the resulting model, the number of predicted cancellations is compared to the number of actual cancellations. As the model stands, the accuracy is at 79.56%.
This model relies on 23 features for the prediction. With this many features it is possible that some features have no effect on the cancellation outcome. It is also possible that some features are so closely related that calculating each individually hinders performance while having little effect on outcome. To evaluate the importance of features, Pearson&amp;rsquo;s correlation coefficent can be used. Correlation coefficients are used in statistics to measure how strong the relationship is between two variables. The calculation formula returns a value between -1 and 1 where a value of 1 indicates a strong positive relationship, -1 indicates a strong negative relationship, and 0 indicates that there is no relationship at all &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Figure 2 shows the correlation between the remaining features.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/raw/main/project/images/figure2.png" alt="Initial Model Correlation">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Pearson&amp;rsquo;s Correlation Graph of Remaining Features&lt;/p>
&lt;p>In Figure 2 it is straightforward to identify the correlation between the target variable &amp;lsquo;is_canceled&amp;rsquo; and the remaining features. It does not appear than any variable has a strong positive or negative correlation, returning a value close to positive or negative 1. There does however appear to be a dominant correlation between &amp;lsquo;is_canceled&amp;rsquo; and three features: &amp;lsquo;lead_time&amp;rsquo;, &amp;lsquo;adr&amp;rsquo;, and &amp;lsquo;room_correct&amp;rsquo;. The train/test ratio is again 60/40 and the baseline accuracy of the model is 79.56%. The remaining features &amp;lsquo;lead_time&amp;rsquo;, &amp;lsquo;adr&amp;rsquo;, and &amp;lsquo;room_correct&amp;rsquo; are used to develop the new model. Accuracy is again determined by comparing the number of predicted cancellations to the number of actual cancellations. The updated model has an accuracy of 85.18%. Figure 3 shows the correlation between the remaining features. It is important to note that the relationship between the remaining features and target value does not appear to be strong, however there is a correlation nonetheless.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/raw/main/project/images/figure3.png" alt="Updated Model Correlation">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Pearson&amp;rsquo;s Correlation Graph of Updated Remaining Features&lt;/p>
&lt;p>As a final visualization, Figure 4 shows a comparison between the predicted and actual cancellation instances. The graph reveals an interesting pattern, the model is over predicting early in the data set and under predicting as it proceeds through the data set. Further inspection and manipulation of the Random Forest parameters were unable to eliminate this pattern.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/raw/main/project/images/figure4.png" alt="Results Predicted vs. Actual">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Model Results Predicted vs. Actual&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>To measure program performance in the Google Colab notebook, Cloudmesh Common &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup> was used to create a benchmark. In this instance, performance was measured for overall code execution, data loading, preparation of the data, the creation of model one, and the creation of model two. The most important increase in performance was between the creation of models one and two. With 23 features, model one took 8.161 seconds to train while model 2, with 3 features, took 7.01 seconds to train. By reducing the number of features between the two models there is a 5.62% increase in accuracy and a 14.10% decrease in processing time. Figure 5 provides more insight into the parameters the benchmark tracked and returned. Additionally, the table provides an analysis of computation time:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Train/Test Ratio&lt;/th>
&lt;th style="text-align:center">Accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">80/20&lt;/td>
&lt;td style="text-align:center">77.64%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">70/30&lt;/td>
&lt;td style="text-align:center">77.66%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">60/40&lt;/td>
&lt;td style="text-align:center">79.56%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">50/50&lt;/td>
&lt;td style="text-align:center">77.92%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">40/60&lt;/td>
&lt;td style="text-align:center">75.73%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">30/70&lt;/td>
&lt;td style="text-align:center">74.71%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">20/80&lt;/td>
&lt;td style="text-align:center">73.13%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/raw/main/project/images/figure5.png" alt="Benchmark Results">&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Cloudmesh Benchmark Results&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>From the results of the updated predictive model, it is apparent that the lodging industry should invest focus into the Big Data they keep on their customers. As each hotel chain is unique, it would be necessary for each to develop their own predictive model however it has been demonstrated that such a model would be effective in reducing the number of rooms going unoccupied from reservation cancellation. As the model is predicting at 85% accuracy, this is a 35% increase in the amount of reservation cancellations that can be accounted for over the current predictive techniques. To prevent further damage from reservation cancellations the hotel would have theoretically been able to overbook room reservations by 35% or less as they anticipated cancellations.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&amp;ldquo;Big Data - Definition, Importance, Examples &amp;amp; Tools&amp;rdquo;, RDA, 2020. [Online]. Available: &lt;a href="https://www.rd-alliance.org/group/big-data-ig-data-development-ig/wiki/big-data-definition-importance-examples-tools#:~:text=Big%20data%20is%20a%20term,day%2Dto%2Dday%20basis.&amp;amp;text=It's%20what%20organizations%20do%20with,decisions%20and%20strategic%20business%20moves">https://www.rd-alliance.org/group/big-data-ig-data-development-ig/wiki/big-data-definition-importance-examples-tools#:~:text=Big%20data%20is%20a%20term,day%2Dto%2Dday%20basis.&amp;amp;text=It's%20what%20organizations%20do%20with,decisions%20and%20strategic%20business%20moves&lt;/a>. [Accessed: 12- Nov- 2020].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>&amp;ldquo;Predicting Hotel Booking Cancellations Using Machine Learning - Step by Step Guide with Real Data and Python&amp;rdquo;, Linkedin.com, 2020. [Online]. Available: &lt;a href="https://www.linkedin.com/pulse/u-hotel-booking-cancellations-using-machine-learning-manuel-banza/">https://www.linkedin.com/pulse/u-hotel-booking-cancellations-using-machine-learning-manuel-banza/&lt;/a>. [Accessed: 08- Nov- 2020].&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>&amp;ldquo;(PDF) Predicting Hotel Booking Cancellation to Decrease Uncertainty and Increase Revenue&amp;rdquo;, ResearchGate, 2020. [Online]. Available: &lt;a href="https://www.researchgate.net/publication/310504011_Predicting_Hotel_Booking_Cancellation_to_Decrease_Uncertainty_and_Increase_Revenue">https://www.researchgate.net/publication/310504011_Predicting_Hotel_Booking_Cancellation_to_Decrease_Uncertainty_and_Increase_Revenue&lt;/a>. [Accessed: 08- Nov- 2020.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>&amp;ldquo;Predicting Hotel Cancellations with Machine Learning&amp;rdquo;, Medium, 2020. [Online]. Available: &lt;a href="https://towardsdatascience.com/predicting-hotel-cancellations-with-machine-learning-fa669f93e794">https://towardsdatascience.com/predicting-hotel-cancellations-with-machine-learning-fa669f93e794&lt;/a>. [Accessed: 08- Nov- 2020].&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>&amp;ldquo;Hotel booking demand&amp;rdquo;, Kaggle.com, 2020. [Online]. Available: &lt;a href="https://www.kaggle.com/jessemostipak/hotel-booking-demand">https://www.kaggle.com/jessemostipak/hotel-booking-demand&lt;/a>. [Accessed: 08- Nov- 2020].&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>&amp;ldquo;Global Cancellation Rate of Hotel Reservations Reaches 40% on Average&amp;rdquo;, Hospitality Technology, 2020. [Online]. Available: &lt;a href="https://hospitalitytech.com/global-cancellation-rate-hotel-reservations-reaches-40-average">https://hospitalitytech.com/global-cancellation-rate-hotel-reservations-reaches-40-average&lt;/a>. [Accessed: 08- Nov- 2020].&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-323/blob/main/project/colabnotebook/DataAnalysis.ipynb">https://github.com/cybertraining-dsc/fa20-523-323/blob/main/project/colabnotebook/DataAnalysis.ipynb&lt;/a>.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>&amp;ldquo;An Implementation and Explanation of the Random Forest in Python&amp;rdquo;, Medium, 2020. [Online]. Available: &lt;a href="https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76">https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76&lt;/a>. [Accessed: 12- Nov- 2020].&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>&amp;ldquo;Correlation Coefficient: Simple Definition, Formula, Easy Calculation Steps&amp;rdquo;, Statistics How To, 2020. [Online]. Available: &lt;a href="https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/">https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/&lt;/a>. [Accessed: 12- Nov- 2020].&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analysis of Future of Buffalo Breeds and Milk Production Growth in India</title><link>/report/fa20-523-326/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-326/project/project/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-326/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-326/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Gangaprasad Shahapurkar, fa20-523-326, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-326/blob/main/project/project.md">Edit&lt;/a> &lt;a href="https://github.com/cybertraining-dsc/fa20-523-326/blob/main/project/code/project.ipynb">Python Notebook&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Water buffalo (&lt;strong>Bubalus bubalis&lt;/strong>) is also called &lt;em>Domestic Water Buffalo&lt;/em> or &lt;em>Asian Water Buffalo&lt;/em>. It is large bovid originating in Indian subcontinent, Southeast Asia, and China and today found in other regions of world - Europe, Australia, North America, South America and some African countries. There are two extant types recognized based on morphological and behavioral criteria:&lt;/p>
&lt;ol>
&lt;li>River Buffalo - Mostly found in Indian subcontinent and further west to the Balkans, Egypt, and Italy&lt;/li>
&lt;li>Swamp Buffalo - Found from west of Assam through Southeast Asia to the Yangtze valley of China in the east&lt;/li>
&lt;/ol>
&lt;p>India is the largest milk producer and consumer compared to other countries in the world and stands unique in terms of the largest share of milk being produced coming from buffaloes. The aim of this academic project is to study the livestock census data of buffalo breeds in India and their milk production using Empirical Benchmarking analysis method at state level. Looking at the small sample of data, our analysis indicates that we have been seeing increasing trends in past few years in livestock and milk production but there are considerable opportunities to increase production using combined interventions.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-datasets">3. Choice of Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-software-components">4.1 Software Components&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-data-processing">4.2 Data Processing&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#421-eda">4.2.1 EDA&lt;/a>&lt;/li>
&lt;li>&lt;a href="#422-feature-engineering">4.2.2 Feature Engineering&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#43-modelling">4.3 Modelling&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#431-data-preperation">4.3.1 Data Preperation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#432-empirical-benchmarking-model">4.3.2 Empirical Benchmarking Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#433-linear-regression">4.3.3 Linear Regression&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-results">5. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> hid 326, i532, buffalo, milk production, livestock, benchmarking, in-milk yield, agriculture, india, analysis&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Indian agriculture sector has been playing a vital role in overall contribution to Indian economy. Most of the rural community in the nation still make their livelihood on dairy farming or agriculture farming. Dairy farming itself has been on its progressive stage from past few years and it is contributing to almost more than 25% of agriculture Gross Domestic Product (GDP) &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Livestock rearing has been integral part of the nation&amp;rsquo;s rural community and this sector is leveraging the economy in a big way considering the growth seen. It not only provides food, income, employment but also plays major role in life of farmers. It also does other contributions to the overall rural development of the nation. The output of livestock rearing such as milk, egg, meat, and wool provides everyday income to the farmers on daily basis, it provides nutrition to consumers and indirectly it helps in contributing to the overall economy and socio-economic development of the country.&lt;/p>
&lt;p>The world buffalo population is estimated at 185.29 million, spread in some 42 countries, of which 179.75 million (97%) are in Asia (Source: Fao.org/stat 2008). Hence major share of buffalo milk production in world comes from Asia (see Figure 1). India has 105.1 million and they comprise approximately 56.7 percent of the total world buffalo population. During the last 10 years, the world buffalo population increased by approximately 1.49% annually, by 1.53% in India, 1.45% in Asia and 2.67% in the rest of the world. Figure 1 shows worldwide share of milk production from buffalo breed. Figure 2 highlights percentage contribution of Asia including other top 2 contributors to world milk production.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/milk_production_world.png" alt="Milk Production World">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Production of Milk, whole fresh buffalo in World + (Total), Average 2013 - 2018 &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/milk_production_by_region.png" alt="Milk Production Share">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Production share of Milk, whole fresh buffalo by region, Average 2013 - 2018 &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>Production of milk and meat from buffaloes in Asian countries over the last decades has shown a varying pattern: in countries such as India, Sri Lanka, Pakistan and China. Buffaloes are known to be better at converting poor-quality roughage into milk and meat. They are reported to have 5% higher digestibility of crude fibre than high-yielding cows; and a 4% to 5% higher efficiency of utilization of metabolic energy for milk production &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>, &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>After studying literatures and researches it was noticed that there has been some research around to quantify livestock yield gaps. There is no standard methodology, but multiple methods were combined for research. Researchers were able to calculate relative yield gaps for the dairy production in India and Ethiopia &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. There was analysis based on attainable yields using Empirical Benchmarking, and Stochastic Frontier Analysis to evaluate possible interventions for increasing production (household modelling). It was noticed that large yield gaps exist for dairy production in both countries, and packages of interventions are required to bridge these gaps rather than single interventions. Part of the research was borrowed to analyze the limited dataset chosen as part of this project.&lt;/p>
&lt;h2 id="3-choice-of-datasets">3. Choice of Datasets&lt;/h2>
&lt;p>Number of online literatures and datasets were checked to find out suitable dataset required for this project analysis. Below dataset were found promising:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://dahd.nic.in/about-us/divisions/statistics">DAHD Data&lt;/a> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>, &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>, &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> (&lt;strong>20th India Livestock Census Data&lt;/strong>)&lt;/li>
&lt;/ol>
&lt;p>The Animal Husbandry Statistics Division of the Department of Animal Husbandry &amp;amp; Dairying Division (DAHD) is responsible for the generation of animal husbandry statistics through the schemes of livestock census and integrated sample surveys &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Survey is defined by Indian Agriculture Statistics Research Institute (IASRI) &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. This is the only scheme through which considerable data, particularly on the production estimate of major livestock products, is being generated for policy formulation in the livestock sector. It is mandate for this division to&lt;/p>
&lt;ul>
&lt;li>Conduct quinquennial livestock census&lt;/li>
&lt;li>Conduct annual sample survey through integrated sample survey&lt;/li>
&lt;li>Publish annual production estimates of milk, eggs, meat, wool and other related animal husbandry statistics based on survey&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>&lt;a href="http://www.fao.org/faostat/en/#data">FAO Data&lt;/a> &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>, &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>Food and Agriculture Organization (FAO) of United Nation publishes worldwide data on the aspects of dairy farming which can also be visualized online with the options provided. Some of the data from this source was used to extract useful summary needed in analysis.&lt;/p>
&lt;ol start="3">
&lt;li>[UIDAI Data] (&lt;a href="https://uidai.gov.in">https://uidai.gov.in&lt;/a>) &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>Unique Identification Authority of India (UIDAI) was created with the objective to issue Unique Identification numbers (UID), named as &lt;strong>Aadhaar&lt;/strong>, to all residents of India. Projected population data of 2020 was extracted from this source.&lt;/p>
&lt;p>In addition to above, other demographics information such as area of each state, district count was extracted from OpenStreetMap &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. Agricultural zone information was extracted from report of Food and Nutrition Security Analysis, India, 2019 &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;h3 id="41-software-components">4.1 Software Components&lt;/h3>
&lt;p>This project has been implemented in Python 3.7 version. Jupyter Notebook application was used to develop the code and produce a notebook document. Jupyter notebook is a Client-Server architecture-based application which allows modification and execution of code through web browser. Jupyter notebook can be installed locally and accessed through localhost browser or it can be installed on a remote machine and accessed via internet &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>, &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Following python libraries were used in overall code development. Before running the code, one must make sure that these libraries are installed.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Pandas&lt;/strong> This is a high performance and easy to use library. It was used for data cleaning, data analysis, &amp;amp; data preparation.&lt;/li>
&lt;li>&lt;strong>NumPy&lt;/strong> NumPy is python core library used for scientific computing. Some of the basic functions were used in this project.&lt;/li>
&lt;li>&lt;strong>Matplotlib&lt;/strong> This is a comprehensive library used for static, animated and interactive visualization.&lt;/li>
&lt;li>&lt;strong>OS&lt;/strong> This is another standard library of Python which provides miscellaneous operating system interface functions.&lt;/li>
&lt;li>&lt;strong>Scikit-learn (Sklearn)&lt;/strong> Robust library that provides efficient tools for machine learning and statistical modelling.&lt;/li>
&lt;li>&lt;strong>Seaborn&lt;/strong> Python data visualization library based on matplotlib.&lt;/li>
&lt;/ul>
&lt;h3 id="42-data-processing">4.2 Data Processing&lt;/h3>
&lt;p>The raw data retrieved from various sources was in excel or report format. The data was pre-processed and stored back in csv format for the purpose of this project and to easily process it. This dataset was further processed through various stages via EDA, feature engineering and modelling.&lt;/p>
&lt;h4 id="421-eda">4.2.1 EDA&lt;/h4>
&lt;p>Preprocessed dataset selected for this analysis contained information at the state level. There were two seperate pre-processed dataset used. Below was the nature of the attributes in the main dataset which had buffalo information:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>State Name:&lt;/strong> Name of each state in India. One record for each state.&lt;/li>
&lt;li>&lt;strong>Buffalo count:&lt;/strong> Total number of male and female buffaloes. Data recorded for 14 types of buffalo breeds. One attribute for each type of female and male breeds.&lt;/li>
&lt;li>&lt;strong>In Milk animals:&lt;/strong> Number of In-Milk animals per state (figures in 000 no&amp;rsquo;s) recorded each year from 2013 to 2019. One attribute per year&lt;/li>
&lt;li>&lt;strong>Yield per In-Milk animal:&lt;/strong> Yield per In-Milk animals per state (figures in kg/day) recorded each year from 2013 to 2019. One attribute per year.&lt;/li>
&lt;li>&lt;strong>Milk production:&lt;/strong> - Milk production per state (figures in 000 tones) recorded each year from 2013 to 2019. One attribute per year.&lt;/li>
&lt;/ul>
&lt;p>Below was the nature of the attributes in the secondary dataset which had demographic information:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Geographic information:&lt;/strong> features captured at state level where each feature represented - projected population of 2020, total districts in each state, total villages in each state, official area of each state in square kilometer.&lt;/li>
&lt;li>&lt;strong>Climatic information:&lt;/strong> One of attribute for each zone highlighted in Table 1&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Table 1:&lt;/strong> Agro climatic regions in India&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Zones&lt;/th>
&lt;th>Agro-Climatic Regions&lt;/th>
&lt;th>States&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Zone 1&lt;/td>
&lt;td>Western Himalayan Region&lt;/td>
&lt;td>Jammu and Kashmir, Himachal Pradesh, Uttarakhand&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 2&lt;/td>
&lt;td>Eastern Himalayan Region&lt;/td>
&lt;td>Assam, Sikkim, West Bengal, Manipur, Mizoram, Andhra Pradesh, Meghalaya, Tripura&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 3&lt;/td>
&lt;td>Lower Gangetic Plains Region&lt;/td>
&lt;td>West Bengal&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 4&lt;/td>
&lt;td>Middle Gangetic Plains Region&lt;/td>
&lt;td>Uttar Pradesh, Bihar, Jharkhand&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 5&lt;/td>
&lt;td>Upper Gangetic Plains Region&lt;/td>
&lt;td>Uttar Pradesh&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 6&lt;/td>
&lt;td>Trans-Gangetic Plains Region&lt;/td>
&lt;td>Punjab, Haryana, Delhi and Rajasthan&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 7&lt;/td>
&lt;td>Eastern Plateau and Hills Region&lt;/td>
&lt;td>Maharashtra, Chhattisgarh, Jharkhand, Orissa and West Bengal&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 8&lt;/td>
&lt;td>Central Plateau and Hills Region&lt;/td>
&lt;td>Madhya Pradesh, Rajasthan, Uttar Pradesh, Chhattisgarh&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 9&lt;/td>
&lt;td>Western Plateau and Hills Region&lt;/td>
&lt;td>Maharashtra, Madhya Pradesh, Chhattisgarh and Rajasthan&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 10&lt;/td>
&lt;td>Southern Plateau and Hills Region&lt;/td>
&lt;td>Andhra Pradesh, Karnataka, Tamil Nadu, Telangana, Chhattisgarh&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 11&lt;/td>
&lt;td>East Coast Plains and Hills Region&lt;/td>
&lt;td>Orissa, Andhra Pradesh, Tamil Nadu and Pondicherry&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 12&lt;/td>
&lt;td>West Coast Plains and Ghat Region&lt;/td>
&lt;td>Tamil Nadu, Kerala, Goa, Karnataka, Maharashtra, Gujarat&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 13&lt;/td>
&lt;td>Gujarat Plains and Hills Region&lt;/td>
&lt;td>Gujarat, Madhya Pradesh, Rajasthan, Maharashtra&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 14&lt;/td>
&lt;td>Western Dry Region&lt;/td>
&lt;td>Rajasthan&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 15&lt;/td>
&lt;td>The Islands Region&lt;/td>
&lt;td>Andaman and Nicobar, Lakshadweep&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Figure 3 shows top 10 states from livestock census having total number of buffalo counts. Uttar Pradesh was the state which reported a greater number of buffaloes compared to any other states in the country.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/top10states.png" alt="Milk Production Share">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Top 10 state by buffalo counts&lt;/p>
&lt;p>There were greater number of female buffaloes reported in country (80%) compared to male buffalo breeds.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/buffaloshare.png" alt="Milk Production Share">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Buffalo breeds ratio by male and female&lt;/p>
&lt;h4 id="422-feature-engineering">4.2.2 Feature Engineering&lt;/h4>
&lt;p>Murrah buffalo shown in Figure 5 is the most productive and globally famous breed &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>, &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>. This breed is resistant to diseases and can adjust to various Indian climate conditions.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/murrah_buffalo.jpeg" alt="Murrah buffalo">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Murrah buffalo (Bubalus bubalis), globally famous local breed of Haryana, were exported to many nations &lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>, &lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup>&lt;/p>
&lt;p>In feature engineering multiple attributes were derived needed for modelling or during analysis. Table 2 shows percentage share of Murrah buffalo breed in top 10 states having highest number of total buffaloes. Though Uttar Pradesh was top state in India in terms of total number of buffaloes but percentage share of Murrah buffalo was more in state of Punjab.&lt;/p>
&lt;p>&lt;strong>Table 2:&lt;/strong> Murrah buffalo percent share in top 10 state with buffalo count&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>State Name&lt;/th>
&lt;th>Murrah Buffalo Count&lt;/th>
&lt;th>Total Buffalo Count&lt;/th>
&lt;th>% Murrah Breed&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>UTTAR PRADESH&lt;/td>
&lt;td>20110852&lt;/td>
&lt;td>30625334&lt;/td>
&lt;td>65.67&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RAJASTHAN&lt;/td>
&lt;td>6448563&lt;/td>
&lt;td>12976095&lt;/td>
&lt;td>49.70&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ANDHRA PRADESH&lt;/td>
&lt;td>5227270&lt;/td>
&lt;td>10622790&lt;/td>
&lt;td>49.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HARYANA&lt;/td>
&lt;td>5011145&lt;/td>
&lt;td>6085312&lt;/td>
&lt;td>82.35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PUNJAB&lt;/td>
&lt;td>4116508&lt;/td>
&lt;td>5159734&lt;/td>
&lt;td>79.78&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BIHAR&lt;/td>
&lt;td>2419952&lt;/td>
&lt;td>7567233&lt;/td>
&lt;td>31.98&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MADHYA PRADESH&lt;/td>
&lt;td>1446078&lt;/td>
&lt;td>8187989&lt;/td>
&lt;td>17.66&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MAHARASHTRA&lt;/td>
&lt;td>986981&lt;/td>
&lt;td>5594392&lt;/td>
&lt;td>17.64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TAMIL NADU&lt;/td>
&lt;td>435634&lt;/td>
&lt;td>780431&lt;/td>
&lt;td>55.82&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UTTARAKHAND&lt;/td>
&lt;td>378917&lt;/td>
&lt;td>987775&lt;/td>
&lt;td>38.36&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Survey dataset had three primary attributes reported at the state level. Data reported from 2013 to 2019 for in-milk animals, yield per in-milk animals and milk production per state were averaged for analysis purpose. Total number of buffaloes per breed type were calculated from the data provided in the dataset. Following list of breeds were identified from dataset &lt;sup id="fnref:21">&lt;a href="#fn:21" class="footnote-ref" role="doc-noteref">21&lt;/a>&lt;/sup>.&lt;/p>
&lt;ul>
&lt;li>Banni&lt;/li>
&lt;li>Bhadawari&lt;/li>
&lt;li>Chilika&lt;/li>
&lt;li>Jaffarabadi&lt;/li>
&lt;li>Kalahandi&lt;/li>
&lt;li>Marathwadi&lt;/li>
&lt;li>Mehsana&lt;/li>
&lt;li>Murrah&lt;/li>
&lt;li>Nagpuri&lt;/li>
&lt;li>Nili Ravi&lt;/li>
&lt;li>Non-Descript&lt;/li>
&lt;li>Pandharpuri&lt;/li>
&lt;li>Surti&lt;/li>
&lt;li>Toda&lt;/li>
&lt;/ul>
&lt;p>Data showed that Uttar Pradesh had highest average milk production with in the top 10 states whereas Punjab state had highest average yield per in-milk animals. Figure 6 shows the share of top 3 breeds in both the state. Common attribute seen between two states was highest number of Murrah breed buffaloes compared to other breeds.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/punjab_up_state.png" alt="TOP TWO States">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Top 3 types of buffalo breeds of Uttar Pradesh and Punjab&lt;/p>
&lt;h3 id="43-modelling">4.3 Modelling&lt;/h3>
&lt;h4 id="431-data-preperation">4.3.1 Data Preperation&lt;/h4>
&lt;p>Data from main dataset and supplementary dataset which was demographics data was merged into one dataset. This dataset was not labelled and it was small dataset. We considered average milk production as our target label for analysis. The rest of the features were divided based on categorical and numerical nature. Our dataset did not had any categorical features except state name which was used as index column so no futher processing was considered for this attribute. All the features were of numerical nature and all the data points were not on same scale. Hence datapoints were normalized for further processing.&lt;/p>
&lt;h4 id="432-empirical-benchmarking-model">4.3.2 Empirical Benchmarking Model&lt;/h4>
&lt;p>There are two dominant approach of economic modelling to estimate the production behavior - Empirical Benchmarking and Stochastic Frontier Analysis &lt;sup id="fnref:22">&lt;a href="#fn:22" class="footnote-ref" role="doc-noteref">22&lt;/a>&lt;/sup>, &lt;sup id="fnref:23">&lt;a href="#fn:23" class="footnote-ref" role="doc-noteref">23&lt;/a>&lt;/sup>. Empirical Benchmarking is simple modelling method, and it is one of the two dominant approach. This method was used to analyze past 6 years of data points available in the livestock dataset. In this approach milk production data of past 6 years was averaged. Top 10 states with most milk production reported were compared with average of the whole sample. The problem analyzed as part of this project was relatively small. The comparison did not consider all possible characteristics for modelling.&lt;/p>
&lt;p>Correlation of target variable with various demographics features available in the dataset was calculated. Table 3 and Table 4 shows the positive and negative coorelation with target variable average milk production respectively. We noticed from Table 3 that average milk production contribution was getting affected by number of in-milk animals reported in the particular year census data and their was also factor of climatic conditions affecting the milk production. India is divided into 15 Agro climate zones. Agro zone 6 is Trans-Gangetic Plains region. Indian states Chandigarh, Delhi, Haryana, Punjab, Rajasthan (some parts) falls under this zone. Agricultural development has shown phenomenal growth in overall productivity and in providing better environment for dairy farming in this zone &lt;sup id="fnref:24">&lt;a href="#fn:24" class="footnote-ref" role="doc-noteref">24&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Table 3:&lt;/strong> Positive coorelation of target with demographics features&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>%&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>avg_milk_production&lt;/td>
&lt;td>1.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>avg_in_milk&lt;/td>
&lt;td>0.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>total_female&lt;/td>
&lt;td>0.90&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>total_male&lt;/td>
&lt;td>0.71&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>agro_climatic_zone6&lt;/td>
&lt;td>0.50&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4:&lt;/strong> Negative coorelation of target with demographics features&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>%&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>official_area_sqkm&lt;/td>
&lt;td>-0.17&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>agro_climatic_zone2&lt;/td>
&lt;td>-0.19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>avg_yield_in_milk&lt;/td>
&lt;td>-0.23&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>district_count&lt;/td>
&lt;td>-0.34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>proj_population_2020&lt;/td>
&lt;td>-0.94&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="433-linear-regression">4.3.3 Linear Regression&lt;/h4>
&lt;p>Our target variable considered was a continous variable. In an attempt to perform dimension reduction, Principal Component Analysis (PCA) was applied to available limited dataset. In the example presented, an pipeline was constructed that had dimension reduction followed by Linear regression classifier. Grid search cross validation was applied (GridSearchCV) to find the best parameters and score. Linear regression was applied with default parameter settings whereas parameter range was passed for PCA. Below is snapshot of pipeline implemented.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#8f5902;font-style:italic"># Define a pipeline to search for the best combination of PCA truncation&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># and classifier regularization.&lt;/span>
&lt;span style="color:#000">pca&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">PCA&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># Linear Regression without parameters&lt;/span>
&lt;span style="color:#000">linear&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">LinearRegression&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;span style="color:#000">full_pipeline_with_predictor&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">Pipeline&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>
&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;preparation&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">num_pipeline&lt;/span>&lt;span style="color:#000;font-weight:bold">),&lt;/span>
&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;pca&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>&lt;span style="color:#000">pca&lt;/span>&lt;span style="color:#000;font-weight:bold">),&lt;/span>
&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;linear&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">linear&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000;font-weight:bold">])&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># Parameters of pipelines:&lt;/span>
&lt;span style="color:#000">param_grid&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#4e9a06">&amp;#39;pca__n_components&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">5&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">15&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">30&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">45&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">64&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="5-results">5. Results&lt;/h2>
&lt;p>Based on simple Empirical Benchmarking Analysis and trends noticed in data it appears that it is possible to increase production past currently attainable yields (see Figure 7). The current scale of the yield does indicate that, leading states have best breeds of buffaloes. Different methods of analyzing yield gaps can be combined to give estimates of attainable yields. It will also help to evaluate possible interventions to increase production and profits.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/avgmilkproduction.png" alt="TOP 10 States">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Average milk production in top 10 state with benchmark&lt;/p>
&lt;p>The best parameter came out of the pipeline implemented are highlighted here. The results were not promising due the limited dataset so further experimentation was not attempted, but one thing was noticed that cross validated score came out to 0.28 and dimension reduction to 5 components.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#000">Best&lt;/span> &lt;span style="color:#000">parameter&lt;/span> &lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">CV&lt;/span> &lt;span style="color:#000">score&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.282&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;pca__n_components&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">5&lt;/span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;span style="color:#000">PCA&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">copy&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#3465a4">True&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">iterated_power&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;auto&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">n_components&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#3465a4">None&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">random_state&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#3465a4">None&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#000">svd_solver&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;auto&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">tol&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.0&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">whiten&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#3465a4">False&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/covariance.png" alt="COV Analysis">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Covariance Heat Map&lt;/p>
&lt;p>We were able to calculate correlation of census data with other socioeconomic factors like population information, climate information (see Figure 8). The biggest probable limitation here was availability of good quality data. It would have been possible to conduct the analysis at finer level if more granular level data would have been available. Our analysis had to be done state level rather than at district level or specific area.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>The analysis done above with the limited dataset showed that there are considerable gaps in the average yield per in-milk buffalo of state Punjab and Uttar Pradesh, compared to other states in top 10 list. These states have larger share of Murrah breed buffaloes. Based on the data trends it appears that it is possible to increase the production past current attenable numbers. However, this would need to combine different methods and multiple strategies.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>PIB Delhi. (2019). Department of Animal Husbandry &amp;amp; Dairying releases 20th Livestock Census, 16 (Oct 2019).
&lt;a href="https://pib.gov.in/PressReleasePage.aspx?PRID=1588304">https://pib.gov.in/PressReleasePage.aspx?PRID=1588304&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>FAO - Food and Agriculture Organization of United Nation, Accessed: Nov. 2020, &lt;a href="http://www.fao.org/faostat/en/#data">http://www.fao.org/faostat/en/#data&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Mudgal, V.D. (1988). &amp;ldquo;Proc. of the Second World Buffalo Congress, New Delhi, India&amp;rdquo;, 12 to 17 Dec.:454.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Alessandro, Nardone. (2010). &amp;ldquo;Buffalo Production and Research&amp;rdquo;. Italian Journal of Animal Science. 5. 10.4081/ijas.2006.203.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Mayberry, Dianne (07/2017). Yield gap analyses to estimate attainable bovine milk yields and evaluate options to increase production in Ethiopia and India. Agricultural systems (0308-521X), 155 , p. 43.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Department of Animal Husbandry and Dairying. &lt;a href="http://dahd.nic.in/about-us/divisions/statistics">http://dahd.nic.in/about-us/divisions/statistics&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Department of Animal Husbandry and Dairying, Accessed: Oct. 2020, &lt;a href="http://dadf.gov.in/sites/default/filess/20th%20Livestock%20census-2019%20All%20India%20Report.pdf">http://dadf.gov.in/sites/default/filess/20th%20Livestock%20census-2019%20All%20India%20Report.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Department of Animal Husbandry and Dairying, Accessed: Oct. 2020, &lt;a href="http://dadf.gov.in/sites/default/filess/Village%20and%20Ward%20Level%20Data%20%5BMale%20%26%20Female%5D.xlsx">http://dadf.gov.in/sites/default/filess/Village%20and%20Ward%20Level%20Data%20%5BMale%20%26%20Female%5D.xlsx&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Department of Animal Husbandry and Dairying, Accessed: Oct. 2020, &lt;a href="http://dadf.gov.in/sites/default/filess/District-wise%20buffalo%20population%202019_0.pdf">http://dadf.gov.in/sites/default/filess/District-wise%20buffalo%20population%202019_0.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>IASRI - Indian Agriculture Statistics Research Institute. &lt;a href="https://iasri.icar.gov.in/">https://iasri.icar.gov.in/&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>F.A.O. (2008). Food and Agriculture Organization. Rome Italy. STAT &lt;a href="http://database.www.fao.org">http://database.www.fao.org&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Unique Identification Authority of India, Accessed: Nov. 2020, &lt;a href="https://uidai.gov.in/images/state-wise-aadhaar-saturation.pdf">https://uidai.gov.in/images/state-wise-aadhaar-saturation.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>OpenStreetMap, Accessed: Nov. 2020, &lt;a href="https://wiki.openstreetmap.org/wiki/Main_Page">https://wiki.openstreetmap.org/wiki/Main_Page&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Food and Nutrition Security Analysis, India, 2019, Accessed: Nov. 2020, &lt;a href="http://mospi.nic.in/sites/default/files/publication_reports/document%281%29.pdf">http://mospi.nic.in/sites/default/files/publication_reports/document%281%29.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>Corey Schafer. Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough. (Sep. 22, 2016). Accessed: Nov. 07, 2020. [Online Video]. Available: &lt;a href="https://www.youtube.com/watch?v=HW29067qVWk">https://www.youtube.com/watch?v=HW29067qVWk&lt;/a>&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16" role="doc-endnote">
&lt;p>&lt;em>The Jupyter Notebook&lt;/em>. Jupyter Team. Accessed: Nov. 07, 2020. [Online]. Available: &lt;a href="https://jupyter-notebook.readthedocs.io/en/stable/notebook.html">https://jupyter-notebook.readthedocs.io/en/stable/notebook.html&lt;/a>&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17" role="doc-endnote">
&lt;p>Bharathi Dairy Farm. &lt;a href="http://www.bharathidairyfarm.com/about-murrah.php">http://www.bharathidairyfarm.com/about-murrah.php&lt;/a>&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18" role="doc-endnote">
&lt;p>Water Buffalo. Accessed: Oct 26, 2020. [Online]. Available: &lt;a href="https://en.wikipedia.org/wiki/Water_buffalo">https://en.wikipedia.org/wiki/Water_buffalo&lt;/a>&amp;#160;&lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19" role="doc-endnote">
&lt;p>Kleomarlo. Own work, CC BY-SA 3.0. [Online]. Available: &lt;a href="https://commons.wikimedia.org/w/index.php?curid=4349862">https://commons.wikimedia.org/w/index.php?curid=4349862&lt;/a>&amp;#160;&lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20" role="doc-endnote">
&lt;p>ICAR - Central Institute for Research on Buffaloes. &lt;a href="https://cirb.res.in/">https://cirb.res.in/&lt;/a>&amp;#160;&lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:21" role="doc-endnote">
&lt;p>List of Water Buffalo Breeds. Accessed: Oct 26, 2020. [Online]. Available: &lt;a href="https://en.wikipedia.org/wiki/List_of_water_buffalo_breeds">https://en.wikipedia.org/wiki/List_of_water_buffalo_breeds&lt;/a>&amp;#160;&lt;a href="#fnref:21" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:22" role="doc-endnote">
&lt;p>Bogetoft P., Otto L. (2011) Stochastic Frontier Analysis SFA. In: Benchmarking with DEA, SFA, and R. International Series in Operations Research &amp;amp; Management Science, vol 157. Springer, New York, NY. &lt;a href="https://doi.org/10.1007/978-1-4419-7961-2_7">https://doi.org/10.1007/978-1-4419-7961-2_7&lt;/a>&amp;#160;&lt;a href="#fnref:22" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:23" role="doc-endnote">
&lt;p>Aigner, Dennis (07/1977). &amp;ldquo;Formulation and estimation of stochastic frontier production function models&amp;rdquo;. Journal of econometrics (0304-4076), 6 (1), p. 21. &lt;a href="https://doi.org/10.1016/0304-4076(77)90052-5">https://doi.org/10.1016/0304-4076(77)90052-5&lt;/a>&amp;#160;&lt;a href="#fnref:23" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:24" role="doc-endnote">
&lt;p>Farm Mechanization-Department of Agriculture and Cooperation, India, Accessed: Nov. 2020, &lt;a href="http://farmech.gov.in/06035-04-ACZ6-15052006.pdf">http://farmech.gov.in/06035-04-ACZ6-15052006.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:24" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Music Mood Classification</title><link>/report/fa20-523-341/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-341/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-341/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-341/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-341/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-341/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Kunaal Shah, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-341/">fa20-523-341&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-341/blob/master/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Music analysis on an individual level is incredibly subjective. A particular song can leave polarizing impressions on the emotions of its listener. One person may find a sense of calm in a piece, while another feels energy. In this study we examine the audio and lyrical features of popular songs in order to find relationships in a song&amp;rsquo;s lyrics, audio features, and its valence. We take advantage of the audio data provided by Spotify for each song in their massive library, as well as lyrical data from popular music news and lyrics site, Genius.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-related-work">2. Related Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-datasets">3. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-analysis">4. Analysis&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-accumulation-of-audio-features-and-lyrics">4.1 Accumulation of audio features and lyrics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-performance-of-sentiment-analysis-on-lyrics">4.2 Performance of sentiment analysis on lyrics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-description-of-select-data-fields">4.3 Description of select data fields&lt;/a>&lt;/li>
&lt;li>&lt;a href="#44-preliminary-analysis-of-data">4.4 Preliminary Analysis of Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#45-scatterplot-analysis">4.5 Scatterplot Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#46-linear-and-polynomial-regression-analyses">4.6 Linear and Polynomial Regression Analyses&lt;/a>&lt;/li>
&lt;li>&lt;a href="#47-multivariate-regression-analysis">4.7 Multivariate Regression Analysis&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-benchmarks">5. Benchmarks&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> music, mood classification, audio, audio content analysis, lyrics, lyrical analysis, big data, spotify, emotion&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The overall mood of a musical piece is generally very difficult to decipher due to the highly subjective nature of music. One person might think a song is energetic and happy, while another may think it is quite sad. This can be attributed to varying interpretations of tone and lyrics in song between different listeners. In this project we study both the audio and lyrical patterns of a song through machine learning and natural language processing (NLP) to find a relationship between the song&amp;rsquo;s lyrics and its valence, or its overall positivity.&lt;/p>
&lt;h2 id="2-related-work">2. Related Work&lt;/h2>
&lt;p>Previous studies take three different ways in classifying the mood of a song according to various mood models by analyzing audio, analyzing lyrics, and analyzing lyrics and audio. Most of these studies have been successful in their goals but have uses a limited collection of songs/words for their analysis &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Perhaps obviously, the best results come when combining audio and lyrics. A simple weighting is given by a study from the University of Illinois to categorize moods of a song by audio and lyrical content analysis, A simple weighting is given by a study from the University of Illinois to categorize moods of a song by audio and lyrical content analysis.&lt;/p>
&lt;p>&lt;code>phybrid = \alpha plyrics + (1 - \alpha )paudio&lt;/code>&lt;/p>
&lt;p>When researching existing work, we found two applications that approach music recommendations based on mood, one is called &amp;lsquo;moooodify&amp;rsquo;, a free web application developed by an independent music enthusiast, Siddharth Ahuja &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Another website, Organize Your Music, aims to organize a Spotify user&amp;rsquo;s music library based on mood, genre, popularity, style, and other categories &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. However, both of these applications do not seem to take into account any lyrical analysis of a song.&lt;/p>
&lt;p>Lyrics of a song can be used to learn a lot about music from lexical pattern analysis to gender, genre, and mood analyses. For example, in an individual study a researcher found that female artists tend to mention girls, women, and friends a lot, while male artists sing about late Saturday Nights, play and love &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Another popular project, SongSim, used repetition to visualize the parts of a song &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. Findings such as these can be used to uncover the gender of an artist based on their lyrics. Similarly, by use of NLP tools, lyrical text can be analyzed to elicit the mood and emotion of a song.&lt;/p>
&lt;h2 id="3-datasets">3. Datasets&lt;/h2>
&lt;p>For the audio content analysis portion of this project, we use Spotify&amp;rsquo;s Web API, which provides a great amount audio data for every song in Spotify&amp;rsquo;s song collection, including valence, energy, and danceability &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>For the lyrical analysis portion of this project, we use Genius&amp;rsquo;s API to pull lyrics information for a song. Genius is a website where users submit lyrics and annotations to several popular songs &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. To perform sentiment analysis on a set of lyrics collected from Genius, we use the NLTK Vader library.&lt;/p>
&lt;h2 id="4-analysis">4. Analysis&lt;/h2>
&lt;p>For the purposes of this study, we analyze a track&amp;rsquo;s lyrics and assign them scores based on their positivity, negativity, and neutrality. We then append this data to the audio feature data we receive from Spotify. To compare and find relationships and meaningfulness in using lyrics and audio features to predict a song&amp;rsquo;s valence, we employ several statistical and machine learning approaches. We try linear regression and polynomial regression to find relationships between several features of a track and a song&amp;rsquo;s valence. Then we perform multivariate linear regression to find how accurately we can predict a song&amp;rsquo;s valence based on the audio and lyrical features available in our dataset.&lt;/p>
&lt;h3 id="41-accumulation-of-audio-features-and-lyrics">4.1 Accumulation of audio features and lyrics&lt;/h3>
&lt;p>From our data sources, we collected data for roughly 10000 of the most popular songs released between 2017 and 2020, taking account of several audio and lyrical features present in the track. We gathered this data by hand, first querying the most popular 2000 newly released songs in each year between 2017 and 2020. We then sent requests to Genius to gather lyrics for each song. Some songs, even though they were popular, did not have lyrics present on Genius, these songs were excluded from our dataset. With BeautifulSoup, we extracted and cleaned up the lyrics, removing anything that is not a part of the song&amp;rsquo;s lyrics like annotations left by users, section headings (Chorus, Hook, etc), and empty lines. After exclusions our data covered 6551 Spotify tracks.&lt;/p>
&lt;h3 id="42-performance-of-sentiment-analysis-on-lyrics">4.2 Performance of sentiment analysis on lyrics&lt;/h3>
&lt;p>With a song&amp;rsquo;s lyrics in hand, we used NLTK&amp;rsquo;s sentiment module, Vader, to read each line in the lyrics. NLTK Vader Sentiment Intensity Analyzer is a pretrained machine learning model that reads a line of text and assigns it scores of positivity, negativity, neutrality, and and overall compound score. We marked lines with a compound score greater than 0.5 as positive, less than -0.1 as negative, and anything in between as neutral. We then found the percentages of positive, negative, and neutral lines in a song&amp;rsquo;s composition and saved them to our dataset.&lt;/p>
&lt;p>We performed a brief analysis of the legibility of the Vader module in determining sentiment on four separate strings. &amp;ldquo;I&amp;rsquo;m happy&amp;rdquo; and &amp;ldquo;I&amp;rsquo;m so happy&amp;rdquo; were used to compare two positive lines, &amp;ldquo;I&amp;rsquo;m happy&amp;rdquo; was expected to have a positive compound score, but slightly less positive than &amp;ldquo;I&amp;rsquo;m so happy&amp;rdquo;. Similarly, we used two negative lines &amp;ldquo;I&amp;rsquo;m sad&amp;rdquo; and the slightly more extreme, &amp;ldquo;I&amp;rsquo;m so sad&amp;rdquo; which were expected to result in negative compound scores with &amp;ldquo;I&amp;rsquo;m sad&amp;rdquo; being less negative than &amp;ldquo;I&amp;rsquo;m so sad&amp;rdquo;.&lt;/p>
&lt;pre>&lt;code>Scores for 'I'm happy': {
'neg': 0.0,
'neu': 0.213,
'pos': 0.787,
'compound': 0.5719
}
Scores for 'I'm so happy': {
'neg': 0.0,
'neu': 0.334,
'pos': 0.666,
'compound': 0.6115
}
Scores for 'I'm sad': {
'neg': 0.756,
'neu': 0.244,
'pos': 0.0,
'compound': -0.4767
}
Scores for 'I'm so sad': {
'neg': 0.629,
'neu': 0.371,
'pos': 0.0,
'compound': -0.5256
}
&lt;/code>&lt;/pre>&lt;p>While these results confirmed our expectations, a few issues come to the table with our use of the Vader module. One is that Vader takes into consideration additional string features such as punctuation in its determination of score, meaning &amp;ldquo;I&amp;rsquo;m so sad!&amp;rdquo; will be more negative than &amp;ldquo;I&amp;rsquo;m so sad&amp;rdquo;. Since lyrics on Genius are contributed by the community, in most cases there is a lack of consistency using accurate punctuation. Additionally, in some cases there can be typos present in a line of lyrics, both of which can skew our data. However we determined that our method in using the Vader module is suitable for our project as we simply want to determine if a track is positive or negative without needing to be too specific. Another issue is that our implementation of Vader acts only on English words. Again, since lyrics on Genius are contributed by the community, there could be errors in our data from misspelled word contributions as well as sections or entire lyrics written in different languages.&lt;/p>
&lt;p>In addition to performing sentiment analysis on the lyrics, we tokenized the lyrics, removing common words such as &amp;lsquo;a&amp;rsquo;, &amp;lsquo;the&amp;rsquo;,&amp;lsquo;for&amp;rsquo;, etc. This was done to collect data on the number of meaningful and number of non-repeating words in each song. Albeit while this data was never used in our study, it could prove useful in future studies.&lt;/p>
&lt;p>&lt;em>Table 1&lt;/em> displays a snapshot of the data we collected from seven tracks released in 2020. The dataset contains 27 fields, 12 of which describe the audio features of a track, and 8 of which describe the lyrics of the track. For the purpose of this study we exclude the use of audio features key, duration, and time signature.&lt;/p>
&lt;p>&lt;strong>Table 1:&lt;/strong> Snapshot of dataset containing tracks released in 2020&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>danceability&lt;/th>
&lt;th>energy&lt;/th>
&lt;th>key&lt;/th>
&lt;th>loudness&lt;/th>
&lt;th>speechiness&lt;/th>
&lt;th>acousticness&lt;/th>
&lt;th>instrumentalness&lt;/th>
&lt;th>liveness&lt;/th>
&lt;th>valence&lt;/th>
&lt;th>tempo&lt;/th>
&lt;th>duration_ms&lt;/th>
&lt;th>time_signature&lt;/th>
&lt;th>name&lt;/th>
&lt;th>artist&lt;/th>
&lt;th>num_positive&lt;/th>
&lt;th>num_negative&lt;/th>
&lt;th>num_neutral&lt;/th>
&lt;th>positivity&lt;/th>
&lt;th>negativity&lt;/th>
&lt;th>neutrality&lt;/th>
&lt;th>word_count&lt;/th>
&lt;th>unique_word_count&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0.709&lt;/td>
&lt;td>0.548&lt;/td>
&lt;td>10&lt;/td>
&lt;td>-8.493&lt;/td>
&lt;td>0.353&lt;/td>
&lt;td>0.65&lt;/td>
&lt;td>1.59E-06&lt;/td>
&lt;td>0.133&lt;/td>
&lt;td>0.543&lt;/td>
&lt;td>83.995&lt;/td>
&lt;td>160000&lt;/td>
&lt;td>4&lt;/td>
&lt;td>What You Know Bout Love&lt;/td>
&lt;td>Pop Smoke&lt;/td>
&lt;td>7&lt;/td>
&lt;td>2&lt;/td>
&lt;td>33&lt;/td>
&lt;td>0.166666667&lt;/td>
&lt;td>0.047619048&lt;/td>
&lt;td>0.785714286&lt;/td>
&lt;td>209&lt;/td>
&lt;td>130&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.799&lt;/td>
&lt;td>0.66&lt;/td>
&lt;td>1&lt;/td>
&lt;td>-6.153&lt;/td>
&lt;td>0.079&lt;/td>
&lt;td>0.256&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0.111&lt;/td>
&lt;td>0.471&lt;/td>
&lt;td>140.04&lt;/td>
&lt;td>195429&lt;/td>
&lt;td>4&lt;/td>
&lt;td>Lemonade&lt;/td>
&lt;td>Internet Money&lt;/td>
&lt;td>8&lt;/td>
&lt;td>15&lt;/td>
&lt;td>34&lt;/td>
&lt;td>0.140350877&lt;/td>
&lt;td>0.263157895&lt;/td>
&lt;td>0.596491228&lt;/td>
&lt;td>307&lt;/td>
&lt;td>177&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.514&lt;/td>
&lt;td>0.73&lt;/td>
&lt;td>1&lt;/td>
&lt;td>-5.934&lt;/td>
&lt;td>0.0598&lt;/td>
&lt;td>0.00146&lt;/td>
&lt;td>9.54E-05&lt;/td>
&lt;td>0.0897&lt;/td>
&lt;td>0.334&lt;/td>
&lt;td>171.005&lt;/td>
&lt;td>200040&lt;/td>
&lt;td>4&lt;/td>
&lt;td>Blinding Lights&lt;/td>
&lt;td>The Weeknd&lt;/td>
&lt;td>3&lt;/td>
&lt;td>10&lt;/td>
&lt;td>22&lt;/td>
&lt;td>0.085714286&lt;/td>
&lt;td>0.285714286&lt;/td>
&lt;td>0.628571429&lt;/td>
&lt;td>150&lt;/td>
&lt;td>75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.65&lt;/td>
&lt;td>0.613&lt;/td>
&lt;td>9&lt;/td>
&lt;td>-6.13&lt;/td>
&lt;td>0.128&lt;/td>
&lt;td>0.00336&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0.267&lt;/td>
&lt;td>0.0804&lt;/td>
&lt;td>149.972&lt;/td>
&lt;td>194621&lt;/td>
&lt;td>4&lt;/td>
&lt;td>Wishing Well&lt;/td>
&lt;td>Juice WRLD&lt;/td>
&lt;td>0&lt;/td>
&lt;td>22&lt;/td>
&lt;td>30&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0.423076923&lt;/td>
&lt;td>0.576923077&lt;/td>
&lt;td>238&lt;/td>
&lt;td>104&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.737&lt;/td>
&lt;td>0.802&lt;/td>
&lt;td>0&lt;/td>
&lt;td>-4.771&lt;/td>
&lt;td>0.0878&lt;/td>
&lt;td>0.468&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0.0931&lt;/td>
&lt;td>0.682&lt;/td>
&lt;td>144.015&lt;/td>
&lt;td>172325&lt;/td>
&lt;td>4&lt;/td>
&lt;td>positions&lt;/td>
&lt;td>Ariana Grande&lt;/td>
&lt;td>10&lt;/td>
&lt;td>5&lt;/td>
&lt;td>33&lt;/td>
&lt;td>0.208333333&lt;/td>
&lt;td>0.104166667&lt;/td>
&lt;td>0.6875&lt;/td>
&lt;td>178&lt;/td>
&lt;td>73&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.357&lt;/td>
&lt;td>0.425&lt;/td>
&lt;td>5&lt;/td>
&lt;td>-7.301&lt;/td>
&lt;td>0.0333&lt;/td>
&lt;td>0.584&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0.322&lt;/td>
&lt;td>0.27&lt;/td>
&lt;td>102.078&lt;/td>
&lt;td>198040&lt;/td>
&lt;td>3&lt;/td>
&lt;td>Heather&lt;/td>
&lt;td>Conan Gray&lt;/td>
&lt;td>3&lt;/td>
&lt;td>4&lt;/td>
&lt;td>22&lt;/td>
&lt;td>0.103448276&lt;/td>
&lt;td>0.137931034&lt;/td>
&lt;td>0.75862069&lt;/td>
&lt;td>114&lt;/td>
&lt;td>66&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.83&lt;/td>
&lt;td>0.585&lt;/td>
&lt;td>0&lt;/td>
&lt;td>-6.476&lt;/td>
&lt;td>0.094&lt;/td>
&lt;td>0.237&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0.248&lt;/td>
&lt;td>0.485&lt;/td>
&lt;td>109.978&lt;/td>
&lt;td>173711&lt;/td>
&lt;td>4&lt;/td>
&lt;td>34+35&lt;/td>
&lt;td>Ariana Grande&lt;/td>
&lt;td>3&lt;/td>
&lt;td>13&lt;/td>
&lt;td>52&lt;/td>
&lt;td>0.044117647&lt;/td>
&lt;td>0.191176471&lt;/td>
&lt;td>0.764705882&lt;/td>
&lt;td>249&lt;/td>
&lt;td>127&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="43-description-of-select-data-fields">4.3 Description of select data fields&lt;/h3>
&lt;p>The following terms defined are important in our analyses. In our data set most terms contain are represented by a value between 0 and 1, indicating least to most. For example, looking at the first two rows in &lt;em>Table 1&lt;/em>, we can see that the track by the artist, Pop Smoke, has a greater speechiness score, indicating a greater percentage of that song contains spoken word.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Danceability:&lt;/strong> uses several musical elements (tempo, stability, beat strength, regularity) to determine how suitable a given track is for dancing&lt;/li>
&lt;li>&lt;strong>Energy:&lt;/strong> measures intensity of a song&lt;/li>
&lt;li>&lt;strong>Loudness:&lt;/strong> a songs overall loudness measured in decibels&lt;/li>
&lt;li>&lt;strong>Speechiness:&lt;/strong> identifies how much of a track contains spoken word&lt;/li>
&lt;li>&lt;strong>Acousticness:&lt;/strong> confidence of a track being acoustic, or with physical instruments&lt;/li>
&lt;li>&lt;strong>Instrumentalness:&lt;/strong> confidence of a track having no vocals&lt;/li>
&lt;li>&lt;strong>Liveness:&lt;/strong> confidence of a track being a live recording&lt;/li>
&lt;li>&lt;strong>Valence:&lt;/strong> predicts the overall happiness, or positivity of a track based on its musical features&lt;/li>
&lt;li>&lt;strong>Tempo:&lt;/strong> the average beats per minute of a track&lt;/li>
&lt;li>&lt;strong>Positivity:&lt;/strong> percentage of lines in a track&amp;rsquo;s lyrics determined to have a positive sentiment score&lt;/li>
&lt;li>&lt;strong>Negativity:&lt;/strong> percentage of lines in a track&amp;rsquo;s lyrics determined to have a negative sentiment score&lt;/li>
&lt;li>&lt;strong>Neutrality:&lt;/strong> percentage of lines in a track&amp;rsquo;s lyrics determined to have a neutral sentiment score&lt;/li>
&lt;/ul>
&lt;p>Out of these fields, we seek to find which audio features correlate to a song&amp;rsquo;s valence and if our positivity and negativity scores of a song&amp;rsquo;s lyrics provide any meaningfulness in determining a song&amp;rsquo;s positivity. For the purpose of this study we mainly focus on valence, energy, danceability, positivity, and negativity.&lt;/p>
&lt;h3 id="44-preliminary-analysis-of-data">4.4 Preliminary Analysis of Data&lt;/h3>
&lt;p>When calculating averages of the feature fields captured in our dataset, we found it interesting that based on our lyrical interpretation, tracks between 2017 and 2020 tended to be more negative than positive. The average negativity score for a track in our dataset was 0.21 which means 21% of the lines in the track were deemed to have negative connotation, while having a 0.08 positivity score.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-341/raw/main/project/images/all_tracks_heatmap.png" alt="Heatmap">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Heatmap of data with fields valence, energy, danceability, positivity, negativity&lt;/p>
&lt;p>Backed by &lt;em>Figure 1&lt;/em>, we find that track lyrics tend to be more negative than positive. However for the most part, even with tracks with negative lyrics, the valence, or overall happiness of the audio features hovers around 0.5; indicating that most songs tend to have neutral audio features. Looking at tracks with lyrics that are highly positive we find that the valence rises to about 0.7 to 0.8 and that songs with extremely high negatively also cause the valence to drop to the 0.3 range. These observations indicate that only extremes in lyrical sentiment correlate significantly in a song&amp;rsquo;s valence, as some songs with negative lyrics may also be fast-tempo and energetic, keeping the valence relatively high compared to lyrical composition. This is shown in our visualization, where both tracks with positive and negative lyricals have high energy and danceability values, indicating fast-tempos and high-pitches.&lt;/p>
&lt;h3 id="45-scatterplot-analysis">4.5 Scatterplot Analysis&lt;/h3>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-341/raw/main/project/images/audio_features_scatterplots.png" alt="Audio_Features_Scatterplots">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Scatterplots showing relation of features danceability, energy, speechiness, positivity, negativity, and neutrality to valence.&lt;/p>
&lt;p>&lt;em>Figure 2&lt;/em> describes the relation of several data fields we collected to a song&amp;rsquo;s valence, or its overall positivity. We find that the positivity and negativity plots reflect that of the speechiness plot in that there seems to be little correlation between the x and y axes. On the other hand neutrality seems to show a positive correlation between a song&amp;rsquo;s lyrical content and its respective valence. If a song is more neutral, it seems more likely to have a higher valence.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-341/raw/main/project/images/spotify_distributions.png" alt="Spotify Distributions">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Distributions of field values across the Spotify music library &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Our scatterplots do show consistency with the expected distributions exemplified in the Spotify API documentation, as shown in &lt;em>Figure 3&lt;/em>. In the top three plots, which use values for audio features obtained exclusively obtained from the audio features given by Spotify, we can see the these matching distributions which imply that most songs fall in the 0.4 to 0.8 range for danceability, energy, and valence, and 0 to 0.1 for speechiness. The low distribution in speechiness can be explained by music features being more dependant on instruments and sounds than spoken word. A track with higher than 0.33 speechiness score indicates that the track is very high in spoken word content over music, like a poetry recitation, talk show clip, etc &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="46-linear-and-polynomial-regression-analyses">4.6 Linear and Polynomial Regression Analyses&lt;/h3>
&lt;p>We performed a simple linear regression test against valence with the audio and lyrical features described in &lt;em>Figure 2&lt;/em> and &lt;em>Figure 3&lt;/em>. Like the charts show, it was hard to find any linear correlation between the fields. &lt;em>Table 2&lt;/em> displays the r-squared results that we obtained when applying linear regression to find the relationship between a song&amp;rsquo;s feature and its valence. The only features that indicate potential relationships with a song&amp;rsquo;s valence are energy, and danceability, as definitions of energy and and danceability indicate some semblance of positivity as well.&lt;/p>
&lt;p>&lt;strong>Table 2:&lt;/strong> R-Squared results obtained from linear regression application on select fields against valence&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>R-Squared&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Positivity&lt;/td>
&lt;td>-0.090859047&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Negativity&lt;/td>
&lt;td>-0.039686828&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Neutrality&lt;/td>
&lt;td>0.093002783&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Energy&lt;/td>
&lt;td>0.367113611&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Danceability&lt;/td>
&lt;td>0.324412662&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Speechiness&lt;/td>
&lt;td>0.066492856&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Since we found little relation between the selected features and valence, we tried applying polynomial regression with the same features as shown in &lt;em>Table 3&lt;/em>. Again, we failed to find any relationship between a feature in our dataset and the song&amp;rsquo;s valence. Energy and danceability once again were found to have the highest relationship with valence. We speculate that some of the data we have is misleading the regression applications; as mentioned before, we found some issues in reading sentiment in the lyrics we collected due to misspelled words, inaccurate punctuations, and non-english words.&lt;/p>
&lt;p>&lt;strong>Table 3:&lt;/strong> R-Square results obtained from polynomial regression application on select data fields against valence&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>R-Squared&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Positivity&lt;/td>
&lt;td>0.013164307&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Negativity&lt;/td>
&lt;td>0.001588184&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Neutrality&lt;/td>
&lt;td>0.010308495&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Energy&lt;/td>
&lt;td>0.136822113&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Danceability&lt;/td>
&lt;td>0.113119545&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Speechiness&lt;/td>
&lt;td>0.008913925&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="47-multivariate-regression-analysis">4.7 Multivariate Regression Analysis&lt;/h3>
&lt;p>We performed multivariate regression tests to predict a song&amp;rsquo;s valence with a training set of 5500 tracks and a test set of 551 tracks. Our first test only included four independent variables: neutrality, energy, danceability, and speechiness. Our second test included all numerical fields available in our data, adding loudness, acousticness, liveness, instrumentalness, tempo, positivity, word count, and unique word count to the regression coefficient calculations. In both tests we calculated the relative mean squared error (RMSE) between our predicted values and the actual values of a song&amp;rsquo;s valence given several features. Our RMSEs were 0.1982 and 0.1905 respectively, indicating that as expected, adding additional pertinent independent variables gave slightly better results. However given that a song&amp;rsquo;s valence is captured between 0 and 1.0, and both our RSMEs were approximately 0.19, it is unclear how significant the results of these tests are. &lt;em>Figure 4&lt;/em> and &lt;em>Figure 5&lt;/em> show the calculated differences between the predicted and actual values for the first 50 tracks in our testing dataset for each regression test respectively.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-341/raw/main/project/images/multivariate_regression_1.png" alt="Multivariate Regression 1">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Differences between expected and predicted values with application of multivariate regression model with 4 independent variables&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-341/raw/main/project/images/multivariate_regression_2.png" alt="Multivariate Regression 2">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Differences between expected and predicted values with application of multivariate regression model with 12 independent variables&lt;/p>
&lt;h2 id="5-benchmarks">5. Benchmarks&lt;/h2>
&lt;p>&lt;em>Table 4&lt;/em> displays the benchmarks we received from key parts of our analyses. As expected, creating our dataset took a longer amount of time relative to the rest of the benchmarks. This is because accumulating the data involved sending two requests to online sources, and running the sentiment intensity analyzer on the lyrics received from the Genius API calls. Getting the sentiment of a line of text itself did not take much time at all. We found it interesting that applying multivariate regression on our dataset was much quicker than calculating averages on our dataset with numpy, and that it was the fastest process to complete.&lt;/p>
&lt;p>&lt;strong>Table 4:&lt;/strong> Benchmark Results&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Status&lt;/th>
&lt;th>Time&lt;/th>
&lt;th>Sum&lt;/th>
&lt;th>Start&lt;/th>
&lt;th>tag&lt;/th>
&lt;th>Node&lt;/th>
&lt;th>User&lt;/th>
&lt;th>OS&lt;/th>
&lt;th>Version&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Create dataset of 10 tracks&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>12.971&lt;/td>
&lt;td>168.523&lt;/td>
&lt;td>2020-12-07 00:19:30&lt;/td>
&lt;td>&lt;/td>
&lt;td>884e3d61f237&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sentiment Intensity Analyzer on a line of lyrical text&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.001&lt;/td>
&lt;td>0.005&lt;/td>
&lt;td>2020-12-07 00:19:49&lt;/td>
&lt;td>&lt;/td>
&lt;td>884e3d61f237&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Load dataset&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.109&lt;/td>
&lt;td>1.08&lt;/td>
&lt;td>2020-12-07 00:19:59&lt;/td>
&lt;td>&lt;/td>
&lt;td>884e3d61f237&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Calculate averages of values in dataset&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.275&lt;/td>
&lt;td>0.597&lt;/td>
&lt;td>2020-12-07 00:19:59&lt;/td>
&lt;td>&lt;/td>
&lt;td>884e3d61f237&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Multivariate Regression Analysis on dataset&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.03&lt;/td>
&lt;td>0.151&lt;/td>
&lt;td>2020-12-07 00:21:49&lt;/td>
&lt;td>&lt;/td>
&lt;td>884e3d61f237&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Generate and display heatmap of data&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.194&lt;/td>
&lt;td>0.194&lt;/td>
&lt;td>2020-12-07 00:20:03&lt;/td>
&lt;td>&lt;/td>
&lt;td>884e3d61f237&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Plot differences&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.504&lt;/td>
&lt;td>1.473&lt;/td>
&lt;td>2020-12-07 00:21:50&lt;/td>
&lt;td>&lt;/td>
&lt;td>884e3d61f237&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>We received inconclusive results from our study. The linear and polynomial regression tests that we performed, showed little correlation between our lyrical features and a track&amp;rsquo;s valence. This was backed by our multivariate regression test which performed with a RSME score of about 0.19 on our dataset. Since valence is recorded on a scale from 0 to 1.0, this means that our predictions typically fall within 20% of the actual value, which is considerably inaccurate. As previous studies have shown massive improvements in combining lyrical and audio features for machine learning applications in music, we believe that the blame for our low scores falls heavily on our approach to assigning sentiment scores on our lyrics &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Future studies should consider the presence of foreign lyrics and the potential inaccuracies of community submitted lyrics.&lt;/p>
&lt;p>There are several other elements of this study that could be improved upon in future iterations. In this project we only worked with songs released after the beginning of 2017, but obviously, people would still enjoy listening to songs from previous years. The Spotiy API contains audio features data for every song in its library, so it would be worth collecting that data on every song for usage in the generation of song recommendations. Secondly, our data set excluded songs on Spotify, whose lyrics could not be found easily on Genius.com. We should have handled these cases by attempting to find the lyrics from other popular websites which store music lyrics. And lastly, we worked with a very small dataset relative to the total amount of songs that exist, or that are available on Spotify. There is great possibility in repeating this study quite easily with a greater selection of songs. We were surprised by how small the file sizes were of our dataset of 6551 songs, the aggregated data set being only 2.3 megabytes in size. Using that value, a set of one million songs can be estimated to only be around 350 megabytes.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>We would like to give our thanks to Dr. Geoffrey Fox, Dr. Gregor von Laszewski, and the other associate instructors who taught FA20-BL-ENGR-E534-11530: Big Data Applications during the Fall 2020 semester at Indiana University, Bloomington for their suggestions and assistance in compiling this project report. Additionally we would like to thank the students who contributed to Piazza by either answering questions that we had ourselves, or giving their own suggestions and experiences in building projects. In taking this course we learned of several applications of and use cases for big data applications, and gained the knowledge to build our own big data projects.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Kashyap, N., Choudhury, T., Chaudhary, D. K., &amp;amp; Lal, R. (2016). Mood Based Classification of Music by Analyzing Lyrical Data Using Text Mining. 2016 International Conference on Micro-Electronics and Telecommunication Engineering (ICMETE). doi:10.1109/icmete.2016.65&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Ahuja, S. (2019, September 25). Sort your music by any mood - Introducing moooodify. Retrieved November 17, 2020, from &lt;a href="https://blog.usejournal.com/sort-your-music-by-any-mood-introducing-moooodify-41749e80faab">https://blog.usejournal.com/sort-your-music-by-any-mood-introducing-moooodify-41749e80faab&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Lamere, P. (2016, August 6). Organize Your Music. Retrieved November 17, 2020, from &lt;a href="http://organizeyourmusic.playlistmachinery.com/">http://organizeyourmusic.playlistmachinery.com/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Jeong, J. (2019, January 19). What Songs Tell Us About: Text Mining with Lyrics. Retrieved November 17, 2020, from &lt;a href="https://towardsdatascience.com/what-songs-tell-us-about-text-mining-with-lyrics-ca80f98b3829">https://towardsdatascience.com/what-songs-tell-us-about-text-mining-with-lyrics-ca80f98b3829&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Morris, C. (2016). SongSim. Retrieved November 17, 2020, from &lt;a href="https://colinmorris.github.io/SongSim/">https://colinmorris.github.io/SongSim/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Get Audio Features for a Track. (2020). Retrieved November 17, 2020, from &lt;a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/">https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Genius API Documentation. (2020). Retrieved November 17, 2020, from &lt;a href="https://docs.genius.com/">https://docs.genius.com/&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Hu, X., &amp;amp; Downie, J. S. (2010). Improving mood classification in music digital libraries by combining lyrics and audio. Proceedings of the 10th Annual Joint Conference on Digital Libraries - JCDL &amp;lsquo;10. doi:10.1145/1816123.1816146&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Does Modern Day Music Lack Uniqueness Compared to Music before the 21st Century</title><link>/report/fa20-523-333/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-333/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-333/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-333/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;ul>
&lt;li>Raymond Adams, fa20-523-333&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-333/blob/main/project/project.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>One of the most influential aspects of human culture is music. It has a way of changing as humans evolve themselves. Music has changed drastically over the last 100 years. Before the 21&lt;sup>st&lt;/sup> century most people seemed to welcome the change. However, in the 2000&amp;rsquo;s people began stating that music seemed to be changing for the worse. Music, usually adults perspectives, has began lacking uniqueness. These statements come from interviews, speaking with family and friends, tv shows, and movies. This project looked at 99 years of spotify music data and determined that all features of most tracks have changed in different ways. Because uniqueness can be related to variation the variation of different features were used to determine if tracks did lack uniqueness. Through data analysis it was concluded that they did.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data">2. Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-methods">3. Methods&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-results">4. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#51-limitations">5.1 Limitations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> music, spotify data, uniqueness, music evolution, 21&lt;sup>st&lt;/sup> century music.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Music is one of the most influential elements in the arts. It has a great impact on the way humans act and feel. Research, done by Nina Avramova, has shown that different genres of music bring about different emotions and feelings through the listener. Nonetheless, humans also have a major impact on the music itself. Music and humans are mutually dependent, therefore when one evolves, so does the other.&lt;/p>
&lt;p>This scientific journal intends to progress the current understanding of how music has changed since the 21&lt;sup>st&lt;/sup> century. It also aims to determine if this change in music has led to a lack of uniqueness amongst the common features of a song compared to music before the 21&lt;sup>st&lt;/sup> century.&lt;/p>
&lt;h2 id="2-data">2. Data&lt;/h2>
&lt;p>The data is located on Kaggle and was collected by a data scientist named Yamac Eren Ay. He collected more than 160,000 songs from Spotify that ranged from 1921 to 2020. Some of the features that this data set includes and will be used to conduct an analysis are: danceability, energy, acousticness, instrumentalness, valence, tempo, key, and loudness. This data frame can be seen in Figure 1.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/raw/main/project/images/DataFrame.png" alt="dataframe">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Dataframe of spotify data collected from Kaggle&lt;/p>
&lt;p>&lt;strong>Danceability&lt;/strong> describes how appropriate a track is for dancing by looking at multiple elements including tempo, rhythm stability, beat strength, and general regularity. The closer the value is to 0.0 the less danceable the song is and the closer it is to 1.0 the more danceable it is. &lt;strong>Energy&lt;/strong> is a sensual measure of intensity and activity. Usually, energetic songs feel fast, loud, and noisy. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The closer this value is to 0.0 the less energetic the track is and the closer it is to 1.0 the more energetic the track is. &lt;strong>Acoustic&lt;/strong> music refers to songs that are created using instruments and recorded in a natural environment as opposed to being recorded by electronic means. The closer the value is to 0.0 the less acoustic it is and the closer it is to 1.0 the more acoustic it is. &lt;strong>Instrumentalness&lt;/strong> predicts how vocal a track is. Thus, songs that contain words other than &amp;ldquo;Oh&amp;rdquo; and &amp;ldquo;Ah&amp;rdquo; are considered vocal. The closer the value is to 0.0 the less likely the track contains vocals and the closer the value is to 1.0 the more likely it contains vocals. &lt;strong>Valence&lt;/strong> describes the musical positiveness expressed through a song. Tracks with high valence (closer to 0.0) sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence (closer to 1.0) sound more negative (e.g. sad, depressed, angry) &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The &lt;strong>tempo&lt;/strong> is the overall pace of a track measured in BPM (beats per minute). The &lt;strong>key&lt;/strong> is the overall approximated pitch that a song is played in. The possible keys and their integer values are: C = 0; C# = 1; D = 2; D#, Eb = 3; E = 4; F = 5; F#, Gb = 6; G = 7; G#, Ab = 8; A = 9; A#, Bb = 10; B, Cb = 11. The overall &lt;strong>loudness&lt;/strong> of a track is measured in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing the relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 dB, 0 being the most loud &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-methods">3. Methods&lt;/h2>
&lt;p>Data analysis was used to answer this paper&amp;rsquo;s research question. The data set was imported into Jupyter notebook using pandas, a software library built for data manipulation and analysis. The first step was cleaning the data. The data set contained 169,909 rows and 19 columns. After dropping rows where at least one column had NaN values, values that are undefined or unrepresentable, the data set still contained all 169,909 rows. Thus, the data set was cleaned by Yamac Ay prior to it being downloaded. The second step in the data analysis was editing the data. The objective of this research was to compare music tracks before the 21&lt;sup>st&lt;/sup> century to tracks after the 21&lt;sup>st&lt;/sup> century and see if, how, and why they differ. As well as do tracks after the 21&lt;sup>st&lt;/sup> century lack uniqueness compared to tracks that were created before the 21&lt;sup>st&lt;/sup> century.&lt;/p>
&lt;p>When Yamac Ay collected the data he separated it into five comma-separated values (csv) files. The first file, titled data.csv, contained all the information that was needed to conduct data analysis. Although this file contained the feature &amp;ldquo;year&amp;rdquo; that was required to analyze the data based on the period of time, it still needed to be manipulated to distinguish what years were attributed to before and after the 21&lt;sup>st&lt;/sup> century. A python script was built to create a new column titled &amp;ldquo;years_split&amp;rdquo; that sorted all rows into qualitative binary values. These values were defined as &amp;ldquo;before_21st_century&amp;rdquo; and &amp;ldquo;after_21st_century&amp;rdquo;. Rows, where the tracks feature &amp;ldquo;year&amp;rdquo; were between 0 and 2000 were assigned to &amp;ldquo;before_21st_century&amp;rdquo; and tracks where the feature &amp;ldquo;year&amp;rdquo; was between 2001 and 2020 were assigned to &amp;ldquo;after_21st_century&amp;rdquo;. It is important to note that over 76% of the rows were attributed to &amp;ldquo;before_21st_century&amp;rdquo;. Therefore, the majority of tracks collected in this dataset were released before 2001.&lt;/p>
&lt;h2 id="4-results">4. Results&lt;/h2>
&lt;p>The features that were analyzed were quantitative values. Thus, it was decided that histograms were the best plots for examining and comparing the data. The first feature that was analyzed is &amp;ldquo;danceability&amp;rdquo;. The visualization for danceability is seen in Figure 2.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/raw/main/project/images/danceability_histogram_before_and_after_21stcentury.png" alt="danceability">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Danceability shows how danceable a song is&lt;/p>
&lt;p>The histogram assigned to &amp;ldquo;before_21st_century&amp;rdquo; resembles a normal distribution. The &lt;strong>mean&lt;/strong> of the histogram is 0.52 while the &lt;strong>mode&lt;/strong> is 0.57. The &lt;strong>variance&lt;/strong> of the data is 0.02986. The histogram assigned to &amp;ldquo;after_21st_century&amp;rdquo; closely resembles a normal distribution. The &lt;strong>mean&lt;/strong> is 0.59 and the &lt;strong>mode&lt;/strong> is 0.61. The &lt;strong>variance&lt;/strong> of the data is 0.02997. The bulk of the data before the 21&lt;sup>st&lt;/sup> century lies between 0.2 and 0.8. However, when looking at the data after the 21&lt;sup>st&lt;/sup> century the majority of it lies between 0.4 and 0.9. This implies that songs have become more danceable but the variation of less danceable to danceable is practically the same.&lt;/p>
&lt;p>The second feature that was analyzed is &amp;ldquo;energy&amp;rdquo;. The visualization for energy is seen in Figure 3.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/raw/main/project/images/energy_histogram_before_and_after_21stcentury.png" alt="energy">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Energy shows how energetic a song is&lt;/p>
&lt;p>The histogram assigned to &amp;ldquo;before_21st_century&amp;rdquo; does not resemble a normal distribution. The &lt;strong>mean&lt;/strong> of the histogram is 0.44 while the &lt;strong>mode&lt;/strong> is 0.25. The &lt;strong>variance&lt;/strong> of the data is 0.06819. The histogram assigned to &amp;ldquo;after_21st_century&amp;rdquo; also does not resemble a normal distribution. The &lt;strong>mean&lt;/strong> is 0.65 and the &lt;strong>mode&lt;/strong> is 0.73. The &lt;strong>variance&lt;/strong> of the data is 0.05030. The data before the 21&lt;sup>st&lt;/sup> century is skewed right while the data after the 21&lt;sup>st&lt;/sup> century is skewed left. This indicates that tracks have become much more energetic since the 21&lt;sup>st&lt;/sup> century. Songs before 2001 on average have an energy level of 0.44 but there are still many songs with high energy levels. Where as, songs after 2001 on average have an energy level of 0.65 but there are very few songs with low energy levels.&lt;/p>
&lt;p>The third feature that was analyzed was &amp;ldquo;acousticness&amp;rdquo;. The visualization for acousticness is seen in Figure 4.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/raw/main/project/images/acousticness_histogram_before_and_after_21stcentury.png" alt="acousticness">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Acousticness shows how acoustic (type of instrument as a collective) a song is&lt;/p>
&lt;p>The &lt;strong>mean&lt;/strong> of the histogram assigned to &amp;ldquo;before_21st_century&amp;rdquo; is 0.57 while the &lt;strong>mode&lt;/strong> is 0.995. The &lt;strong>variance&lt;/strong> of the data is 0.13687. The histogram assigned to &amp;ldquo;after_21st_century&amp;rdquo; has a &lt;strong>mean&lt;/strong> of 0.26 and &lt;strong>mode&lt;/strong> of 0.114. The &lt;strong>variance&lt;/strong> of the data is 0.08445 . The graph shows that music made before the 21&lt;sup>st&lt;/sup> century varied from non-acoustic to acoustic. However, when analyzing music after the 21&lt;sup>st&lt;/sup> century the graph shows that most music is created using non-acoustic instruments. It is assumed that this change in outlet of sounds is due to music production transitioning from acoustic to analog to now digital. However, more in depth research would need to be completed to confirm this assumption.&lt;/p>
&lt;p>The fourth histogram to be anaylzed was &amp;ldquo;instrumentalness&amp;rdquo;. The visualization for instrumentalness is seen in Figure 5.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/raw/main/project/images/instrumentalness_histogram_before_and_after_21stcentury.png" alt="instrumentalness">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Instrumentalness shows how instrumental a song is&lt;/p>
&lt;p>The &lt;strong>mean&lt;/strong> of the histogram assigned to &amp;ldquo;before_21st_century&amp;rdquo; is 0.19 while the &lt;strong>mode&lt;/strong> is 0.0. The &lt;strong>variance&lt;/strong> of the data is 0.10699. The histogram assigned to &amp;ldquo;after_21st_century&amp;rdquo; has a &lt;strong>mean&lt;/strong> of 0.07 and &lt;strong>mode&lt;/strong> of 0.0. The &lt;strong>variance&lt;/strong> of the data is 0.04786 . By analyzing the graph it appears that the instrumentalness for before and after the 21&lt;sup>st&lt;/sup> century are relatively similar. Both histograms are skewed right but the histogram attributed to after the 21&lt;sup>st&lt;/sup> century has much less songs that are instrumental compared to songs before the 21&lt;sup>st&lt;/sup> century. The variation of non-instrumental to instrumental tracks after the 21&lt;sup>st&lt;/sup> century is all far less compared to tracks before the 21&lt;sup>st&lt;/sup> century.&lt;/p>
&lt;p>The fifth histogram that was analyzed is &amp;ldquo;valence&amp;rdquo;. The visualization for valence is seen in Figure 6.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/raw/main/project/images/valence_histogram_before_and_after_21stcentury.png" alt="valence">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Valence shows how positive a song is&lt;/p>
&lt;p>The &lt;strong>mean&lt;/strong> of the histogram assigned to &amp;ldquo;before_21st_century&amp;rdquo; is 0.54 while the &lt;strong>mode&lt;/strong> is 0.961. The &lt;strong>variance&lt;/strong> of the data is 0.07035. The histogram assigned to &amp;ldquo;after_21st_century&amp;rdquo; has a &lt;strong>mean&lt;/strong> of 0.49 and &lt;strong>mode&lt;/strong> of 0.961. The &lt;strong>variance&lt;/strong> of the data is 0.06207. By analyzing the graph we can see that the valence before and after the 21&lt;sup>st&lt;/sup> century has remained fairly the same in terms of shape. However, the average value of valence after the 21&lt;sup>st&lt;/sup> century decreased by 0.05. Thus, songs have become less positive but there are still a good amount of positive songs being created.&lt;/p>
&lt;p>The sixth histogram that was analyzed is &amp;ldquo;tempo&amp;rdquo;. The visualization for tempo is seen in Figure 7.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/raw/main/project/images/tempo_histogram_before_and_after_21stcentury.png" alt="tempo">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Tempo shows the speed a song is played in&lt;/p>
&lt;p>The &lt;strong>mean&lt;/strong> of the histogram attributed to &amp;ldquo;before_21st_century&amp;quot;is 115.66. The &lt;strong>variance&lt;/strong> of the data is 933.57150. The histogram assigned to &amp;ldquo;after_21st_century&amp;rdquo; has a &lt;strong>mean&lt;/strong> of 121.19. The &lt;strong>variance&lt;/strong> of the data is 955.44287. This indicates that tracks after the 21&lt;sup>st&lt;/sup> century have increased tempo by a little over 6 BPM. Tracks after the 21&lt;sup>st&lt;/sup> century also have more variation than tracks before the 21&lt;sup>st&lt;/sup> century.&lt;/p>
&lt;p>The seventh histogram that was analyzed is &amp;ldquo;key&amp;rdquo;. The visualization for key is seen in Figure 8.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/raw/main/project/images/key_histogram_before_and_after_21stcentury.png" alt="key">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Key labels the overall pitch a song is in&lt;/p>
&lt;p>The &lt;strong>mean&lt;/strong> of the histogram assigned to &amp;ldquo;before_21st_century&amp;rdquo; is 5.19 The &lt;strong>variance&lt;/strong> of the data is 12.22468. The histogram assigned to &amp;ldquo;after_21st_century&amp;rdquo; has a &lt;strong>mean&lt;/strong> of 5.24. The &lt;strong>variance&lt;/strong> of the data is 12.79017. This information implies that the key of songs have mostly stayed the same hoever, there are less songs after the 21&lt;sup>st&lt;/sup> century being created in C, C#, and D compared to songs before the 21&lt;sup>st&lt;/sup> century. The key of songs after 2001 are also more spread out compared to songs before 2001.&lt;/p>
&lt;p>The eighth histogram that was analyzed is &amp;ldquo;loudness&amp;rdquo;. The visualization for loudness is seen in Figure 9.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-333/raw/main/project/images/loudness_histogram_before_and_after_21stcentury.png" alt="loudness">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> Loudness shows the average decibels a track is played in&lt;/p>
&lt;p>The &lt;strong>mean&lt;/strong> of the histogram assigned to &amp;ldquo;before_21st_century&amp;rdquo; is -12.60 while the &lt;strong>mode&lt;/strong> is -11.82. The &lt;strong>variance&lt;/strong> of the data is 29.17625. The histogram assigned to &amp;ldquo;after_21st_century&amp;rdquo; has a &lt;strong>mean&lt;/strong> of -7.32 and &lt;strong>mode&lt;/strong> of -4.80. The &lt;strong>variance&lt;/strong> of the data is 20.35049. The shapes of both histograms are the same, however the histogram attributed to after the 21&lt;sup>st&lt;/sup> century shifted to the right by 5.28. This displays the increase in loudness of songs created after 2001. The variance of the data shows that songs after 2001 are mostly very loud. Wheres as, songs before 2001 had greater variation of less loud to more loud.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>Music over the centuries has continuously changed. This change has typically been embraced by the masses. However, in the early 2000s adults who grew up on different styles of music began stating through word of mouth, interviews, tv shows, and movies that &amp;ldquo;music isn&amp;rsquo;t the same anymore&amp;rdquo;. They even claimed that most current songs sound the same and lack uniqueness. This scientific research set out to determine how music has changed and if in fact, modern music lacks uniqueness.&lt;/p>
&lt;p>After analyzing the songs before and after the 21&lt;sup>st&lt;/sup> century it was determined that all the features of a track have changed in some way. The danceability, energy, tempo, and loudness of a song have increased. While the acousticness, valence, and instrumentalness have decreased. The number of songs after the 21&lt;sup>st&lt;/sup> century that was created in the key of C, C#, and D has decreased. The variation of energetic, acoustic, instrumental, valent, and loud songs have decreased since 2001. While the variation of tempo and key has increased since 2001.&lt;/p>
&lt;p>The factor for determining whether music lacks uniqueness in this paper will be variance. If a feature has more than alpha = 0.01 difference of variability from before to after the 21&lt;sup>st&lt;/sup> century then it will be determined that the feature is less unique after the 21&lt;sup>st&lt;/sup> century. The difference in variances among the feature &amp;ldquo;energetic&amp;rdquo; is 0.01789, &amp;ldquo;acousticness&amp;rdquo; is 0.05242, &amp;ldquo;instrumentalness&amp;rdquo; is 0.05913, &amp;ldquo;valence&amp;rdquo; is 0.00828, and &amp;ldquo;loudness&amp;rdquo; is 8.82576. Thus, the only feature that does not lack uniqueness when compared to songs before the 21&lt;sup>st&lt;/sup> century is &amp;ldquo;valence&amp;rdquo;. Based on this information music overall after the 21&lt;sup>st&lt;/sup> century lacks uniqueness compared to music before the 21&lt;sup>st&lt;/sup> century.&lt;/p>
&lt;p>This lack of uniqueness did not start after the 21&lt;sup>st&lt;/sup> century. It started during the Enlightenment period. During this period of time, classical music was the most popular genre of music. Before this era, Baroque music was extremely popular. Artists such as Johann Sebastian Bach created complex compositions that were played for the elite. This style of music &amp;ldquo;was filled with complex melodies and exaggerated ornamentation, music of the Enlightenment period was technically simpler.&amp;rdquo; &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> Instead of focusing on these complexities the new music focused on enjoyment, pleasure, and being memorable. People now wanted to be able to hum and play songs themselves. This desired feature has caused music to constantly become simpler. Thus, reducing songs variances amongst features.&lt;/p>
&lt;p>A further step that could be taken in this research is predicting what these features will look like in the future. Machine learning could be used to make this prediction. This would give music enthusiasts and professionals a greater understanding of where music is headed and how to adapt.&lt;/p>
&lt;h2 id="51-limitations">5.1 Limitations&lt;/h2>
&lt;p>Initially this project was supposed to be conducted on Hip-Hop music. However, the way the data was collected and stored did not allow for this analysis to be done. In the future a more in depth analysis could be conducted on a specific genre.&lt;/p>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Developer.spotify.com. 2020. &lt;em>Get Audio Features For A Track | Spotify For Developers&lt;/em>. [online] Available at: &lt;a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/">https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/&lt;/a> [Accessed 6 November 2020].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Muscato, C. and Clayton, J., 2020. &lt;em>Music During The Enlightenment Period&lt;/em>. [online] Study.com. Available at: &lt;a href="https://study.com/academy/lesson/music-during-the-enlightenment-period.html">https://study.com/academy/lesson/music-during-the-enlightenment-period.html&lt;/a> [Accessed 5 November 2020].&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: NBA Performance and Injury</title><link>/report/fa20-523-301/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-301/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;ul>
&lt;li>Gavin Hemmerlein, fa20-523-301&lt;/li>
&lt;li>Chelsea Gorius, fa20-523-344&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/blob/main/project/project.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Sports Medicine will be a $7.2 billion dollar industry by 2025. The NBA has a vested interest in predicting performance of players as they return from injury. The authors evaluated datasets available to the public within the 2010 decade to build machine and deep learning models to expect results. The team utilized Gradient Based Regressor, Light GBM, and Keras Deep Learning models. The results showed that the coefficient of determination for the deep learning model was approximately 98.5%. The team recommends future work to predicting individual player performance utilizing the Keras model.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-data-transformations-and-calculations">3.1 Data Transformations and Calculations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-development-of-models">4.1 Development of Models&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#411-evaluation-metrics">4.1.1 Evaluation Metrics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-gradient-boost-regression">4.1.2 Gradient Boost Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-lightgbm-regression">4.1.2 LightGBM Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#413-keras-deep-learning-models">4.1.3 Keras Deep Learning Models&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-inference">5. Inference&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#61-limitations">6.1 Limitations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#71-work-breakdown">7.1 Work Breakdown&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> basketball, NBA, injury, performance, salary, rehabilitation, artificial intelligence, convolutional neural network, lightGBM, deep learning, gradient based regressor.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The topic to be investigated is basketball player performance as it relates to injury. The topic of injury and recovery is a multi-billion dollar industry. The Sports Medicine field is expected to reach $7.2 billion dollars by 2025 &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The scope of this effort is to explore National Basketball Association(NBA) teams, but the additional uses of a topic such as this could expand into other realms such as the National Football League, Major League Baseball, the Olympic Committees, and many other avenues. For leagues with salaries, projecting an expected return on the investment can assist in contract negotiations and cater expectations. Competing at such a high level of intensity puts these players at a greater risk to injury than the average athlete because of the intense and constant strain on their bodies. The overall valuation of the NBA in recent years is over 2 billion dollars, meaning each team is spending millions of dollars in the pursuit of a championship every season. Injuries to players can cost teams not only wins but also significant profits. Ticket sales alone for a single NBA finals game have reported greater than 10 million dollars in profit for the home team, if a team&amp;rsquo;s star player gets injured just before the playoffs and the team does not succeed, that is a lot of money lost. These injuries can have an effect no matter the time of year, regular season ticket sales have been known to fluctuate with injuries from the team&amp;rsquo;s top performers. Besides ticket sales these injuries can also influence viewership, TV or streaming, and potentially lead to a greater loss in profits. With the health of the players and so much money at stake NBA team organizations as a whole do their best to take care of their players and keep them injury free.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>The assumptions were made based on current literature as well. The injury return and limitations upon return of Anterior Cruciate Ligament (ACL) rupture (ACLR) are well documented and known. Interesting enough, forty percent of the players in the study occurred during the fourth quarter &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This leads some credence to the idea that fatigue is a major factor in the occurrence of these injuries.&lt;/p>
&lt;p>The current literature also shows that a second or third injury can occur more frequently due to minor injuries. &lt;em>&amp;ldquo;When an athlete is recovering from an injury or surgery, tissue is already compromised and thus requires far more attention despite the recovery of joint motion and strength. Moreover, injuries and surgical procedures can create detraining issues that increase the likelihood of further injury&amp;rdquo;&lt;/em> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>To compare performance and injury, a minimum of two datasets will be needed. The first is a dataset of injuries for players &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This dataset created the samples necessary for review.&lt;/p>
&lt;p>Once the controls for injuries were established, the next requirement was to establish pre-injury performance parameters and post-injury parameters. These areas were where the feature engineering took place. The datasets needed had to include appropriate basketball performance stats to establish a metric to encompass a player&amp;rsquo;s performance. One example that ESPN has tried in the past is the Player Efficiency Rating (PER). To accomplish this, it was important to review player performance within games such as in the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> dataset because of how it allowed the team to evaluate the player performance throughout the season, and not just the average stats across the year. In addition to that the data from the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> dataset was valuable in order to compare the calculated performance metrics just before an injury or after recovery to the player&amp;rsquo;s overall performance that season or in seasons prior. That comparison provided a solid baseline to understand how injuries can effect a player&amp;rsquo;s performance. With in depth information about each game of the season, and not just the teams and players aggregated stats, added to the data provided from the injury dataset &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> the team was be able to compose new metrics to understand how these injuries are actually affecting the players performance.&lt;/p>
&lt;p>Along the way attempted to discover if there is also a causal relationship to the severity of some of the injuries, based on how the player was performing just before the injury. The term &lt;em>load management&lt;/em> has become popular in recent years to describe players taking rest periodically throughout the season in order to prevent injury from overplaying. This new practice has received both support for the player safety it provides and also criticism around players taking too much time off. Of course not all injuries are entirely based on the recent strain under the players body, but a better understanding about how that affects the injury as a whole could give better insight into avoiding more injuries. It is important to remember though that any pattern identification would not lead to an elimination of all injuries, any contact sport will continue to have injuries, especially one as high impact as the NBA. There is value to learn from why some players are able to return from certain injuries more quickly and why some return to almost equivalent or better playing performance than before the injury. This comparison of performance was attempted by deriving metrics based on varying ranges of games immediately leading up to injury and then immediately after returning from injury. In addition to that performed comparisons to the players known peak performance to better understand how the injury affected them. Another factor that was important to include is the length of time recovering from the injury. Different players take differing amounts of time off, sometimes even with similar injuries. Something will be said about the player’s dedication to recovery and determination to remain at peak performance, even through injury, when looking at how severe their injury was, how much time was taken for recovery, and how they performed upon returning.&lt;/p>
&lt;p>These datasets were chosen because they allow for a review of individual game performance, for each team, throughout each season in the recent decade. Aggregate statistics such as points per game (ppg) can be deceptive because duration of the metric is such a large period of time. The large sample of 82 games can lead to a perception issue when reviewing the data. These datasets include more variables to help the team determine effects to player injury, such as minutes per game (mpg) to understand how strenuous the pre-injury performance or how fatigue may have played a factor in the injury. Understanding more of the variables such as fouls given or drawn can help determine if the player or other team seemed to be the primary aggressor before any injury.&lt;/p>
&lt;h3 id="31-data-transformations-and-calculations">3.1 Data Transformations and Calculations&lt;/h3>
&lt;p>Using the Kaggle package the datasets were downloaded direct from the website and unzipped to a directory accessible by the ‘project_dateEngineering.ipynb’ notebook. The 7 unzipped datasets are then loaded into the notebook as pandas data frames using the ‘.read_csv()’ function. The data engineering performed in the notebook includes removal of excess data and data type transformations across almost all the data frames loaded. This data transformation includes transforming the games details column ‘MIN’, meaning minutes played, from a timestamp format to a numerical format that could have calculations like summation or average performed on it. This was a crucial transformation since minutes played have a direct correlation to player fatigue, which can increase a player’s chance of injury.&lt;/p>
&lt;p>One of the more difficult tasks was transforming the Injury dataset into something that would provide more information through machine learning and analysis. The dataset is loaded as one data set where 2 columns ‘Relinquished’ and ‘Acquired’ defined if the row in questions was a player leaving the roster due to injury or returning from injury, respectively. In this case for each for one of those two columns contained a players name and the other was blank. Besides that the data frame contained information like the date, notes, and the team name. In order to appropriately understand each injury as whole the data frame needs to be transformed into one where each row contains the player, the start date of the injury, and the end date of the injury. In order to do this first the original Injury dataset was separated into rows marking the start of an injury and those marking the end of an injury. Data frames from the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> data set were used to join TeamID and PlayerID columns to the Injury datasets. An ‘iterrows():’ loop was then used on the data frame marking the start of an injury to specifically locate the corresponding row in the Injury End data frame with the same PlayerID and where the return date was the closest date after the injury date. As this new data frame was being transformed, it was noted that sometimes a Player would have multiple rows with the same Injury ending date but different injury start dates, this can happen if an injury worsens or the player did not play due to last minute decision. In order to solve this the table was grouped by the PlayerID and InjuryEnd Date while keeping the oldest Injury Start date, since the model will want to see the full length of the injury. From there it was simple to calculate the difference in days for each row between the Injury start and end dates. This data frame is called ‘df_Injury_length’ in the notebook and is much easier to use for improved understanding of NBA injuries than the original format of the Injury data set.&lt;/p>
&lt;p>Once created, the ‘df_Injury_length’ data frame was copied and built upon. Using ‘iterrows():’ loop again to filter down the games details data frame rows with the same PlayerId, over 60 calculated columns are created to produce the ‘df_Injury_stats’ data frame. The data frame includes performance statistics specifically from the game the player was injured and the game the player returned from that injury. In addition to this aggregate performance metrics were calculated based on the 5 games prior to the injury and the 5 games post returning from injury. At this time the season of when the injury occurred and when the player returned is also stored in the dataframe. This will allow comparisons between the ‘df_Injury_stats’ data frame and the ‘df_Season_stats’ data frame which contains the players average performance metrics for entire seasons.&lt;/p>
&lt;p>A few interesting figures were generated within the Exploratory Data Analysis (EDA) stage. &lt;strong>Figure 1&lt;/strong> gave a view of the load of the player returning from injury. The load to the player will show how recovered the player is upon completion of rehab. Many teams decide to slowly work a returning player in. Additionally, the amount of time for an injury can be seen on this graph. The longer the injury, the more unlikely the player will return to action.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/avg_min_played_post5.png" alt="Average Minutes Played in First Five Games Upon Return over Injury Length in Days">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Average Minutes Played in First Five Games Upon Return over Injury Length in Days*&lt;/p>
&lt;p>&lt;strong>Figure 2&lt;/strong> shows the frequency in which a player is injured. The idea behind this graph is to see a relationship between the time leading up to the injury. Interesting enough, there is no key indication of where injury is more likely to occur. It can be assumed that there is a rarity of players who see playing time greater than 30 minutes. The histogram only shows a near flat relationship; which was surprising.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/frequencies_by_average_minutes.png" alt="Frequency of Injuries by Average Minutes Played in Prior Five Games">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Frequency of Injuries by Average Minutes Played in Prior Five Games*&lt;/p>
&lt;p>&lt;strong>Figure 3&lt;/strong> shows the length of injury over number of injuries. By reviewing this data, it can be seen that most injuries occur fewer rather than more often. A player that is deemed injury prone will be a lot more likely to be cut from the team. This data makes sense.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injury_length.png" alt="Injury Length in Days over Number of Injuries">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Injury Length in Days over Number of Injuries&lt;/p>
&lt;p>&lt;strong>Figure 4&lt;/strong> shows the injury length over average minutes played in the five games before injury. This graph attempts to show all of the previous games and the impacts to the players injury. The data looks evenly distributed, but the majority of plaers do not play close to 40 minutes per game. By looking at this data, it shows that minutes played does likely contribute to the injury severity.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injury_length_over_avg_min.png" alt="Injury Length in Days over Avg Minutes Played in Prior 5 Games">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Injury Length in Days over Avg Minutes Played in Prior 5 Games&lt;/p>
&lt;p>&lt;strong>Figure 5&lt;/strong> shows that in general the number of games played does not have a significant relationship to the length of the injury. There is a darker cluster between 500-1000 days injured that exists over the 40-82 games played, this could suggest that as more games are played there is likeliness for more severe injury.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_gamesplayed.png" alt="Injury Length in Days over Player Games Played that Season">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Injury Length in Days over Player Games Played that Season&lt;/p>
&lt;p>&lt;strong>Figures 6&lt;/strong>, &lt;strong>Figure 7&lt;/strong>, and &lt;strong>Figure 8&lt;/strong> attempt to demonstrate if any relationship exists visually between a player&amp;rsquo;s injury length and their age, weight, or height. For the most part &lt;strong>Figure 6&lt;/strong> shows most severe injuries occurring to younger players, which could make sense considering they can perform more difficult moves or have more stamina than older players. Some severe injuries still exist among the older players, this also makes sense considering their bodies have been under stress for many years and are more prone to injury. It should be noted that there are more players in the league that fall into the younger age bucket than the older ages. It is difficult to identify any pattern on &lt;strong>Figure 7&lt;/strong>. If anything the graph is somewhat normally shaped similar to the heights of players across the league. Suprisingly the injuries on &lt;strong>Figure 8&lt;/strong> are clustered a bit towards the left, being the lighter players. This could be explained through the fact that the lighter players are often more athletic and perform more strenuous moves than heavier players. It is also somewhat surprising since the argument that heavier players are putting more strain on their bodies could be used as a reason why heavier players would have worse injuries. One possible explanation could be the musculature adding more of the dense body mass could add protection to weakened joints. More investigation would be needed to identify an exact reason.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerage.png" alt="Injury Length in Days over Player Age that Season">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Injury Length in Days over Player Age that Season&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerHeight.png" alt="Injury Length in Days over Player Height in Inches">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Injury Length in Days over Player Height in Inches&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerWeight.png" alt="Injury Length in Days over Player Weight in Kilograms">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Injury Length in Days over Player Weight in Kilograms&lt;/p>
&lt;p>Finally, the team decided to use the z-score to normalize all of the data. By using the Z-score from the individual data in a column of df_Injury_stats, the team was able to limit variability of multiple metrics across the dataframe. A player&amp;rsquo;s blocks and steals should be a miniscule amount compared to minutes or points of some players. The same can be said of assists, technical fouls, or any other statistic in the course of an NBA game. The Z-score, by nature of the metric from the mean, allows for much less variability across the columns.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;p>The objective of this project was to develop performance indicators for injured players returning to basketball in the NBA. It is unreasonable to expect a player to return to the same level of play post injury immediately upon starting back up after recovery. It often takes a player months if not years to return to the same level of play as pre-injury, especially considering the severity of the injuries. In order to successfully analyze this information from the datasets, a predictive model will need to be created using a large set of the data to train.&lt;/p>
&lt;p>From this point, a test run was used to gauge the validity and accuracy of the model compared to some of the data set aside. The model created was able to provide feature importance to give a better understanding of which specific features are the most crucial when it comes to determining how bad the effects of an injury may or may not be on player performance. Feature engineering was performed prior to training the model in order to improve the chances of higher accuracy from the predictions. This model could be used to keep an eye out for how a player&amp;rsquo;s performance intensity and the engineered features could affect how long a player takes to recover from injury, if there are any warning signs prior to an injury, and even how well they perform when returning.&lt;/p>
&lt;h3 id="41-development-of-models">4.1 Development of Models&lt;/h3>
&lt;p>To help with review of the data, conditioned data was used to save resources on Google Colab. By conditioning the data and saving the files as a .CSV, the team was able to create a streamlined process. Additionally, the team found benefit by uploading these files to Google Drive to quickly import data near real time. After operating in this fashion for some time, the team was able to load the datasets into Github and utilize that feature. By loading the datasets up to Github, a url could be used to link the files directly to the files saved on Github without using a token like with Kaggle or Google Drive. The files saved were the following:&lt;/p>
&lt;p>&lt;strong>Table 1:&lt;/strong> Datasets Imported&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Dataframe&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Title&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">1.&lt;/td>
&lt;td style="text-align:center">df_Injury_stats&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">2.&lt;/td>
&lt;td style="text-align:center">df_Injury_length&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">3.&lt;/td>
&lt;td style="text-align:center">df_Season_stats&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">4.&lt;/td>
&lt;td style="text-align:center">games&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">5.&lt;/td>
&lt;td style="text-align:center">df_Games_gamesDetails&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">6.&lt;/td>
&lt;td style="text-align:center">injuries_2010-2018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">7.&lt;/td>
&lt;td style="text-align:center">players&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">8.&lt;/td>
&lt;td style="text-align:center">ranking&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">9.&lt;/td>
&lt;td style="text-align:center">teams&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Every time Google Colab loads data, it takes time and resources. The team was able to utilize the cross platform connectivity of the Google utilities. The team could then focus on building models as opposed to conditioning data every time the code was ran.&lt;/p>
&lt;h4 id="411-evaluation-metrics">4.1.1 Evaluation Metrics&lt;/h4>
&lt;p>The metrics chosen were designed to give results on Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and the Explained Variance (EV) Score. MAE is a measure of errors between paired observations experiencing the same expression. RMSE is the standard deviation of the prediction errors for our dataset. EV is the relationship between the train data and the test data. By using these metrics, the team is capable of reviewing the data in a statistical manner.&lt;/p>
&lt;h4 id="412-gradient-boost-regression">4.1.2 Gradient Boost Regression&lt;/h4>
&lt;p>The initial model that was used was a Gradient Boosting Regressor (GBR) model. This model produced the results shown in Table 2. The GBR model builds in a stage-wise fashion; similarly to other boosting methods. GBR also generalizes the data and attempts to optimize the results utilizing a loss function. An example of the algorithm can be seen in &lt;strong>Figure 5&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/gbr.png" alt="Gradient Boosting Regressor">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Gradient Boosting Regressor &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The team saw a relationship given the data. &lt;strong>Table 2&lt;/strong> shows the results of that model. The results were promising given the speed and utility of a GBR model. The team reviewed the data multiple times after multiple stages of conditioning the data.&lt;/p>
&lt;p>&lt;strong>Table 2:&lt;/strong> GBR Results&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Category&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Value&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MAE Mean&lt;/td>
&lt;td style="text-align:center">-10.787&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MAE STD&lt;/td>
&lt;td style="text-align:center">0.687&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE Mean&lt;/td>
&lt;td style="text-align:center">-115.929&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE STD&lt;/td>
&lt;td style="text-align:center">96.64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV Mean&lt;/td>
&lt;td style="text-align:center">1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV STD&lt;/td>
&lt;td style="text-align:center">0.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>After running a GBR model, the decision was made to try multiple models to see what gives the best results. The team settled on LightGBM and a Deep Learning model utilizing Keras built on the TensorFlow platform. These results will be seen in &lt;em>4.1.2&lt;/em> and &lt;em>4.1.3&lt;/em>.&lt;/p>
&lt;h4 id="412-lightgbm-regression">4.1.2 LightGBM Regression&lt;/h4>
&lt;p>Another algorithm chosen was a Light Gradient Boost Machine (LightGBM) model. LightGBM is known for its lightweight and resource sparse abilities. The model is built from decision tree algorithms and used for ranking, classification, and other machine learning tasks. By choosing LightGBM data scientists are able to analyze larger data a faster approach. LightGBM can often over fit a model if the data is too small, but fortunately for the purpose of this assignment the data available for NBA injuries and stats is extremely large. Availability of data allowed for smooth operation of the LightGBM model. Mandot explains the model really well in The Medium. Mandot said, &lt;em>&amp;ldquo;Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development&amp;rdquo;&lt;/em> &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. There are a lot of benefits available to this algorithm.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/lightGBM_regressor.png" alt="LightGBM Algorithm: Leafwise searching">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> LightGBM Algorithm: Leafwise searching &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>&lt;/p>
&lt;p>When running the model &lt;strong>Table 3&lt;/strong> was generated. This table uses the same metrics as the GBR Results Table (&lt;strong>Table 2&lt;/strong>). After reviewing the results, the GBR model still appeared to be a viable avenue. The Keras model will be evaluated next to see most optimal model to use for repeatable fresults.&lt;/p>
&lt;p>&lt;strong>Table 3:&lt;/strong> LightGBM Results&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Category&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Value&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MAE Mean&lt;/td>
&lt;td style="text-align:center">-0.011&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MAE STD&lt;/td>
&lt;td style="text-align:center">0.001&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE Mean&lt;/td>
&lt;td style="text-align:center">-0.128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE STD&lt;/td>
&lt;td style="text-align:center">0.046&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV Mean&lt;/td>
&lt;td style="text-align:center">0.982&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV STD&lt;/td>
&lt;td style="text-align:center">0.013&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="413-keras-deep-learning-models">4.1.3 Keras Deep Learning Models&lt;/h4>
&lt;p>The final model attempted was a Deep Learning model. A few runs of different layers and epochs were chosen. They can be seen in &lt;strong>Table 4&lt;/strong> (shown later). The model was sequentially ran through the test layers to refine the model. When this is done, each predecessor layer acts as an input to the next layer&amp;rsquo;s input for the model. The results can produce accurate results while using unsupervised learning. The visualization for this model can be seen in the following figure:&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/simple_neural_network_vs_deep_learning.jpg" alt="Neural Network">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Neural Network &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>&lt;/p>
&lt;p>When the team ran the Neural Networks, the data went through three layers. Each layer was built upon the previous similarly to the figure. This allowed for the team to capture information from the processing. &lt;strong>Table 4&lt;/strong> shows the results for the deep learning model.&lt;/p>
&lt;p>&lt;strong>Table 4:&lt;/strong> Epochs and Batch Sizes Chosen&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Number&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Regressor Epoch&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Regressor Batch Sizes&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>KFolds&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Model Epochs&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>R2&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">&lt;em>1.&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>25&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>25&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>10&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>10&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>0.985&lt;/em>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">2.&lt;/td>
&lt;td style="text-align:center">40&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.894&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">3.&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.966&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">4.&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.707&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">5.&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">0.611&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">6.&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">0.982&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The team has decided that the results for the Deep Learning are the most desirable. This model would be the one that the team would recommend based on the results from the metrics available. The parameters the team recommends are italicized in &lt;em>Line 1&lt;/em> of &lt;strong>Table 4&lt;/strong>.&lt;/p>
&lt;h2 id="5-inference">5. Inference&lt;/h2>
&lt;p>With the data available, some conclusions can be made. Not all injuries are of the same severity. By treating an ACL tear in the same manner as a bruise, the team doctors would take terrible approaches to rehab. The severity of the injury is a part of the approach to therapy. This detail is nearly impossible to capture in the model.&lt;/p>
&lt;p>Another aspect to come to a conclusion is that not every player recovers in the same timetable as another. Genetics, diet, effort, and mental health can all harm or reinforce the efforts from the medical staff. These areas are hard to capture in the data and cannot be appropriately reviewed with this model.&lt;/p>
&lt;p>It is also difficult to indicate where a previous injury may have contributed to a current injury. The kinetic chain is a structure of the musculoskeletal system that moves the body using the muscles and bones. If one portion of the chain is compromised, the entire chain will need to be modified to continue movement. This modification can result in more injuries. The data cannot provide this information. It is important to remember these possible confounding variables when interpreting the results of the model.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>After reviewing the results, the team created a robust model to predict the performance of a player after an injury. The coefficient of determination for the deep learning model shows a strong relationship between the training and test sets. After conditioning the data, the results can be seen in &lt;strong>Table 2&lt;/strong>, &lt;strong>Table 3&lt;/strong>, and &lt;strong>Table 5&lt;/strong>. The team had an objective to find this correlation and build it to the point where injury and performance can be modeled. The team was able to accomplish this goal.&lt;/p>
&lt;p>Additionally, these results are consistent with the current scientific literature &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The biological community has been able to record these results for decades. By leveraging this effort, the scientific community could move to a more proactive approach as opposed to reactive with respect to injury controls. This data will also allow for proper contract negotiations to take place in the NBA, considering potential decisions to avoid injury may include less playing time. The negotiations are pivotal to ensuring that expectations are met in the future seasons; especially when injury occurs in the final year of a player&amp;rsquo;s contract. Teams with an improved understanding of how players can or will return from injury have an opportunity to make the best of scenarios where other teams may be hesitant to sign an injured player. These different opportunities for a team&amp;rsquo;s front office could be the difference between a championship ring and missing the playoffs entirely.&lt;/p>
&lt;h2 id="61-limitations">6.1 Limitations&lt;/h2>
&lt;p>With respect to the current work, the models could be continued to be refined. Currently the results are to the original intentions of the team, but improvements can be made. Feature Engineering is always an area where the models can improve. Some valuable features to be created in the future are the calculations for the player&amp;rsquo;s efficiency overall, as well as offensinve and defensive efficiencies in each game. The team would also like to develop a model to use the stats of a player in pre-injury and apply that to the post-injury set of metrics. Also, the team would like to move to where the same could be applied given the length of the injury to the player while considering the severity of the injury. Longer and more severe injury will lead to different future results than say a long not severe injury, or a short injury that was somewhat severe. The number of varaibles that could provide more valuable information to the model are endless.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The authors would like to thank Dr. Gregor von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article. In addition to that the community of students from the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course also deserve a thanks from the author for the support, continued engagement, and valuable discussions through Piazza.&lt;/p>
&lt;h3 id="71-work-breakdown">7.1 Work Breakdown&lt;/h3>
&lt;p>For the effort developed, the team split tasks between each other to cover more ground. The requirements for the investigation required a more extensive effort for the teams in the ENGR-E 534 class. To accomplish the requirements, the task was expanded by addressing multiple datasets within the semester and building in multiple models to display the results. The team members were responsible for committing in Github multiple times throughout the semester. The tasks were divided as follows:&lt;/p>
&lt;ol>
&lt;li>Chelsea Gorius
&lt;ul>
&lt;li>Exploratory Data Analysis&lt;/li>
&lt;li>Feature Engineering&lt;/li>
&lt;li>Keras Deep Learning Model&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Gavin Hemmerlein
&lt;ul>
&lt;li>Organization of Items&lt;/li>
&lt;li>Model Development&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Both
&lt;ul>
&lt;li>Report&lt;/li>
&lt;li>All Outstanding Items&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>A. Mehra, &lt;em>Sports Medicine Market worth $7.2 billion by 2025&lt;/em>, [online] Markets and Markets.
&lt;a href="https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp">https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&lt;/a> [Accessed Oct. 15, 2020].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>J. Harris, B. Erickson, B. Bach Jr, G. Abrams, G. Cvetanovich, B. Forsythe, F. McCormick, A. Gupta, B. Cole,
&lt;em>Return-to-Sport and Performance After Anterior Cruciate Ligament Reconstruction in National Basketball Association Players&lt;/em>, Sports Health. 2013 Nov;5(6):562-8. doi: 10.1177/1941738113495788. [Online serial]. Available: &lt;a href="https://pubmed.ncbi.nlm.nih.gov/24427434">https://pubmed.ncbi.nlm.nih.gov/24427434&lt;/a> [Accessed Oct. 24, 2020].&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>W. Kraemer, C. Denegar, and S. Flanagan, &lt;em>Recovery From Injury in Sport: Considerations in the Transition From Medical Care to Performance Care&lt;/em>, Sports Health.
2009 Sep; 1(5): 392–395.[Online serial]. Available: &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445177">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445177&lt;/a> [Accessed Oct. 24, 2020].&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>R. Hopkins, &lt;em>NBA Injuries from 2010-2020&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/ghopkins/nba-injuries-2010-2018">https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&lt;/a> [Accessed Oct. 9, 2020].&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>N. Lauga, &lt;em>NBA games data&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv">https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&lt;/a> [Accessed Oct. 9, 2020].&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>J. Cirtautas, &lt;em>NBA Players&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/justinas/nba-players-data">https://www.kaggle.com/justinas/nba-players-data&lt;/a> [Accessed Oct. 9, 2020].&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>V. Aliyev, &lt;em>A hands-on explanation of Gradient Boosting Regression&lt;/em>, [online] Medium. &lt;a href="https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e">https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e&lt;/a> [Accessed Nov., 9 2020].&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>P. Mandon, &lt;em>What is LightGBM, How to implement it? How to fine tune the parameters?&lt;/em>, [online] Medium. &lt;a href="https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc">https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc&lt;/a> [Accessed Nov., 9 2020].&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>The Data Scientist, &lt;em>What deep learning is and isn’t&lt;/em>, [online] The Data Scientist. &lt;a href="https://thedatascientist.com/what-deep-learning-is-and-isnt">https://thedatascientist.com/what-deep-learning-is-and-isnt&lt;/a> [Accessed Nov., 9 2020].&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: NFL Regular Season Skilled Position Player Performance as a Predictor of Playoff Appearance Overtime</title><link>/report/fa20-523-308/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-308/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-308/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-308/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Travis Whitaker, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-308">fa20-523-308&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-308/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The present research investigates the value
of in-game performance metrics for NFL skill position players (i.e., Quarterback, Wide Receiver, Tight End, Running Back and Full Back) in predicting post-season qualification. Utilizing nflscrapR-data that collects all regular season in-game performance metrics between 2009-2018, we are able to analyze the value of each of these in-game metrics by including them in a regression model that explores each variables strength in predicting post-season qualification. We also explore a comparative analysis between two time periods in the NFL (2009-2011 vs 2016-2018) to see if there is a shift in the critical metrics that predict post-season qualification for NFL teams. Theoretically, this could help inform the debate as to whether there has been a shift in the style of play in the NFL across the previous decade and where those changes may be taking place according to the data. Implications and future research are discussed.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-results">5. Results&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#inference">Inference&lt;/a>&lt;/li>
&lt;li>&lt;a href="#preliminary-results">Preliminary Results&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#2009-2011-skill-position-player-performance-as-playoff-predictor">2009-2011 Skill Position Player Performance as Playoff Predictor&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2016-2018-skill-position-player-performance-as-playoff-predictor">2016-2018 Skill Position Player Performance as Playoff Predictor&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#comparative-results">Comparative Results&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-discussion">6. Discussion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#limitations-and-future-research">Limitations and Future Research&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgements">8. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ANOVA, Comparative Analysis, Exploratory Analysis, Football, Sports, Metrics&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>In the modern NFL the biggest negotiating tools for players in signing a new contract is their on-field performance. Many players choose to &amp;ldquo;hold-out&amp;rdquo; of pre-season practice or regular season games as a negotiating tool in their attempt to sign a more lucrative contract. In this situation players feel as though their exceptional performance on the field is not reflected in the monetary compensation structure of their contract. This is most often reflected in skill position players such as wide receivers or running backs whose play on the field is most often celebrated (e.g., touchdowns) and discussed by fans of the game. While these positions are no doubt important to a team’s success, the question remains how important is one players contribution to a team’s overall success? The current project will attempt to evaluate the importance of skill position players' (i.e., Quarterback (QB), Wide Receiver (WR), Running Back (RB), and Tight End (TE)) performance during the regular season and use in-game performance metrics as a predictive factor for their team making the playoffs. This is an attempt to answer the question can qualifying for the post-season be predicted by skill position metrics measured during the regular season? If so, then which metrics are most crucial to predicting post-season qualification?&lt;/p>
&lt;p>A secondary analysis in this project will look at a comparison between 2009-2011 vs. 2016-2018 regular season metrics and build separate models for each three year span to investigate whether a shift in performance metric importance has occurred over the past decade. NFL insiders and journalists have noted the shift in play-calling over the past decade in the NFL as well as the change at the quarterback position to more &amp;ldquo;dual-threat&amp;rdquo; (running and throwing) quarterbacks that have transitioned the game to a more &amp;ldquo;aggressive&amp;rdquo; play-style &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Yet punditry and reporting have not always been supported by performance metrics and this specific claim of a transition over the past decade needs some exploring. Therefore, we will be investigating whether there has been a shift in the performance metrics that are important in predicting team success. Again, team success will be measured by making the post-season.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>There are many playoff predictor models that focus on team performance or team wins vs losses as a predictor of making the playoffs &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. However, few take into consideration individual player performance as an indicator of their team making the post-season playoffs in the NFL. The most famous model that takes into consideration player performance is ELO rating &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The first ELO rating was a straightforward model that took head-to-head results and player-vs-team model to predict win probability in an NFL game. However, in 2019 Silver and his team at FiveThirtyEight updated their ELO model to give a value rating to the quarterback position for each team. This quarterback value included metrics such as pass attempts, completions, passing yards, passing touchdowns, interceptions, sacks, rush attempts, rushing yards, and rushing touchdowns. Taking these metrics along with the defensive quality metrics, which is an adjustment of quarterback value based on the opposing defense ranking, gives you an overall value for your quarterback. Thus, this widely accepted model takes head-to-head team comparisons on a week-to-week basis and includes the quarterback value in predicting the winners of these head-to-head matchups. However, no model has taken just player performance and tried to predict team success for an entire regular season based on each of their individual players. These previous models primarily look at offensive vs. defensive units and try to predict win/loss records based off each of these units.&lt;/p>
&lt;p>The goal of the present research is not to compare our model vs previous models, as these standing models are not meant for playoff prediction, rather these previous models are used for a game-by-game matchup comparison. The present research investigates whether looking at position players at each of the skilled positions, maps onto predicting the post-season qualifying teams. Further, how does this predictive model change over time? Looking at 2009-2011 NFL skill player performance vs 2016-2018 skill player performance we will investigate if there are differences in metrics that predict post-season appearance. This will show us whether there are shifts in skill position play that impact predicting post-season playoff appearance. Specifically, by comparing the two models which use two different periods of time (i.e., 2009-2011 vs 2016-2018) we will be able to better investigate specific metrics at each position that are important for predicting success. For example, if the wide receiver position is important, what is most important about that position, yards-after-catch or number of receptions? Further, how might the importance of those metrics shift over time? We hope to explore and understand better the impact of skill players and the metrics that they are measured on in terms of making the post-season.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>The datasets used are in a github folder that holds nflscrapR-data that originates from NFL.com &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The folder includes play-by-play data, including performance measures, for all regular season games from 2009 to 2019. This file was paired with week-by-week regular season roster data for each team in the NFL. This allowed us to track skilled position player performance during the regular season and then compare this regular season file with the files that contain playoff teams for each year from 2009-2019. Supplemental data was pulled from Pro-Football-Reference.com &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;p>The first step we took to understand the data was to use various slices of the data put into scatterplots and bar charts to find trends, as well as various time series charts. This was an exploratory step in understanding the data.&lt;/p>
&lt;p>Then each metric from player performance during the regular season was included in the analysis or models that were built to predict post-season appearance. Post-season appearance is a designation for a team qualifying for the post-season or playoffs. We think it is important to engineer some new features to potentially provide insights. For instance, it is possible to determine whether a play was during the final two minutes of a half and if a play was in the red zone. During these critical points of a game a win or lose is often determined. Our thought is by weighing these moments and performance metrics with more importance the model will better predict a team’s likelihood of making the playoffs. Another secondary metric that may strengthen the predictive ability of the model would be to use Football Outsider’s Success Rate, which is a determination of a play’s success rate for the offense that is on the field &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. This can also provide me with the down and distance to go for the offense and players that are on the field. We will also use college position designations as way to normalize the positions performance across teams. Many NFL teams utilize different player sets. Thus, it is important to use a standard, which college football uses across all teams. Since we are only interested in skill position players this will include Wide Receiver (WR), Running Back (RB), Full Back (FB), Quarterback (QB), and Tight End (TE). These designations will allow the model to compare player performance by position.&lt;/p>
&lt;p>After breaking down the data into key categorical variables to see if there was an impact for these performance variables in making the playoffs for the NFL teams. These individual position statistics were analyzed as a group and then separated into &amp;ldquo;Post Season&amp;rdquo; meaning the player&amp;rsquo;s team qualified for the playoffs, or &amp;ldquo;No Post&amp;rdquo; meaning the player&amp;rsquo;s team did not qualify for the playoffs. By doing this we were able to verify that a reasonable number of players fell into the &amp;ldquo;Post Season&amp;rdquo; group, as only 12 out of 32 teams qualify for the post-season. Further we were able to use these designations in the next step of modeling, where the data was analyzed in an ANOVA to see how important each metric was in predicting post-season appearance for each player.&lt;/p>
&lt;p>Metric measurement needs to be consistent across years. A comparison of year-to-year metrics was completed comparing each years measurements from 2009-2011 and 2016-2018 in order to make sure that the measurement techniques were stable and do not vary across time. If there were changes in the way metrics are measured than either that year will need to be dropped from the model or adjustments will need to be made to the metric to balance it with the other years included in the model. Luckily, there were no variation in metric measurement across years, so all measurements initially included in the model were kept.&lt;/p>
&lt;p>Finally, once all metrics were balanced and the team performance metrics had been aggregated. The ANOVA model was built to analyze metric measurement as a predictor of a player making post-season play. This ANOVA model was created twice, once for the 2009-2011 players and then again for the 2016-2018 players. Once this analysis was run, we were able to see the strength of the model in predicting playoff appearance by player, based on metric measurement.&lt;/p>
&lt;h2 id="5-results">5. Results&lt;/h2>
&lt;h3 id="inference">Inference&lt;/h3>
&lt;p>An individual player-performance model for NFL skill position players (i.e., quarterback, wide receiver, tight end, and running back) will perform better than chance level (50%) of identifying playoff teams from the NFL during the season&amp;rsquo;s of 2009-2011 and 2016-2018. Further, an exploratory analysis of time periods (2009-2011 vs 2016-2018) will expose differences in the key metrics that are used to predict playoff appearance over time. This will give us a better understanding of the shifts in performance in the NFL at each skill position.&lt;/p>
&lt;h3 id="preliminary-results">Preliminary Results&lt;/h3>
&lt;p>The descriptive statistics for the player performance revealed no issues with normality or different metric standards across seasons for player performance measurements. Figure 1 represents a count check for players qualifying for the post-season in the seasons of 2009-2011. Based on the NFL post-season structure 12 out of 32 teams qualify for the post-season, or 37.5%. Figure 1 shows that roughly 1/3 of players qualify for the post season at each position. However, it is important to note that each team structure and roster is different. For example, one team may carry 7 receivers, 2 running backs, 2, quarter backs, 2 tight ends, and 0 full backs, where another team may carry 4 receivers, 4 running backs, 3 quarter backs, 4 tight ends, and 1 full back. This is an important distinction to make because the &amp;ldquo;Post Season&amp;rdquo; players shown in figure 1 are not at an equal percentage across position.&lt;/p>
&lt;h4 id="2009-2011-skill-position-player-performance-as-playoff-predictor">2009-2011 Skill Position Player Performance as Playoff Predictor&lt;/h4>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Player_summary_2009_2011.png" alt="Player Qualifying for Post-Season">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Breakdown of Players by Skill Position That Qualified for Post-Season Play (2009-2011)&lt;/p>
&lt;p>We also wanted to investigate whether play-count at each position was balanced across post-season players and players who did not qualify for the post-season. Figure 2 shows that players on teams who did qualify for the post-season were involved in more plays at their position than players at their position who did not qualify for the playoffs. Thinking about this finding as a result of the regular season, players in skilled positions on post-season qualifying teams play on offenses that won more games than teams who did not qualify for the playoffs. While we did not look at time of possession for players by position, it seems fairly reasonable through logic and the results in figure 2 that play count is higher because these teams are more successful and the players on post-season qualifying teams are on the field more than teams with more losses who did not qualify for the post-season.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Play_count_position_2009.png" alt="Count of Play-type by Post-Season Qualification category">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Count of Play-type by Post-Season Qualification category (2009-2011)&lt;/p>
&lt;p>Figure 3 below is a reference table for the features included in the ANOVA regression model determining the key features that predict post-season qualification. These features are used for reference in figures 4 and 7.&lt;/p>
&lt;p>Using the player performance metrics for the regular season, an ANOVA was run to see if these metrics placed together would be a successful predictor of post-season qualification. Figure 4 shows the top 10 metrics that had the highest f-values for predicting post-season qualification, all of which were all significant (p&amp;lt;.05) in the model. Reviewing the model, the most significant factors for predicting post-season qualification for teams in order were; 1. Successful Reception (wide receiver or tight end), 2. Total Receiving Yards (Wide Receiver or Tight End), 3. Yards After Catch (wide receiver or tight end), 4. Total Receiving Touchdowns (Wide Receiver or Tight End), 5. Total Touchdowns (All positions), 6. Receiver Plays (Wide Receiver), 7. Redzone Plays (All positions), 8. Successful Plays (All positions), 9. Yards Gained (All positions), 10. Total Offensive Plays in the 3rd Quarter (All positions). The model accounted for 78.2% of variance. The model successfully accounted for predicting post-season qualifying teams in 78.2% of instances.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/figure_description_table.png" alt="Feature Descriptions">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Description for top features included in ANOVA regression model&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_pvalue_table_2009.png" alt="ANOVA for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> ANOVA Table for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2009-2011)&lt;/p>
&lt;h4 id="2016-2018-skill-position-player-performance-as-playoff-predictor">2016-2018 Skill Position Player Performance as Playoff Predictor&lt;/h4>
&lt;p>We paralleled our analysis from the 2009-2011 analysis above in completing the 2016-2018 analysis represented in Figures 5-7. Figure 5 represents the player count that qualified for post-season play (orange) and the non-post-season players (blue). Again, player count was compared to the roughly 37.5% rate that should be expected for 12 teams out of 32 qualifying for post-season play. However, the rates were a bit below the 37.5% rate. This can be explained by the number of injuries and roster changes that occur throughout the season. Teams shuffle in-and-out players at each position based on injury or performance. Teams will not have a static roster throughout the season, this includes post-season teams who cut players or put players on IR. These players, even though they played for post-season qualifying teams, would be in blue because they are not on the post-season rosters for the teams who qualify for post-season play. This along with the roster structure described for figure 1 explains the lower than 1/3 rate of players qualifying for post-season play.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Player_summary_2016-2018.png" alt="Player Qualifying for Post-Season">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Breakdown of Players by Skill Position That Qualified for Post-Season Play (2016-2018)&lt;/p>
&lt;p>Figure 6 investigates play-count at each position, like figure 2, but this time for the seasons of 2016-2018. Again, the analysis was balanced across post-season players and players who did not qualify for the post-season. Figure 6 shows that players on teams who did qualify for the post-season were involved in more plays at their position than players at their position who did not qualify for the playoffs. Figure 6 shows a consistent pattern with figure 2. Descriptive statistic comparison between the 2009-2011 seasons and the 2016-2018 seasons will be revisited in the discussion section.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Play_Count_Position_2016.png" alt="Count of Play-type by Post-Season Qualification category">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Count of Play-type by Post-Season Qualification category (20016-2018)&lt;/p>
&lt;p>Using the player performance metrics for the regular season, an ANOVA was run to see if these metrics placed together would be a successful predictor of post-season qualification. Figure 7 shows the top 10 metrics that had the highest f-values for predicting post-season qualification, all of which were significant (p&amp;lt;.05) in the model. The model successfully accounted for predicting post-season qualifying teams in 77.8% of instances.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_pvalue_table_2016.png" alt="ANOVA for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> ANOVA Table for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2016-2018)&lt;/p>
&lt;h3 id="comparative-results">Comparative Results&lt;/h3>
&lt;p>Comparing the 2016-2018 with the 2009-2011 season model, certain shifts have occurred from the 2009-2011 seasons model. Namely, yards-after-catch has become the strongest predictor of post-season qualification, flipping positions with successful reception in the 2009-2011 model. Another notable shift is the importance of number of plays run in the second quarter in the 2016-2018 model, overtaking number of plays run in the third quarter from the 2009-2011. The models themselves also shift in their strength of prediction. The 2009-2011 model shows stronger predictive capability (78.2% vs 77.8%), which is reflected in the f-values for the top 10 metrics of the model. The 2009-2011 model has four variables with f-values above 50 and one above 60. The 2016-2018 model only has one variable with an f-value above 50. These values represent the variance accounted for in the model by a variable. The higher the f-value the more variance accounted for in the model by that specific variable. Since the f-values were so high for many of the top 10 variables listed in each model, the p-values showed highly significant far exceeding the p=.05 level that was needed. The f-values were high because they accounted for so much of the variance in the model, meaning the predictive nature of the model was due in large part to many of the variables in the top 10. Another way to state this is that each of these top 10 variables were significantly better at predicting post-season qualification than would be expected due to chance.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_sig_features_2009.png" alt="ANOVA Chart for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> ANOVA Chart for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2009-2011)&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_sig_features_2016.png" alt="ANOVA Chart for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> ANOVA Chart for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2016-2018)&lt;/p>
&lt;p>Cloudmesh Comon &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> is used to create the benchmark.&lt;/p>
&lt;h2 id="6-discussion">6. Discussion&lt;/h2>
&lt;p>The first inference of this project was investigating the possibility of using in-game performance metrics as a competent and better-than-chance predictor of selecting skill position players making the NFL post-season. Both the 2009-2011 and the 2016-2018 season models were able to predict player post-season qualification at 78.2% and 77.8% levels of success, both above chance level. This success highlights the critical nature of skill performance players and provides confidence to the modern metric model of NFL players as a useful and qualified tool to evaluate player performance as a measure of success. This also gives clout to the skill position players who believe their contributions on the field are deserving of top dollar compensation in the NFL. According to our models, wide receivers are deserving of high compensation as their game play impacts the likelihood of their team making the playoff more than running backs. However, it is hard to discriminate whether quarterback play is also key to the success of wide receivers. It could well be that these two positions work hand-in-hand.&lt;/p>
&lt;p>Investigating the second inference regarding changes in the predictive model across time. In comparing the descriptive statistic models (figs. 1, 2 vs. 5, 6). There are some noticeable, but not significant differences in the two-time ranges. First, there are more receivers in the 2016-2018 time range, which reflects the NFL’s shift towards a more pass prone league. Since there was not an increase in roster size between the two-time ranges, the increase in receivers lead to a decrease in the number of quarterbacks and fullbacks on a roster, but these additional receivers carried probably took the roster spots of non-skill positions players that are not accounted for here. Both models show the importance of pass plays, successful pass plays, receiving touchdowns and yards, yards after catch, and other passing variables that highlight the importance of wide receivers and tight ends. The NFL has shifted towards a more pass-friendly league &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, and the models built here highlight the reasons why that occurred. Receiver plays are significantly more important in predicting post-season qualification than any other skill position metrics. It is likely that the shift towards receivers and away from running backs has taken place over time. It is possible that we have pulled two time periods that are too close together to reflect the shift in NFL play, and if we had pulled data from the 1990’s or 1980’s (unfortunately this data is not available in the needed metrics) we would see more running back heavy metrics at the tops of our models and significant changes in the two time periods.&lt;/p>
&lt;h3 id="limitations-and-future-research">Limitations and Future Research&lt;/h3>
&lt;p>Metrics are not provided for non-skill position players who could be critical in predicting playoff qualification. For instance, if we could include offensive linemen metrics, we would have a stronger model that would be better able to predict post-season qualification. Further, the NFL data we had access to does not measure defensive player metrics that we believe are critical in being able to predict post-season qualification for NFL teams. Future work should look to include defensive player metrics into their model, as well as non-skill position players to improve on this model.&lt;/p>
&lt;p>Though we were able to build a model to predict player qualification for the post-season, future research can build on this model by making a composite of players on a team to then predict a team making the playoffs. The present study is a nice first step in understanding the capabilities of game performance for predicting player success, but NFL teams are equally interested in a team&amp;rsquo;s success, not just individual skill players. Therefore, future research can build on this project by incorporating defensive player metrics, non-skill position offensive metrics, and composites of players on one team to predict a team&amp;rsquo;s projected chances of making the playoffs.&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>Utilizing skill position performance metrics shows to be a successful predictor of player post-season qualification above chance level (50%). Further, there are slight shifts in which metrics are best at predicting post-season qualification between the 2009-2011 and 2016-2018 time periods. However, the key metrics that were significant in both models from the two time periods (2009-2011 and 2016-2018) did not change. Therefore, we cannot say definitively that there has been a shift in style of play from 2009 to 2018.&lt;/p>
&lt;h2 id="8-acknowledgements">8. Acknowledgements&lt;/h2>
&lt;p>Thank you to my friends and family who supported me through working on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Seifert, K. (2020, June 18). How pro football has changed in the past 10 years: 12 ways the NFL evolved this decade. Retrieved November 17, 2020, from &lt;a href="https://www.espn.com/nfl/story/_/id/28373283/how-pro-football-changed-10-years-12-ways-nfl-evolved-decade">https://www.espn.com/nfl/story/_/id/28373283/how-pro-football-changed-10-years-12-ways-nfl-evolved-decade&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Zita, C. (2020, September 16). Improving a Famous NFL Prediction Model. Retrieved November 02, 2020, from &lt;a href="https://towardsdatascience.com/improving-a-famous-nfl-prediction-model-1295a7022859">https://towardsdatascience.com/improving-a-famous-nfl-prediction-model-1295a7022859&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Silver, N. (2018, September 05). How Our NFL Predictions Work. Retrieved November 02, 2020, from &lt;a href="https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/">https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Ryurko. Ryurko/NflscrapR-Data. 2 Mar. 2020, &lt;a href="https://github.com/ryurko/nflscrapR-data">https://github.com/ryurko/nflscrapR-data&lt;/a>.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Sports Reference, LLC. Pro Football Statistics and History. Retrieved October 09, 2020. &lt;a href="https://www.pro-football-reference.com/">https://www.pro-football-reference.com/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Training A Vehicle Using Camera Feed from Vehicle Simulation</title><link>/report/sp21-599-358/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/sp21-599-358/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-358/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-358/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-358/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-358/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Jesus Badillo, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-358/">sp21-599-358&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-358/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Code:
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-358/blob/main/project/code/tutorialEgo.py">tutorialEgo.py&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Deep Learning has become the main form of machine learning that has been used to train, test, and gather data for self-driving cars. The CARLA simulator
has been developed from the ground up so that reasearchers who normally do not have the capital to generate their own data for self-driving vehicles
can do so to fit their spcific model. CARLA provides many tools that can simulate many scenarios that an autonomous vehicle would run into. The benefit
of CARLA is that it can simulate scenarios that may be too dangerous for a real vehicle to perform, such as a full self-driving car in a heavly populated
area. CARLA has the backing of many companies who lead industry like Toyota who invested $100,000 dollars in 2018 [^6]. This project uses the CARLA
simulator to visualize how a real camera system based self-driving car sees obstacles and objects.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-using-the-carla-simulator">2. Using the CARLA Simulator&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-existing-work-on-carla">2.1 Existing Work on Carla&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-using-the-tensorflow-object-detection-api">3. Using the TensorFlow Object Detection API&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-implementation">4. Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-system-requirements">4.1 System Requirements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-download-and-install-carla">4.2 Download and Install CARLA&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#download-for-carla-version-099">Download for CARLA version 0.9.9&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#43-download-and-install-tensorflow-object-detection-api">4.3 Download and Install TensorFlow Object Detection API&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#from-the-downloads-folder-clone-the-tensorflow-models-git">From the Downloads folder clone the TensorFlow models git&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#44-download-protobuf">4.4 Download Protobuf&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#the-link-to-the-protobuf-repository-download-is-shown-below">The link to the ProtoBuf repository download is shown below&lt;/a>&lt;/li>
&lt;li>&lt;a href="#run-the-pwd-command-from-powershell-and-get-the-path-from-root-to-downloads-folder">Run the pwd command from powershell and get the path from root to Downloads folder&lt;/a>&lt;/li>
&lt;li>&lt;a href="#when-running-the-command-make-sure-that-you-are-in-downloadsmodels-masterresearch">When running the command make sure that you are in &amp;lsquo;~/Downloads/models-master/research&amp;rsquo;&lt;/a>&lt;/li>
&lt;li>&lt;a href="#make-sure-that-you-are-in-downloadsmodels-masterresearch-when-running-this-command">Make sure that you are in Downloads/models-master/research when running this command&lt;/a>&lt;/li>
&lt;li>&lt;a href="#test-installation">Test Installation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#test-success">Test Success&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#45-running-carla-with-object-detection">4.5 Running Carla With Object Detection&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#run-carla-client">Run CARLA Client&lt;/a>&lt;/li>
&lt;li>&lt;a href="#run-carla-object-detection-program">Run Carla Object Detection Program&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-training-model">5. Training Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-results">6. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-benchmark">7. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-conclusion">8. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-acknowledgments">9. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Making cars self driving has been a problem that many car companies have been trying to tackle in the 21st century.
There are many different approaches that have been used which all involve deep learning. The approaches all train data
that are gathered from a variety of sensors working together. Lidar and computer vision are the main sensors that are
used by commercial companies. Tesla uses video gathered from multiple cameras to train their neural network &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> which
is known as HydraNet. In this project, a simulation of a real driving vehicle with a camera feed will be used to see the
objects that a car would need to see to train the vehicle to be self-driving&lt;/p>
&lt;h2 id="2-using-the-carla-simulator">2. Using the CARLA Simulator&lt;/h2>
&lt;p>The CARLA simulator which uses the driver inputs and puts into a driving log which contains data of
the trajectory and the surroundings of the simulated vehicle. The CARLA simulator uses the the steering angle and throttle
to act much like the controllable inputs of a real vehicle. CARLA is an open-source and has been developed from the ground
up to support development, training, and validation of autonomous driving systems. In addition to open-source code and protocols,
CARLA provides open digital urban layouts, buildings, and vehicles that were created for this purpose and can be used freely.
The simulation platform supports flexible specification of sensor suites, environmental conditions, full control of all static
and dynamic actors, maps generation &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The simulation will be created by driving the vehicle in the simulator and using the
camera feed so that the neural network can be trained. Driving in the simulator looks much like Figure 1.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-358/raw/main/project/images/CARLA_Image.png" alt="Figure1">&lt;/p>
&lt;p>&lt;strong>Figure 1&lt;/strong> Driving in Carla Simulator &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/p>
&lt;h3 id="21-existing-work-on-carla">2.1 Existing Work on Carla&lt;/h3>
&lt;p>The tutorials over Carla from the youtuber SentDex provide a good introduction into projects that could use deep learning to train self-driving cars.
His tutorials provide a good insight into the Carla Environment so that one could perform their own study &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-using-the-tensorflow-object-detection-api">3. Using the TensorFlow Object Detection API&lt;/h2>
&lt;p>The Tenserflow object detection API is used to classify objects with a specific level of confidence. Image recognition is useful
for self-driving cars because it can provide known obstacles where the vehicle is prohibited from traveling. The API has been trained
on the COCO dataset which is a dataset consisting of about 300,000 of 90 of the most commonly found objects. Google provided this API to
improve the state of the Computer vision field. Figure2 shows how the bounding boxes classify images using the object detection API.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-358/raw/main/project/images/ObjectCars.png" alt="Figure2">&lt;/p>
&lt;p>&lt;strong>Figure 2&lt;/strong> Obect Detection for Cars &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="4-implementation">4. Implementation&lt;/h2>
&lt;h3 id="41-system-requirements">4.1 System Requirements&lt;/h3>
&lt;p>This project uses windows 10 along with visual studio code and python 3.7. Note that this project may work with other systems, but CARLA
is a resource intensive program.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>OS Version&lt;/th>
&lt;th>GPU&lt;/th>
&lt;th>RAM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Windows 10.0.18363 Build 18363&lt;/td>
&lt;td>NVIDIA GTX 1660 Super&lt;/td>
&lt;td>32 GB&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>In this study the CARLA version 0.9.9 is being used along with python 3.7 to control the simulated vehicle in the CARLA simulator.&lt;/p>
&lt;h3 id="42-download-and-install-carla">4.2 Download and Install CARLA&lt;/h3>
&lt;h4 id="download-for-carla-version-099">Download for CARLA version 0.9.9&lt;/h4>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/carla-simulator/carla/releases/tag/0.9.9">https://github.com/carla-simulator/carla/releases/tag/0.9.9&lt;/a> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/p>
&lt;/blockquote>
&lt;h5 id="the-file-to-download-is-shown-below">The file to download is shown below:&lt;/h5>
&lt;blockquote>
&lt;p>CARLA_0.9.9.zip&lt;/p>
&lt;/blockquote>
&lt;p>Make sure to download the compiled version for Windows. The Carla Simulator is around 25GB, so to replicate the study one must have 30-50GB
of free disk space. Once the file is finished downloading, extract the content of the CARLA zip file into the Downloads folder.&lt;/p>
&lt;h3 id="43-download-and-install-tensorflow-object-detection-api">4.3 Download and Install TensorFlow Object Detection API&lt;/h3>
&lt;h4 id="from-the-downloads-folder-clone-the-tensorflow-models-git">From the Downloads folder clone the TensorFlow models git&lt;/h4>
&lt;pre>&lt;code>git clone https://github.com/tensorflow/models.git
&lt;/code>&lt;/pre>&lt;p>Make sure to clone this git repository into the Downloads folder of your windows machine&lt;/p>
&lt;h3 id="44-download-protobuf">4.4 Download Protobuf&lt;/h3>
&lt;h4 id="the-link-to-the-protobuf-repository-download-is-shown-below">The link to the ProtoBuf repository download is shown below&lt;/h4>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/protocolbuffers/protobuf/releases/download/v3.16.0/protoc-3.16.0-win64.zip">https://github.com/protocolbuffers/protobuf/releases/download/v3.16.0/protoc-3.16.0-win64.zip&lt;/a> &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/p>
&lt;/blockquote>
&lt;p>The Tensorflow Object Detection API uses Protobufs to configure model and training parameters. Before the framework can be used,
the Protobuf libraries must be downloaded and compiled &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. Make sure that you extract the file to the Downloads folder. To configure
the model within the directory structure run the commands below.&lt;/p>
&lt;h4 id="run-the-pwd-command-from-powershell-and-get-the-path-from-root-to-downloads-folder">Run the pwd command from powershell and get the path from root to Downloads folder&lt;/h4>
&lt;pre>&lt;code>pwd
&lt;/code>&lt;/pre>&lt;h4 id="when-running-the-command-make-sure-that-you-are-in-downloadsmodels-masterresearch">When running the command make sure that you are in &amp;lsquo;~/Downloads/models-master/research&amp;rsquo;&lt;/h4>
&lt;pre>&lt;code>'PathFromDownloads/Downloads'/protoc object_detection/protos/*.proto --python_out=.
&lt;/code>&lt;/pre>&lt;p>The command shown above configures protobuf so that the object detection API could be used. Make sure you are in the Downloads/models-master/research path.
Run the commands below to install all of the necessary packages to run the object detection API.&lt;/p>
&lt;h4 id="make-sure-that-you-are-in-downloadsmodels-masterresearch-when-running-this-command">Make sure that you are in Downloads/models-master/research when running this command&lt;/h4>
&lt;pre>&lt;code>cp object_detection/packages/tf2/setup.py .
python -m pip install .
&lt;/code>&lt;/pre>&lt;p>After installing the packages test your installation from the Downloads/models-master/research path and run the command below.&lt;/p>
&lt;h4 id="test-installation">Test Installation&lt;/h4>
&lt;pre>&lt;code>python object_detection/builders/model_builder_tf2_test.py
&lt;/code>&lt;/pre>&lt;h4 id="test-success">Test Success&lt;/h4>
&lt;p>If the test was successful than you will a result similar to the one showed in Figure 3.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-358/raw/main/project/images/ModelSuccess.png" alt="Figure3">&lt;/p>
&lt;p>&lt;strong>Figure 3&lt;/strong>&lt;/p>
&lt;h3 id="45-running-carla-with-object-detection">4.5 Running Carla With Object Detection&lt;/h3>
&lt;p>The directory structure for the CARLA for the project shoud have protobuf, the tensorflow models-master directory, and the CARLA_0.9.9 directory
all in the Downloads folder. To correctly run this project one would need to open two powershell windows and run the CARLA client and the file which
is providid in this git repository called tutorialEgo.py. The two code snippets below show how to both programs&lt;/p>
&lt;h4 id="run-carla-client">Run CARLA Client&lt;/h4>
&lt;pre>&lt;code>&amp;quot;your path&amp;quot;\Downloads\CARLA_0.9.9\WindowsNoEditor&amp;gt; .\CarlaUE4.exe
&lt;/code>&lt;/pre>&lt;h4 id="run-carla-object-detection-program">Run Carla Object Detection Program&lt;/h4>
&lt;pre>&lt;code>#Make sure to place the tutorialEgo.py in the examples folder from the downloaded carla folder
&amp;quot;your path&amp;quot;\Downloads\CARLA_0.9.9\WindowsNoEditorPythonAPI\examples&amp;gt; py -3.7 .\tutorialEgo.py
&lt;/code>&lt;/pre>&lt;h2 id="5-training-model">5. Training Model&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model Name&lt;/th>
&lt;th>Speed&lt;/th>
&lt;th>COCO mAP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ssd_mobilenet_v1_coco&lt;/td>
&lt;td>fast&lt;/td>
&lt;td>21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ssd_inception_v2_coco&lt;/td>
&lt;td>fast&lt;/td>
&lt;td>24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>rfcn_resnet101_coco&lt;/td>
&lt;td>medium&lt;/td>
&lt;td>30&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>faster_rcnn_resnet101_coco&lt;/td>
&lt;td>medium&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>faster_rcnn_inception_resnet_v2_astrous_coco&lt;/td>
&lt;td>slow&lt;/td>
&lt;td>37&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>To perform the object detection in the Cara simulator this project uses the TensorFlow object detection API. The model uses the COCO dataset
which contains five different models each with a different mean average precision. The mean average precison, or mAP, is the product of precision
and recall on detecting bounding boxes. The higher the mAP score, the more accurate the network is but that slows down the speed of the model &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.
In this project the ssd_mobilenet_v1_coco model was used because it is the fastest of the 5 models providie for the COCO dataset.&lt;/p>
&lt;h2 id="6-results">6. Results&lt;/h2>
&lt;p>The accuracy of the model was not very good at detecting other scenery, but it was able to detect the most important obstacles for self-driving cars
such as other vehicles, pedestrians, and traffic signals. The video below shows a simulation in the Carla simulated vehicle with object detection.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-358/raw/main/project/images/ProgramRunning.png" alt="Figure4">&lt;/p>
&lt;p>&lt;strong>Figure 4&lt;/strong> Object Detection in CARLA&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://drive.google.com/file/d/13RXIy74APbwSqV_zBs_v59v4ZOnWdtrT/view?usp=sharing">https://drive.google.com/file/d/13RXIy74APbwSqV_zBs_v59v4ZOnWdtrT/view?usp=sharing&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h2 id="7-benchmark">7. Benchmark&lt;/h2>
&lt;p>The benchmark used for this project was the StopWatch function from the cloudmesh package &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. The function can see how long a particular section
of code took compared to a different section in the program. In this project the section that took the longest was to setup pedestrian and traffic accross
the simulated city. This makes sense because there are many vehicles and pedestrians that need to be spawned while also pre computing there trajectories.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-358/raw/main/project/images/Benchmark.png" alt="Figure5">&lt;/p>
&lt;p>&lt;strong>Figure 5&lt;/strong>&lt;/p>
&lt;h2 id="8-conclusion">8. Conclusion&lt;/h2>
&lt;p>The ssd_mobilenet_v1_coco model did not perform as well as it could have because it sometimes classified some objects wrong. For example, some
pedestrians walking produced shadows which the object detection models perceived as ski&amp;rsquo;s. The mean average precision of the model was the lowest
of the models trained by the COCO dataset which played a factor in the accuracy of the model. This caused issues in the vehicle&amp;rsquo;s detection of its
surroundings. Overall, the model was good at classifying the main objects it needs to know to drive safely such as pedestrians and other vehicles.
This project fulfilled its purpose by showcasing that it can use the object detection from the camera feed along with built in collison detector
to be able to train a self-driving vehicle in CARLA.&lt;/p>
&lt;h2 id="9-acknowledgments">9. Acknowledgments&lt;/h2>
&lt;p>The author if this project would like to thank Harrison Kinsley from the youtube channel SentDex for providing good resources for how to use deep learning
using carla and tensorflow. The author would also like to thank Dr. Gregor von Laszewski for feedback on this report, and Dr. Geoffrey Fox for sharing
his knowledge in Deep Learning and Artificial Intelligence throughout this course taught at Indiana University.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Explains architecture of Tesla&amp;rsquo;s Neural Network,[Online Resource]
&lt;a href="https://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Mullapudi_HydraNets_Specialized_Dynamic_CVPR_2018_paper.pdf">https://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Mullapudi_HydraNets_Specialized_Dynamic_CVPR_2018_paper.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Link to Carla website, [Online Resource]
&lt;a href="http://carla.org/">http://carla.org/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Documentation Explaing Key Features of Carla, [Online Resource]
&lt;a href="https://carla.readthedocs.io/en/0.9.9/getting_started/">https://carla.readthedocs.io/en/0.9.9/getting_started/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Introduction to Carla with Python, [Online Resource]
&lt;a href="https://pythonprogramming.net/introduction-self-driving-autonomous-cars-carla-python/">https://pythonprogramming.net/introduction-self-driving-autonomous-cars-carla-python/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Object Detection Image, [Online Resource]
&lt;a href="https://www.researchgate.net/figure/Object-detection-in-a-dense-scene_fig4_329217107">https://www.researchgate.net/figure/Object-detection-in-a-dense-scene_fig4_329217107&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Link to the Carla_0.9.9 github, [GitHub]
&lt;a href="https://github.com/carla-simulator/carla/releases/tag/0.9.9">https://github.com/carla-simulator/carla/releases/tag/0.9.9&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Protobuf github download, [GitHub]
&lt;a href="https://github.com/protocolbuffers/protobuf/releases/download/v3.16.0/protoc-3.16.0-win64.zip">https://github.com/protocolbuffers/protobuf/releases/download/v3.16.0/protoc-3.16.0-win64.zip&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Explains differences between models being used for Object Detection and performance, [Online Resource]
&lt;a href="https://towardsdatascience.com/is-google-tensorflow-object-detection-api-the-easiest-way-to-implement-image-recognition-a8bd1f500ea0">https://towardsdatascience.com/is-google-tensorflow-object-detection-api-the-easiest-way-to-implement-image-recognition-a8bd1f500ea0&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item></channel></rss>