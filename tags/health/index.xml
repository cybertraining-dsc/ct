<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cybertraining â€“ health</title><link>/tags/health/</link><description>Recent content in health on Cybertraining</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 16 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/health/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Health Informatics</title><link>/docs/modules/bigdataapplications/2019/applications/health/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/modules/bigdataapplications/2019/applications/health/</guid><description>
&lt;p>&lt;a href="https://drive.google.com/open?id=0B6wqDMIyK2P7UGRJNmlkYkNkQk0">&lt;img src="images/presentation.png" alt="Presentation"> Health Informatics
(131)&lt;/a>&lt;/p>
&lt;p>This section starts by discussing general aspects of Big Data and Health
including data sizes, different areas including genomics, EBI, radiology
and the Quantified Self movement. We review current state of health care
and trends associated with it including increased use of Telemedicine.
We summarize an industry survey by GE and Accenture and an impressive
exemplar Cloud-based medicine system from Potsdam. We give some details
of big data in medicine. Some remarks on Cloud computing and Health
focus on security and privacy issues.&lt;/p>
&lt;p>We survey an April 2013 McKinsey report on the Big Data revolution in US
health care; a Microsoft report in this area and a European Union report
on how Big Data will allow patient centered care in the future. Examples
are given of the Internet of Things, which will have great impact on
health including wearables. A study looks at 4 scenarios for healthcare
in 2032. Two are positive, one middle of the road and one negative. The
final topic is Genomics, Proteomics and Information Visualization.&lt;/p>
&lt;h2 id="big-data-and-health">Big Data and Health&lt;/h2>
&lt;p>This lesson starts with general aspects of Big Data and Health including
listing subareas where Big data important. Data sizes are given in
radiology, genomics, personalized medicine, and the Quantified Self
movement, with sizes and access to European Bioinformatics Institute.&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=ZkM-yZJQ1Cg">&lt;img src="images/video.png" alt="Video"> Big Data and Health
(10:02)&lt;/a>&lt;/p>
&lt;h2 id="status-of-healthcare-today">Status of Healthcare Today&lt;/h2>
&lt;p>This covers trends of costs and type of healthcare with low cost genomes
and an aging population. Social media and government Brain initiative.&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=x9TpdMBqYrk">&lt;img src="images/video.png" alt="Video"> Status of Healthcare Today
(16:09)&lt;/a>&lt;/p>
&lt;h2 id="telemedicine-virtual-health">Telemedicine (Virtual Health)&lt;/h2>
&lt;p>This describes increasing use of telemedicine and how we tried and
failed to do this in 1994.&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=Pe4CVXQaL_U">&lt;img src="images/video.png" alt="Video"> Telemedicine
(8:21)&lt;/a>&lt;/p>
&lt;h2 id="medical-big-data-in-the-clouds">Medical Big Data in the Clouds&lt;/h2>
&lt;p>An impressive exemplar Cloud-based medicine system from Potsdam.&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=GldSVijkJcM">&lt;img src="images/video.png" alt="Video"> Medical Big Data in the Clouds
(15:02)&lt;/a>&lt;/p>
&lt;h3 id="medical-image-big-data">Medical image Big Data&lt;/h3>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=GOcVtwx2R2k">&lt;img src="images/video.png" alt="Video"> Medical Image Big Data
(6:33)&lt;/a>&lt;/p>
&lt;h3 id="clouds-and-health">Clouds and Health&lt;/h3>
&lt;p>&lt;a href="http://youtu.be/9Whkl_UPS5g">&lt;img src="images/video.png" alt="Video"> Clouds and Health (4:35)&lt;/a>&lt;/p>
&lt;h3 id="mckinsey-report-on-the-big-data-revolution-in-us-health-care">McKinsey Report on the big-data revolution in US health care&lt;/h3>
&lt;p>This lesson covers 9 aspects of the McKinsey report. These are the
convergence of multiple positive changes has created a tipping point for&lt;/p>
&lt;p>innovation; Primary data pools are at the heart of the big data
revolution in healthcare; Big data is changing the paradigm: these are
the value pathways; Applying early successes at scale could reduce US
healthcare costs by $300 billion to $450 billion; Most new big-data
applications target consumers and providers across pathways; Innovations
are weighted towards influencing individual decision-making levers; Big
data innovations use a range of public, acquired, and proprietary data&lt;/p>
&lt;p>types; Organizations implementing a big data transformation should
provide the leadership required for the associated cultural
transformation; Companies must develop a range of big data capabilities.&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=fu-TWnIk980">&lt;img src="images/video.png" alt="Video"> McKinsey Report
(14:53)&lt;/a>&lt;/p>
&lt;h3 id="microsoft-report-on-big-data-in-health">Microsoft Report on Big Data in Health&lt;/h3>
&lt;p>This lesson identifies data sources as Clinical Data, Pharma &amp;amp; Life
Science Data, Patient &amp;amp; Consumer Data, Claims &amp;amp; Cost Data and
Correlational Data. Three approaches are Live data feed, Advanced
analytics and Social analytics.&lt;/p>
&lt;p>&lt;a href="http://youtu.be/PjffvVgj1PE">&lt;img src="images/video.png" alt="Video"> Microsoft Report on Big Data in Health
(2:26)&lt;/a>&lt;/p>
&lt;h3 id="eu-report-on-redesigning-health-in-europe-for-2020">EU Report on Redesigning health in Europe for 2020&lt;/h3>
&lt;p>This lesson summarizes an EU Report on Redesigning health in Europe for
2020. The power of data is seen as a lever for change in My Data, My
decisions; Liberate the data; Connect up everything; Revolutionize
health; and Include Everyone removing the current correlation between
health and wealth.&lt;/p>
&lt;p>&lt;a href="http://youtu.be/9mbt_ZSs0iw">&lt;img src="images/video.png" alt="Video"> EU Report on Redesigning health in Europe for 2020
(5:00)&lt;/a>&lt;/p>
&lt;h3 id="medicine-and-the-internet-of-things">Medicine and the Internet of Things&lt;/h3>
&lt;p>The Internet of Things will have great impact on health including
telemedicine and wearables. Examples are given.&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=Jk3EeFzZnuU">&lt;img src="images/video.png" alt="Video"> Medicine and the Internet of Things
(8:17)&lt;/a>&lt;/p>
&lt;h3 id="extrapolating-to-2032">Extrapolating to 2032&lt;/h3>
&lt;p>A study looks at 4 scenarios for healthcare in 2032. Two are positive,
one middle of the road and one negative.&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=a5G4HACeokg">&lt;img src="images/video.png" alt="Video"> Extrapolating to 2032
(15:13)&lt;/a>&lt;/p>
&lt;h3 id="genomics-proteomics-and-information-visualization">Genomics, Proteomics and Information Visualization&lt;/h3>
&lt;p>A study of an Azure application with an Excel frontend and a cloud BLAST
backend starts this lesson. This is followed by a big data analysis of
personal genomics and an analysis of a typical DNA sequencing analytics
pipeline. The Protein Sequence Universe is defined and used to motivate
Multi dimensional Scaling MDS. Sammon&amp;rsquo;s method is defined and its use
illustrated by a metagenomics example. Subtleties in use of MDS include
a monotonic mapping of the dissimilarity function. The application to
the COG Proteomics dataset is discussed. We note that the MDS approach
is related to the well known chisq method and some aspects of nonlinear
minimization of chisq (Least Squares) are discussed.&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=zGzBtxq1ZRE">&lt;img src="images/video.png" alt="Video"> Genomics, Proteomics and Information Visualization
(6:56)&lt;/a>&lt;/p>
&lt;p>Next we continue the discussion of the COG Protein Universe introduced
in the last lesson. It is shown how Proteomics clusters are clearly seen
in the Universe browser. This motivates a side remark on different
clustering methods applied to metagenomics. Then we discuss the
Generative Topographic Map GTM method that can be used in dimension
reduction when original data is in a metric space and is in this case
faster than MDS as GTM computational complexity scales like N not N
squared as seen in MDS.&lt;/p>
&lt;p>Examples are given of GTM including an application to topic models in
Information Retrieval. Indiana University has developed a deterministic
annealing improvement of GTM. 3 separate clusterings are projected for
visualization and show very different structure emphasizing the
importance of visualizing results of data analytics. The final slide
shows an application of MDS to generate and visualize phylogenetic
trees.&lt;/p>
&lt;p>\TODO{These two videos need to be uploaded to youtube}
&lt;a href="https://drive.google.com/file/d/0B5plU-u0wqMobXdEQWRHWl95UTA/view?usp=sharing">&lt;img src="images/video.png" alt="Video"> Genomics, Proteomics and Information Visualization I
(10:33)&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://drive.google.com/file/d/0B5plU-u0wqModlhmdVUwdGlQNTA/view?usp=sharing">&lt;img src="images/video.png" alt="Video"> Genomics, Proteomics and Information Visualization: II
(7:41)&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://drive.google.com/open?id=0B8936_ytjfjmX0lEMWhMX2kwRHc">&lt;img src="images/presentation.png" alt="Presentation"> Proteomics and Information Visualization
(131)&lt;/a>&lt;/p>
&lt;h3 id="resources">Resources&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://wiki.nci.nih.gov/display/CIP/CIP+Survey+of+Biomedical+Imaging+Archives">https://wiki.nci.nih.gov/display/CIP/CIP+Survey+of+Biomedical+Imaging+Archives&lt;/a> [@wiki-nih-cip-survey]&lt;/li>
&lt;li>&lt;a href="http://grids.ucs.indiana.edu/ptliupages/publications/Where%20does%20all%20the%20data%20come%20from%20v7.pdf">http://grids.ucs.indiana.edu/ptliupages/publications/Where\%20does\%20all\%20the\%20data\%20come\%20from\%20v7.pdf&lt;/a> [@fox2011does]&lt;/li>
&lt;li>&lt;del>&lt;a href="http://www.ieee-icsc.org/ICSC2010/Tony%20Hey%20-%2020100923.pdf">http://www.ieee-icsc.org/ICSC2010/Tony\%20Hey\%20-\%2020100923.pdf&lt;/a>&lt;/del>(this link does not exist any longer)&lt;/li>
&lt;li>&lt;a href="http://quantifiedself.com/larry-smarr/">http://quantifiedself.com/larry-smarr/&lt;/a> [@smarr13self]&lt;/li>
&lt;li>&lt;a href="http://www.ebi.ac.uk/Information/Brochures/">http://www.ebi.ac.uk/Information/Brochures/&lt;/a> [@www-ebi-aboutus]&lt;/li>
&lt;li>&lt;a href="http://www.kpcb.com/internet-trends">http://www.kpcb.com/internet-trends&lt;/a> [@www-kleinerperkins-internet-trends]&lt;/li>
&lt;li>&lt;a href="http://www.slideshare.net/drsteventucker/wearable-health-fitness-trackers-and-the-quantified-self">http://www.slideshare.net/drsteventucker/wearable-health-fitness-trackers-and-the-quantified-self&lt;/a> [@www-slideshare-wearable-quantified-self]&lt;/li>
&lt;li>&lt;a href="http://www.siam.org/meetings/sdm13/sun.pdf">http://www.siam.org/meetings/sdm13/sun.pdf&lt;/a> [@archive&amp;ndash;big-data-analytics-healthcare]&lt;/li>
&lt;li>&lt;a href="http://en.wikipedia.org/wiki/Calico_%28company%29">http://en.wikipedia.org/wiki/Calico_\%28company\%29&lt;/a> [@www-wiki-calico]&lt;/li>
&lt;li>&lt;a href="http://www.slideshare.net/GSW_Worldwide/2015-health-trends">http://www.slideshare.net/GSW_Worldwide/2015-health-trends&lt;/a> [@www-slideshare-2015-health trends]&lt;/li>
&lt;li>&lt;a href="http://www.accenture.com/SiteCollectionDocuments/PDF/Accenture-Industrial-Internet-Changing-Competitive-Landscape-Industries.pdf">http://www.accenture.com/SiteCollectionDocuments/PDF/Accenture-Industrial-Internet-Changing-Competitive-Landscape-Industries.pdf&lt;/a> [@www-accenture-insight-industrial-internet]&lt;/li>
&lt;li>&lt;a href="http://www.slideshare.net/schappy/how-realtime-analysis-turns-big-medical-data-into-precision-medicine">http://www.slideshare.net/schappy/how-realtime-analysis-turns-big-medical-data-into-precision-medicine&lt;/a> [@www-slideshare-big-medical-data-medicine]&lt;/li>
&lt;li>&lt;a href="http://medcitynews.com/2013/03/the-body-in-bytes-medical-images-as-a-source-of-healthcare-big-data-infographic/">http://medcitynews.com/2013/03/the-body-in-bytes-medical-images-as-a-source-of-healthcare-big-data-infographic/&lt;/a> [@medcitynews-bytes-medical-images]&lt;/li>
&lt;li>&lt;del>&lt;a href="http://healthinformatics.wikispaces.com/file/view/cloud_computing.ppt">http://healthinformatics.wikispaces.com/file/view/cloud_computing.ppt&lt;/a>&lt;/del> (this link does not exist any longer)&lt;/li>
&lt;li>&lt;a href="https://www.mckinsey.com/~/media/mckinsey/industries/healthcare%20systems%20and%20services/our%20insights/the%20big%20data%20revolution%20in%20us%20health%20care/the_big_data_revolution_in_healthcare.ashx">https://www.mckinsey.com/~/media/mckinsey/industries/healthcare%20systems%20and%20services/our%20insights/the%20big%20data%20revolution%20in%20us%20health%20care/the_big_data_revolution_in_healthcare.ashx&lt;/a> [@www-mckinsey-industries-healthcare]&lt;/li>
&lt;li>&lt;del>&lt;a href="https://partner.microsoft.com/download/global/40193764">https://partner.microsoft.com/download/global/40193764&lt;/a>&lt;/del> (this link does not exist any longer)&lt;/li>
&lt;li>&lt;a href="https://ec.europa.eu/eip/ageing/file/353/download_en?token=8gECi1RO">https://ec.europa.eu/eip/ageing/file/353/download_en?token=8gECi1RO&lt;/a>&lt;/li>
&lt;li>&lt;del>&lt;a href="http://www.liveathos.com/apparel/app">http://www.liveathos.com/apparel/app&lt;/a>&lt;/del>&lt;/li>
&lt;li>&lt;a href="http://debategraph.org/Poster.aspx?aID=77">http://debategraph.org/Poster.aspx?aID=77&lt;/a> [@debategraph-poster]&lt;/li>
&lt;li>&lt;del>&lt;a href="http://www.oerc.ox.ac.uk/downloads/presentations-from-events/microsoftworkshop/gannon">http://www.oerc.ox.ac.uk/downloads/presentations-from-events/microsoftworkshop/gannon&lt;/a>&lt;/del>(this link does not exist any longer)&lt;/li>
&lt;li>&lt;del>&lt;a href="http://www.delsall.org">http://www.delsall.org&lt;/a>&lt;/del> (this link does not exist any longer)&lt;/li>
&lt;li>&lt;a href="http://salsahpc.indiana.edu/millionseq/mina/16SrRNA_index.html">http://salsahpc.indiana.edu/millionseq/mina/16SrRNA_index.html&lt;/a> [@www-salsahpc-millionseq]&lt;/li>
&lt;li>&lt;a href="http://www.geatbx.com/docu/fcnindex-01.html">http://www.geatbx.com/docu/fcnindex-01.html&lt;/a> [@www-geatbx-parametric-optimization]&lt;/li>
&lt;/ul></description></item><item><title>Report: Investigating the Classification of Breast Cancer Subtypes using KMeans</title><link>/report/su21-reu-362/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-362/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-362/actions">&lt;img src="https://github.com/cybertraining-dsc/hid-example/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-362/actions">&lt;img src="https://github.com/cybertraining-dsc/hid-example/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Kehinde Ezekiel, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-362">su21-reu-362&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-362/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Breast cancer is an heterogenous disease that is characterized by abnormal growth of the cells in the breast region[^1]. There are four major molecular subtypes of breast cancer. This classification was based on a 50-gene signature profiling test called PAM50. Each molecular subtype has a specific morphology and treatment plan. Early diagnosis and detection of possible cancerous cells usually increase survival chances and provide a better approach for treatment and management. Different tools like ultrasound, thermography, mammography utilize approaches like image processing and artificial intelligence to screen and detect breast cancer. Artificial Intelligence (AI) involves the simulation of human intelligence in machines and can be used for learning or to solve problems. A major subset of AI is Machine Learning which involves training a piece of software (called model) to makwe useful predictions using dataset.&lt;/p>
&lt;p>In this project, a machine learning algorithm, KMeans, was implemented to design and analyze a proteomic dataset into clusters using its proteins identifiers. These protein identifiers were associated with the PAM50genes that was used to originally classify breast cancer into four molecular subtypes. The project revealed that further studies can be done to investigate the relationship between the data points in each cluster with the biological properties of the molecular subtypes which could lead to newer discoveries and developmeny of new therapies, effective treatment plan and management of the disease. It also suggests that several machine learning algorithms can be leveraged upon to address healthcare issues like breast cancer and other diseases which are characterized by subtypes.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-the-kmeans-approach">3. The KMeans Approach&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-results-and-images">5. Results and Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> AI, cancer, breast, algorithms, machine learning, healthcare, subtypes, classification.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Breast cancer is the most common cancer, and also the primary cause of mortality due to cancer in females around the World. It is an heterogenous disease that is characterized by the abnormal growth of cells in the breast region&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Early diagnosis and detection of possible cancerous cells in the breast usually increase survival chances and provide a better approach for treatment and management. Treatment and management often depend on the stage of cancer, the subtype, the tumor size, location and many other factors. During the last 20 years, four major intrinsic molecular subtypes for breast cancer- luminal A, luminal B, HER2-enriched and Basal-like have been identified, classified and intensively studied. Each subtype has its distinct morphologies and clinical treatment. The classification is based on gene expression profiling, specificaly defined by mRNA expression of 50 genes (also known as, PAM50 Genes). This test is known as the PAM50 test. The accurate grouping of breast cancer into its relevant subtypes can improve accurate treatment-decision making&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The PAM50 test is now known as the Prosigna Breast Cancer Prognostic Gene Signature Assay 50 (known as Prosigna) and it analyzes the activity of certain genes in early-stage, hormone-receptor-positive breast cancer&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. This classification is based on the mRNA expression and the activity of 50 genes and it aims to estimate the risk of distant reccurrence of breast cancer. Since the assay was based on mRNA expression, this project suggested that a classification based on the final product of mRNA, that is protein, can be implemented to investigate its role in the classifictaion of molecular breast cancer subtypes. As a result, the project was focused on the use of a proteomic dataset which contained published iTRAQ proteome profiling of 77 breast cancer samples and expression values for the proteins of each sample.&lt;/p>
&lt;p>Most times, breast cancer is diagnosed and detected through a combination of different approaches such as imaging (e.g. mammogram and ultrasound), physical examination by a radiologist and biopsy. Biopsy is used to confirm the breast cancer symptoms. However, research has shown that radiologists can miss up to 30% of breast cancer tissues during detection&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This gap has brought about the introduction of Computer aided Diagnosis (CAD) systems can help detect abnormalities in an efficient manner. CAD is a technology that includes utilizing the concept of artificial intelligence(AI) and medical image processing to find abnormal signs in the human body&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. Machine Learning is a subset of AI and it has several algorithms that can be used to build a model to perform a specific task or to predict a pattern. KMeans is one of such algorithm.&lt;/p>
&lt;p>Building a model using machine learning involves selecting and preparing the appropriate dataset, identifying the accurate machine learnning algorithm to use, training the algorithm on the data to build a model, validating the resulting model&amp;rsquo;s performance on testing data and using the model on a new data&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. In this project, KMeans was the algorithm used in this project, the datasets were prepared through several procedures like filtering, merging. KMeans clustering method was used to investigate the classification of the molecular subtypes. Its efficacy is often tested by a silhouette score. A silhouette score shows how similar an object is to its own cluster and it ranges from -1 to 1 where a high values indicates that an object is well matched to its own cluster. A homogeneity score determines if a cluster should only contain samples that belong to a particular class. It ranges from a value between 0 to 1 with low values indicating a low homogeneity.&lt;/p>
&lt;p>The project investigated the efficient number of clusters that could be generated for the proteome dataset which would consequetly provide an optimal classification of the protein expression values for the breast cancer samples. The proteins that were used in the KMeans analysis were the proteins that were associated with the PAM50 genes. The result of the project could provide insights to medical scientists and researchers to identify any interelatedness between the original classification of breast cancer molecular subtypes.&lt;/p>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;p>Datasets are eseential in drawing conclusion. In the diagnosis, detection and classification of breast cancecr, datasets have been essential to draw conclusion by identifying patterns. These datasets range from imaging datasets to clinical datasets, proteomic datasets etc. Large amounts of data have been collected due to new technological and computational advances like the use of websites like NCBI, equipments like Electroencephalogram (EEG) which record clinical information. Medical researchers leverage these datasets to make useful health care decisions that affect a region, gender or the world. The need for accuracy and reproducibilty has led to the use of machine learning as an important tool for drawing conclusions.&lt;/p>
&lt;p>Machine Learning involves training a piece of software, also known as model, to idnetify patterns from a dataset and make useful predictions. There are several factors to be considered when using datasets. One of such is data privacy. Recently, measures have been taken to ensure that the privacy of data. Some of these measures include, replacing codes for patients name, using documents and mobile applications that ask for permission from patients before using their data. Recently, the World Health Organization (WHO) made her report on AI and provided priniples that ensure that AI works for all. On of such is that the designer of AI technologies should satisfy regulatory requirements for safety, accuracy and efficacy for well-defined use cases or indications. Measures of quality control in practice and quality improvement in the use of AI must be available&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Building a model using machine learning involves selecting and preparing the appropriate dataset, identifying the accurate machine learnning algorithm to use, training the algorithm on the data to build a model, validating the resulting model&amp;rsquo;s performance on testing data and using the model on a new data&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. In this project, KMEans was the algorithm used in this project, the datasets were prepared through several procedures like filtering, merging.&lt;/p>
&lt;h2 id="3-the-kmeans-approach">3. The KMeans Approach&lt;/h2>
&lt;p>KMeans clustering is an unsupervised machine learning algorithm that makes inferences from datasets without referring to a known outcome. It aims to identify underlying patterns in a dataset by looking for a fixed number of clusters, (known as k). The required number of clusters is chosen by the person building the model. KMeans was used in this project to classify the protein IDs (or RefSeq_IDs) into clusters. Each cluster was designed to be associated with related protein IDs.&lt;/p>
&lt;p>Three datasets were used for the algorithm. The first and main dataset was a proteomic dataset. It contained published iTRAQ proteome profiling of 77 breast cancer samples generated by the Clinical Proteomic Tumor Analysis Consortium (NCI/NIH). Each sample contained expression values for ~12000 proteins, with missing values present when a given protein could not be quantified in a given sample. The variables in the dataset included the RefSeq_accession_number(also known as RefSeq protein ID), &amp;ldquo;the gene_symbol&amp;rdquo; (which was unique to each gene), &amp;ldquo;the gene_name&amp;rdquo; (which was the full name of the gene). The remaining columns were the log2 iTRAQ ratios for each of the 77 samples while the last three columns are from healthy individuals.&lt;/p>
&lt;p>The second dataset was a PAM50 dataset. It contained the list of genes and proteins used in the PAM50 classification system. The variables include the RefSeqProteinID which matched the Protein IDs(or RefSeq_IDs) in the main proteome dataset.&lt;/p>
&lt;p>The third dataset was a clinical data of about 105 clinical breast cancer samples. 77 of the breast cancer samples were the samples in the first dataset. The excluded samples were as a result of protein degradation&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. The variables in the dataset are:
â€˜Complete TCGA ID', &amp;lsquo;Gender&amp;rsquo;, &amp;lsquo;Age at Initial Pathologic Diagnosis&amp;rsquo;, &amp;lsquo;ER Status&amp;rsquo;, &amp;lsquo;PR Status&amp;rsquo;, &amp;lsquo;HER2 Final Status&amp;rsquo;, &amp;lsquo;Tumor&amp;rsquo;, &amp;lsquo;Tumor&amp;ndash;T1 Coded&amp;rsquo;, &amp;lsquo;Node&amp;rsquo;, &amp;lsquo;Node-Coded&amp;rsquo;, &amp;lsquo;Metastasis&amp;rsquo;, &amp;lsquo;Metastasis-Coded&amp;rsquo;, &amp;lsquo;AJCC Stage&amp;rsquo;, &amp;lsquo;Converted Stage&amp;rsquo;, &amp;lsquo;Survival Data Form&amp;rsquo;, &amp;lsquo;Vital Status&amp;rsquo;, &amp;lsquo;Days to Date of Last Contact&amp;rsquo;, &amp;lsquo;Days to date of Death&amp;rsquo;, &amp;lsquo;OS event&amp;rsquo;, &amp;lsquo;OS Time&amp;rsquo;, &amp;lsquo;PAM50 mRNA&amp;rsquo;, &amp;lsquo;SigClust Unsupervised mRNA&amp;rsquo;, &amp;lsquo;SigClust Intrinsic mRNA&amp;rsquo;, &amp;lsquo;miRNA Clusters&amp;rsquo;, &amp;lsquo;methylation Clusters&amp;rsquo;, &amp;lsquo;RPPA Clusters&amp;rsquo;, &amp;lsquo;CN Clusters&amp;rsquo;, &amp;lsquo;Integrated Clusters (with PAM50)&amp;rsquo;, &amp;lsquo;Integrated Clusters (no exp)&amp;rsquo;, &amp;lsquo;Integrated Clusters (unsup exp).&amp;rsquo;&lt;/p>
&lt;p>During the preparation of the datasets for KMeans analysis, unused columns like &amp;ldquo;gene_name&amp;rdquo; and &amp;ldquo;gene_symbol&amp;rdquo; were removed in the first dataset. The first and third dataset were merged together. Prior to merging, the variable &amp;lsquo;Complete TCGA ID&amp;rsquo; in the third dataset was found to be the same as the TCGAs in the first dataset. The Complete TCGA ID refered to a breast cancer patient, some patients were found in both datasets. The TCGA ID in the first dataset was renamed to match with the TCGA of the third dataset, thereby giving the same syntax. The first dataset was also transposed as a row and its gene expression as the columns. These processes were done in order to merge both dataset efficiently.&lt;/p>
&lt;p>After merging, the &amp;ldquo;PAM5O RNA&amp;rdquo; variable from the second dataset was selected to join the merged dataset. This single dataset was named &amp;ldquo;pam50data&amp;rdquo;. It contained all the variables that were needed for KMeans Analysis which included the genes that were used for the PAM50 classification (only 43 were available in the dataset), the complete TCGA ID of each 80 patient, and their molecular tumor type. Missing values in the dataset were imputed using SimpleImputer. Then, KMeans clustering was performed. The metrics were tested with cluster numbers of 3, 4, 5, 20 and 79. The bigger numbers (20 and 79) were tested just for comparison. Further details on the codes written can be found in &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Also, &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup> and &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup> were kernels that provided insights for the written code.&lt;/p>
&lt;h2 id="5-results-and-images">5. Results and Images&lt;/h2>
&lt;p>Several codes were written to determine the best number of clusters for the model. The effectiveness of a cluster is often measured by scores such as silhouette score, homogeneity score and adjusted rand score.&lt;/p>
&lt;p>The silhouette score for a cluster of 3, 4, and 5, 8, 20 and 79 were 0.143, 0.1393, 0.1193, 0.50968, 0.0872, 0.012 while the homogenenity scores were 0.4635, 0.4749, 0.1193, 0.5617, 0.6519 and 1.0 respectively. The homogeneity score for 79 is 1.0 since the algorithm can assign all the points into sepearate clusters. However, it is not efficient for the dataset we used. A cluster of 3 works best since the silhouette score is high and the homogeneity score jumps ~2-fold.&lt;/p>
&lt;p>Figures 1 and 2 show the results of the visualization of the clusters of 3 and 4.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-362/raw/main/project/images/new.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> The classification of Breast Cancer Moleecular Subtypes using KMeans Clustering. (k=3). Each data point represnt the expression value for the genes that were used for clustering.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-362/raw/main/project/images/k%3D4_image.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> The classification of Breast Cancer Moleecular Subtypes using KMeans Clustering. (k=4). Each data point represnt the expression value for the genes that were used for clustering.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>This program was executed on a Google Colab server and the entire runtime took 1.012 seconds Table 1 lists the amount of time taken to loop for n_components. The n_components is gotten from the code and it refers to the features of the dataset.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Status&lt;/th>
&lt;th>Time(s)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>parallel 1&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.647&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 3&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.936&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 5&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.952&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 7&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.943&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 9&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>1.002&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 11&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.991&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 13&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.958&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 15&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>1.012&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Benchmark:&lt;/strong> The table shows the parallel process time take the for loop for n_components.&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>The results of the KMeans analysis showed that three clusters provided an optimal result for the classification using a proteomic dataset. A cluster of 3 provided a balanced silhouette and homogeneity score. This predict that some interrelatedness could exist between the original PAM50 subtype classfication, since the result of classifying a protein dataset using a machine learning algorithm identified a cluster of 3 as one with the optimal result.
Also, future research could be done by using other machine learning algorithms, possibly a supervised learning algotithm, to identify the correlation between the clusters and the four molecular subtypes.
This model can be improved on and if proven to show that there truly exist a relationship between the four molecular subtypes, more research could be done to identify the factors that contribure to the interelatedness. This would lead medical scientists and researchers to work on better innovative methods that will aid the treatment and management of breast cancer.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>This projected was immensely supported by Dr. Gregor von Laszewski.
Also, a big appreciation to the REU Instructors (Carlos Theran, Yohn Jairo and Victor Adankai) for their contribution, support, teachings and advice.
Also, gratitude to my colleagues who helped me out; Jacques Fleischer, David Umanzor and Sheimy Paz Serpa. gratitude to my colleagues.
Lastly, appreciation to Dr. Byron Greene, the Florida A&amp;amp;M University, the Indiana University and Bethune Cookman University for providing a platform to be able to learn new things and embark on new projects.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Akram, Muhammad et al. &amp;ldquo;Awareness and current knowledge of breast cancer.&amp;rdquo; Biological research vol. 50,1 33. 2 Oct. 2017, doi:10.1186/s40659-017-0140-9&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Wallden, Brett et al. &amp;ldquo;Development and verification of the PAM50-based Prosigna breast cancer gene signature assay.&amp;rdquo; BMC medical genomics vol. 8 54. 22 Aug. 2015, doi:10.1186/s12920-015-0129-6&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Breast Cancer.org Prosigna Breast Cancer Prognostic Gene Signature Assay. &lt;a href="https://www.breastcancer.org/symptoms/testing/types/prosigna">https://www.breastcancer.org/symptoms/testing/types/prosigna&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>L. Hussain, W. Aziz, S. Saeed, S. Rathore and M. Rafique, &amp;ldquo;Automated Breast Cancer Detection Using Machine Learning Techniques by Extracting Different Feature Extracting Strategies,&amp;rdquo; 2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE), 2018, pp. 327-331, doi: 10.1109/TrustCom/BigDataSE.2018.00057.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Halalli, Bhagirathi et al. &amp;ldquo;Computer Aided Diagnosis - Medical Image Analysis Techniques.&amp;rdquo; 20 Dec. 2017, doi: 10.5772/intechopen.69792&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Salod, Zakia, and Yashik Singh. &amp;ldquo;Comparison of the performance of machine learning algorithms in breast cancer screening and detection: A protocol.&amp;rdquo; Journal of public health research vol. 8,3 1677. 4 Dec. 2019, doi:10.4081/jphr.2019.1677Articles&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>WHO, WHO issues first global report on Artificial Intelligence (AI) in health and six guiding principles for its design and use. &lt;a href="https://www.who.int/news/item/28-06-2021-who-issues-first-global-report-on-ai-in-health-and-six-guiding-principles-for-its-design-and-use">https://www.who.int/news/item/28-06-2021-who-issues-first-global-report-on-ai-in-health-and-six-guiding-principles-for-its-design-and-use&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Mertins, Philipp et al. &amp;ldquo;Proteogenomics connects somatic mutations to signalling in breast cancer.&amp;rdquo; Nature vol. 534,7605 (2016): 55-62. doi:10.1038/nature18003&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Kehinde Ezekiel, Project Code, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-362/blob/main/project/code/final_breastcancerproject.ipynb">https://github.com/cybertraining-dsc/su21-reu-362/blob/main/project/code/final_breastcancerproject.ipynb&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Kaggle_breast_cancer_proteomes &amp;laquo;&lt;a href="https://pastebin.com/A0Wj41DP%3E">https://pastebin.com/A0Wj41DP&amp;gt;&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Proteomes_clustering_analysis &lt;a href="https://www.kaggle.com/shashwatwork/proteomes-clustering-analysis">https://www.kaggle.com/shashwatwork/proteomes-clustering-analysis&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Detection of Autism Spectrum Disorder with a Facial Image using Artificial Intelligence</title><link>/report/su21-reu-378/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-378/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-378/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-378/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final: Project&lt;/p>
&lt;p>Myra Saunders, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-378">su21-reu-378&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-378/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Utilized CNN Code: &lt;a href="https://github.com/cybertraining-dsc/su21-reu-378/blob/main/project/code/autism_classification.ipynb">autism_classification.ipynb&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This project uses artificial intelligence to explore the possibility of using a facial image analysis to detect Autism in children. Early detection and diagnosis of Autism, along with treatment, is needed to minimize some of the difficulties that people with Autism encounter. Autism is usually diagnosed by a specialist through various Autism screening methods. This can be an expensive and complex process. Many children that display signs of Autism go undiagnosed because there families lack the expenses needed to pay for Autism screening and diagnosing. The development of a potential inexpensive, but accurate way to detect Autism in children is necessary for low-income families. In this project, a Convolutional Neural Network (CNN) is utilized, along with a dataset obtained from Kaggle. This dataset consists of collected images of male and female, autistic and non-autistic children between the ages of two to fourteen years old. These images are used to train and test the CNN model. When one of the images are received by the model and importance is assigned to various features in the image, an output variable (autistic or non-autistic) is received.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-related-work">2. Related Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-proposed-methodology">4. Proposed Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-results">5. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusions-and-future-work">7. Conclusions and Future Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Autism Spectrum Disorder, Detection, Artificial Intelligence, Deep Learning, Convolutional Neural Network.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Autism Spectrum Disorder (ASD) is a broad range of lifelong developmental and neurological disorders that usually appear during early childhood. Autism affects the brain and can cause challenges with speech and nonverbal communication, repetitive behaviors, and social skills. Autism Spectrum Disorder can occur in all socioeconomic, ethnic, and racial groups, and can usually be detected and diagnosed from the age of three years old and up. As of June 2021, the World Health Organization has estimated that one in 160 children have an Autism Spectrum Disorder worldwide&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Early detection of Autism, along with treatment, is crucial to minimize some of the difficulties and symptoms that people with Autism face&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Symptoms of Autism Spectrum Disorder are normally identified based on psychological criteria&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Specialists use techniques such as behaivoral observation reports, questionaires, and a review of the child&amp;rsquo;s cognitive ability to detect and diagose Autism in children.&lt;/p>
&lt;p>Many researchers believe that there is a correlation between facial morphology and Autism Spectrum Disorder, and that people with Autism have distinct facial features that can be used to detect their Autism Spectrum Disorder&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Human faces encode important markers that can be used to detect Autism Spectrum Disorder by analyzing facial features, eye contact,facial movements, and more&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. Scientists found that children diagnosed with Autism share common facial feature distinctions from children who are not diagnosed with Autism&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Some of these facial features are wide-set eyes, short middle region of the face, and a broad upper face. Figure 1 provides an example of the facial feature differences between a child with Autism and a child without.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/raw/main/project/images/Autistic%20compared%20with%20Non-Autistic%20(4).png" alt="Autistic and Non-Autistic Child">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Image of Child with Autism (left) and Child with no Autism (right)&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Due to the distinct features of Autistic individuals, we believe that it is necessary to explore the possiblities of using a facial analysis to detect Autism in children, using Artificial Intelligence (AI). Many researchers have attempted to explore the possibility of using various novel algorithms to detect and diagnose children, adolescents, and adults with Autism&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Previous research has been done to determine if Autism Spectrum Disorder can be detected in children by analyzing a facial image&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. The author of this research collected approximately 1500 facial images of children with Autism from websites and Facebook pages associated with Autism. The facial images of non-autistic children were randomly downloaded from online and cropped.The author aimed to provide a first level screening for autism diagnosis, whereby parents could submit an image of their child and in return recieve a probability of the potential of Autism, without cost.&lt;/p>
&lt;p>To contribute to this previous research&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>, this project will propose a model that can be used to detect the presence of Autism in children based on a facial image analysis.
A deep learning algorithm will be used to develop an inexpensive, accurate, and effective method to detect Autism in children. This project implements and utilizes a Convolutional Neural Network (CNN) classifier to explore the possibility of using a facial image analysis to detect Autism in children, with an accuracy of 95% or higher. Most of the coding used for this CNN model was obtained from the Kaggle dataset and was done by Fran Valuch&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. We made changes to some parts of this code, which will be discussed further in this project. The goal of this project is not to diagnose Autism, but to explore the possibility of detecting Autism at its early stage, using a facial image analysis.&lt;/p>
&lt;h2 id="2-related-work">2. Related Work&lt;/h2>
&lt;p>Previous work exists on the use of artificial intelligence to detect Autism in children using a facial image. Most of this previous work used the Autism kaggle dataset&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>, which was also used for this project. One study utilized MobileNet followed by two dense layers in order to perform deep learning on the dataset&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. MobileNet was used because of its ability to compute outputs much faster, as it can reduce both computation and model size. The first layer was dedicated to distribution, and allowed customisation of weights to input into the second dense layer. The second dense layer allowed for classification. The architecture of this algorithm is shown below in Figure 2.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/raw/main/project/images/2021-07-29.png" alt="Algorithm Architecture using MobileNet">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Algorithm Architecture using MobileNet&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Training of this model completed after fifteen epochs, which resulted in a test accuracy of 94.64%. In this project we utilize a classic Convolutional Neural Network model using tensorflow. This will be done in hopes of obtaining a test accuracy of 95% or higher.&lt;/p>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>The dataset used for this project was obtained from Kaggle&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. This dataset contained approximately 1500 facial images of children with Autism that were obtained from websites and Facebook pages associated with Autism. The facial images of non-autistic children were randomly downloaded from online. The pictures obtained were not of the best quality or consistency with respect to the facial alignment. Therefore, the author developed a python program to automatically crop the images to include only the extent possible for a facial image. These images consist of male and female children that are of different races and range from around ages two to fourteen.&lt;/p>
&lt;p>This project uses version 12 of this dataset, which is the latest version. The dataset consists of three directories labled test, train, and valid, along with a CSV file. The training set is labeled as train, and consists of &amp;lsquo;Autistic&amp;rsquo; and &amp;lsquo;Non-Autistic&amp;rsquo; subdirectories. These subdirectories contain 1269 images of autistic and 1269 images of non-autistic children respectively. The validation set located in the valid directory are separated into &amp;lsquo;Autistic&amp;rsquo; and &amp;lsquo;Non-autistic&amp;rsquo; subdirectories. These subdirectories also contain 100 images of autistic and 100 images of non-autistic children respectively. The testing set located in the test directory is divided into 100 images of autistic children and 100 images of non-autistic children. All of the images provided in this dataset are in 224 X 224 X 3, jpg format. Table 1 provides a summary of the content in the dataset.&lt;/p>
&lt;p>&lt;strong>Table 1:&lt;/strong> Summary Table of Dataset.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/raw/main/project/images/DATASET%20BREAKDOWN.png" alt="Summary Table of the Kaggle Dataset">&lt;/p>
&lt;h2 id="4-proposed-methodology">4. Proposed Methodology&lt;/h2>
&lt;p>Convolutional Neural Network (CNN)&lt;/p>
&lt;p>This project utilizes a Convolution Neural Network (CNN) to develop a program that can be used to detect the presence of Autism in children from a facial image analysis. If successful this program can be used an inexpensive method to detect Autism in children at its early stages. We believed that a CNN model would be the best way create this program because of its little dependence on preprocessing data. A Convolutional Neural Network was also used becuase of its ability to take in an image and assign importance to, and identify diferent objects within the image. CNN also has very high accuracy when dealing with image recognition. The dataset used contains 1269 training images that were used to train and test this Convolution Neural Network model. The architecture of this model can be seen in Figure 3.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/raw/main/project/images/CNN%20Architecture.png" alt="CNN Architecture">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Architecture of utilized Convolutional Neural Network Model.&lt;/p>
&lt;h2 id="5-results">5. Results&lt;/h2>
&lt;p>The results of this project is estimated by affectability and accuracy by utilizing the Confusion Matrix CNN. The results also rely on how correct and precise the model was trained. This model was created to explore the possibility of detecting Autism in children at its early stage, using a facial image analysis. A Convolutional Neural Network classifier was used to create this model. For this CNN model we utilized max pooling and Rectified Linear Unit (ReLU), with two epochs. This resulted in an accuracy of 71%. These results can be seen below in Figure 4. Figure 5 displays some of the images that were classified and labeled correctly (right) and the others that were labeled incorrectly (left).&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/raw/main/project/images/Training%20and%20Validation%20Loss%20and%20Accuracy%20(3).png" alt="Results after Execution">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Results after Execution.&lt;/p>
&lt;p>validation loss: 57% - validation accuracy: 68% - training loss: 55% - training accuracy: 71%&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/raw/main/project/images/Labels.png" alt="Correct and Incorrect Labels">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Correct Labels and Incorrect Labels.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Figure 6 shows the Confusion Matrix of the Convolutional Neural Network model used in this project. The Confusion Matrix displays a summary of the model&amp;rsquo;s predicted results after its attempt to classify each image as either autistic or non-autistic. Out of the 200 images, 159 of the images were labled correctly and 41 of the images were labled incorrectly.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/raw/main/project/images/Confusion%20Matrix%20CNN.png" alt="Confusion Matrix CNN">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Confusion Matrix of the Convolutional Neural Network model.&lt;/p>
&lt;p>Cloudmesh-common&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> was used to create a Stopwatch module, that was used to measure and study the training and testing time of the model. Table 2 shows the cloudmesh benchmark output.&lt;/p>
&lt;p>&lt;strong>Table 2:&lt;/strong> Cloudmesh Benchmark&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Status&lt;/th>
&lt;th>Time&lt;/th>
&lt;th>Sum&lt;/th>
&lt;th>Start&lt;/th>
&lt;th>tag&lt;/th>
&lt;th>msg&lt;/th>
&lt;th>Node&lt;/th>
&lt;th>User&lt;/th>
&lt;th>OS&lt;/th>
&lt;th>Version&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Train&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>3745.28&lt;/td>
&lt;td>3745.28&lt;/td>
&lt;td>2021-08-10 16:08:57&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>dab8db0489cd&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Sat Jun 5 09:50:34 PDT 2021&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Test&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>2.088&lt;/td>
&lt;td>2.088&lt;/td>
&lt;td>2021-08-10 17:43:09&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>dab8db0489cd&lt;/td>
&lt;td>collab&lt;/td>
&lt;td>Linux&lt;/td>
&lt;td>#1 SMP Sat Jun 5 09:50:34 PDT 2021&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="7-conclusions-and-future-work">7. Conclusions and Future Work&lt;/h2>
&lt;p>Autism Spectrum Disorder is a broad range of lifelong developmental and neurological disorders that is considered one of the most growing disorders in children. The World Health Organization has estimated that one in 160 children have an Autism Spectrum Disorder worldwide&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Techniques that are used by specialists to detect autism can be time consuming and inconvenient for some families. Considering these factors, finding effective and essential ways to detect Autism in children is a neccesity. The aim of this project was to create a model that would analyze facial images of children, and in return determine if the child is Autistic or not. This was done in hopes of receiving 95% accuracy or higher. After executing the model we received an accuracy of 71%.&lt;/p>
&lt;p>As shown in the results section above, some of the pictures that were initially labeled as Autistic, were labeled incorrectly after running the model. This low accuracy rate could be improved if the CNN model is combined with other algorithms such as transfer learning and VGG-19. This low accuracy could also be improved by using a dataset that includes a wider variety and larger amount of images. We could also ensure that images in the dataset includes children that are of a wider age range. These improvements could possibly increase our chances of obtaining an accuracy of 95% or higher. When this model is improved and an accuracy of atleast 95% is achieved, furture work can be done to create a model that can be used for Autistic individuals outside of the dataset age range (2 - 14 years old).&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>The author of this project would like to express a vote of thanks to Yohn Jairo, Carlos Theran, and Dr. Gregor von Laszewski for their encouragement and guidance throughout this project. A special vote of thanks also goes to Florida A&amp;amp;M University for funding this wonderful research program. The completion of this project could not have been possible without their support.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>World Health Organization. 2021. Autism spectrum disorders, [Online resource] &lt;a href="https://www.who.int/news-room/fact-sheets/detail/autism-spectrum-disorders">https://www.who.int/news-room/fact-sheets/detail/autism-spectrum-disorders&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Raj, S., and Masood, S., 2020. Analysis and Detection of Autism Spectrum Disorder Using Machine Learning Techniques, [Online resource &lt;a href="https://reader.elsevier.com/reader/sd/pii/S1877050920308656?token=D9747D2397E831563D1F58D80697D9016C30AAC6074638AA926D06E86426CE4CBF7932313AD5C3504440AFE0112F3868&amp;amp;originRegion=us-east-1&amp;amp;originCreation=20210704171932">https://reader.elsevier.com/reader/sd/pii/S1877050920308656?token=D9747D2397E831563D1F58D80697D9016C30AAC6074638AA926D06E86426CE4CBF7932313AD5C3504440AFE0112F3868&amp;amp;originRegion=us-east-1&amp;amp;originCreation=20210704171932&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Khodatars, M., Shoeibi, A., Ghassemi, N., Jafari, M., Khadem, A., Sadeghi, D., Moridian, P., Hussain, S., Alizadehsani, R., Zare, A., Khosravi, A., Nahavandi, S., Acharya, U. R., and Berk, M., 2020. Deep Learning for Neuroimaging-based Diagnosis and Rehabilitation of Autism Spectrum Disorder: A Review. [Online resource] &lt;a href="https://arxiv.org/pdf/2007.01285.pdf">https://arxiv.org/pdf/2007.01285.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Musser, M., 2020. Detecting Autism Spectrum Disorder in Children using Computer Vision, Adapting facial recognition models to detect Autism Spectrum Disorder. [Online resource] &lt;a href="https://towardsdatascience.com/detecting-autism-spectrum-disorder-in-children-with-computer-vision-8abd7fc9b40a">https://towardsdatascience.com/detecting-autism-spectrum-disorder-in-children-with-computer-vision-8abd7fc9b40a&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Akter, T., Ali, M. H., Khan, I., Satu, S., Uddin, Jamal., Alyami, S. A., Ali, S., Azad, A., and Moni, M. A., 2021. Improved Transfer-Learning-Based Facial Recognition Framework to Detect Autistic Children at an Early Stage. [Online resource] &lt;a href="https://www.mdpi.com/2076-3425/11/6/734">https://www.mdpi.com/2076-3425/11/6/734&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Beary, M., Hadsell, A., Messersmith, R., Hosseini, M., 2020. Diagnosis of Autism in Children using Facial Analysis and Deep Learning. [Online resource] &lt;a href="https://arxiv.org/ftp/arxiv/papers/2008/2008.02890.pdf">https://arxiv.org/ftp/arxiv/papers/2008/2008.02890.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Piosenka, G., 2020. Detect Autism from a facial image. [Online resource] &lt;a href="https://www.kaggle.com/gpiosenka/autistic-children-data-set-traintestvalidate?select=autism.csv">https://www.kaggle.com/gpiosenka/autistic-children-data-set-traintestvalidate?select=autism.csv&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Valuch, F., 2021. Easy Autism Detection with TF.[Online resource] &lt;a href="https://www.kaggle.com/franvaluch/easy-autism-detection-with-tf/comments">https://www.kaggle.com/franvaluch/easy-autism-detection-with-tf/comments&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark - Cloudmesh-Common, [GitHub] &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Analyzing the Advantages and Disadvantages of Artificial Intelligence for Breast Cancer Detection in Women</title><link>/report/su21-reu-377/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-377/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-377/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-377/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-377/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-377/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>RonDaisja Dunn, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-377">su21-reu-377&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-377/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The AI system is improving its diagnostic accuracy by significantly decreasing unnecessary biopsies. AI&amp;rsquo;s algorithms for workflow improvement and outcome analyses are advancing. Although artificial intelligence can be beneficial to detecting and diagnosing breast cancer, there are some limitations to its techniques. The possibility of insufficient quality, quantity or appropriateness is possible. When compared to other imaging modalities, breast ultrasound screening offers numerous benefits, including a cheaper cost, the absence of ionizing radiation, and the ability to examine pictures in real time. Despite these benefits, reading breast ultrasound is a difficult process. Different characteristics, such as lesion size, shape, margin, echogenicity, posterior acoustic signals, and orientation, are used by radiologists to assess US pictures, which vary substantially across individuals. The development of AI systems for the automated detection of breast cancer using Ultrasound Screening pictures has been aided by recent breakthroughs in deep learning.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-methods-from-literature-review">2. Methods From Literature Review&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-results-from-literature-review">3. Results From Literature Review&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-datasets">4. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgments">6. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> project, reu, breast cancer, Artificial Intelligence, diagnosis detection, women, early detection, advantages, disadvantages&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The leading cause of cancer death in women worldwide is breast cancer. This deadly form of cancer has impacted many women across the globe. Specifically, African American women have been the most negatively impacted. Their death rates due to breast cancer have surpassed all other ethnicities. Serial screening is an essential part in detecting Breast cancer. Detecting the early stages of this disease and decreasing mortality rates is most effective by utilizing serial screening. Some women detect that they could have breast cancer by discovering a painless lump in their breast. Other women began to detect that there may be a problem due to annual and bi-annual breast screenings. Screening in younger women is not likely, because breast cancer is most likely to be detected in older women. Women from the age 55 to 69 are likely to be diagnosed with breast cancer. Women who frequently participate in receiving mammograms reduce the chance of breast cancer mortality.&lt;/p>
&lt;p>Artificial Intelligence is the branch of computer science dedicated to the development of computer algorithms to accomplish tasks traditionally associated with human intelligence, such as the ability to learn and solve problems. This branch of computer science coincides with diagnosing breast cancer in individuals because of the use of radiology. Radiological images can be quantitated and can inform and train some algorithms. There are many terms that relate to Artificial Intelligence such as artificial neural networks (ANNs), machine and deep learning (ML, DL). These techniques complete duties in healthcare, including radiology. Machine learning interprets pixel data and patterns from mammograms. Benign or malignant features for inputs are defined by microcalcifications. Deep learning is effective in breast imaging, where it can identify several features such as edges, textures, and lines. More intricate features such as organs, shapes, and lesions can also be detected. Neural networks algorithms are used for image feature extractions that cannot be detected beyond human recognition.&lt;/p>
&lt;p>A computer system that can perform complicated data analysis and picture recognition tasks is known as artificial intelligence (AI). Both massive processing power and the application of deep learning techniques made this possible, and are increasingly being used in the medical field. Mammograms are the x-rays used to detect breast cancer in women. Early detection is important to reduce deaths, because that is when the cancer is most treatable. Screenings have presented a 15%-35% false report in screened women. Errors and the ability to view the cancer from the human eye are the reasons for the false reports. Artificial Intelligence offers many advantages when detecting breast cancer. These advantages include less false reports, fewer cases missed because the AI program does not get tired and it reduces the effort of reading thousands of mammograms.&lt;/p>
&lt;h2 id="2-methods-from-literature-review">2. Methods From Literature Review&lt;/h2>
&lt;p>The goal was to emphasize the present data in terms of test accuracy and clinical utility results, as well as any gaps in the evidence. Women are screened by getting photos taken of each breast from different views. Two readers are assigned to interpret the photographs in a sequential order. Each reader decides whether the photograph is normal or whether a woman should be recalled for further examination. Arbitration is used when there is a disagreement. If a woman is recalled, she will be offered extra testing to see if she has cancer.&lt;/p>
&lt;p>Another goal is to detect cancer at an earlier stage during screening so that therapy can be more successful. Some malignancies found during screening, on the other hand, might never have given the woman symptoms. Overdiagnosis is a term used to describe a situation in which a person has caused harm to another person during their lifetime. As a result, overtreatment (unnecessary treatment) occurs. Since some malignancies are overlooked during screening, the women are misled.&lt;/p>
&lt;p>The methods in diagnostic procedures vary between radiologists and Artificial Intelligence networks. In a breast ultrasound exam, radiologists look for abnormal abnormalities in each image, while AI networks analyze each image in an exam that is processed separately using a ResNet-18 model, and a saliency map is generated, identifying the most essential sections. With radiologists, the focus is on photos with abnormal lesions and with AI networks the image is given an attention score based on its relative value. To make a final diagnosis, radiologists consider signals in all photos, and AI computes final predictions for benign and malignant results by combining information from all photos using an attention technique.&lt;/p>
&lt;h2 id="3-results-from-literature-review">3. Results From Literature Review&lt;/h2>
&lt;p>Using pathology data, each breast in an exam was given a label indicating the presence of cancer. Image-guided biopsy or surgical excision were used to collect tissues for pathological tests. The AI system was shown to perform comparably to board-certified breast radiologists in the reader study subgroup. In this reader research, the AI system detected tumors with the same sensitivity as radiologists, but with greater specificity, a higher PPV, and a lower biopsy rate. Furthermore, the AI system outperformed all ten radiologists in terms of AUROC and AUPRC. This pattern was replicated in the subgroup study, which revealed that the algorithm could correctly interpret Ultrasound Screening examinations that radiologists considered challenging.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-377/raw/main/project/images/IMG_9446.jpg" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Analysis of saliency maps on a qualitative level- This figure displays the sagittal and transverse views of the lesion (left) and the AI&amp;rsquo;s saliency maps indicating the anticipated sites of benign (center) and malignant (right) findings in each of the six instances (a-f) from the reader study.&lt;/p>
&lt;h2 id="4-datasets">4. Datasets&lt;/h2>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-377/main/project/images/Dataset%20Image.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> The probabilistic forecasts of each hybrid model were randomly divided to fit the reader&amp;rsquo;s sensitivity. The dichotomization of the AI&amp;rsquo;s predictions matches the sensitivity of the average radiologists. Readers' AUROC, AUPRC, specificity, and PPV improve as a result of the collaboration between AI and readers, whereas biopsy rates decrease.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>There are some benefits of AI help with mammogram screenings. The reduction in treatment expenses is one of the advantages of screening. Treatment for people who are diagnosed sooner is less invasive and expensive, which may lessen patient anxiety and improve their prognosis. One or all human readers could be replaced by AI. AI may be used to pre-screen photos, with only the most aggressive ones being reviewed by humans. AI could be employed as a reader aid, with the human reader relying on the AI system for guidance during the reading process.&lt;/p>
&lt;p>However, there is also fear that AI could discover changes that would never hurt women. Because the adoption of AI systems will alter the current screening program, it&amp;rsquo;s crucial to determine how accurate AI is in breast screening clinical practice before making any changes. It&amp;rsquo;s uncertain how effective AI is at detecting breast cancer in different sorts of women or in different groups of women (for example different ethnic groups). AI could significantly minimize staff workload, as well as the proportion of cancers overlooked during screening, and the amount of women who are asked to return for more tests despite the fact that they do not have cancer. According to the findings of the reader survey, such teamwork between AI systems and radiologists increases diagnosis accuracy and decreases false positive biopsies for all 10 radiologists. This research indicated that integrating the Artificial intelligence system&amp;rsquo;s predictions enhanced the performance of all readers.&lt;/p>
&lt;h2 id="6-acknowledgments">6. Acknowledgments&lt;/h2>
&lt;p>Thank you to the extremely intellectual, informative, patient and courteous instructors of the Research Experience for Undergraduates Program.&lt;/p>
&lt;ol>
&lt;li>Carlos Theran, REU Instructor&lt;/li>
&lt;li>Yohn Jairo Parra, REU Instructor&lt;/li>
&lt;li>Gregor von Laszewski, REU Instructor&lt;/li>
&lt;li>Victor Adankai, Graduate Student&lt;/li>
&lt;li>REU Peers&lt;/li>
&lt;li>Florida Agricultural and Mechanical University&lt;/li>
&lt;/ol>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;ol>
&lt;li>Coleman C. Early Detection and Screening for Breast Cancer. Semin Oncol Nurs. 2017 May;33(2):141-155. doi: 10.1016/j.soncn.2017.02.009. Epub 2017 Mar 29. PMID: 28365057&lt;/li>
&lt;li>Freeman, K., Geppert, J., Stinton, C., Todkill, D., Johnson, S., Clarke, A., &amp;amp; Taylor-Phillips, S. (2021, May 10). Use of Artificial Intelligence for Image Analysis in Breast Cancer Screening. &lt;a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/987021/AI_in_BSP_Rapid_review_consultation_2021.pdf">https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/987021/AI_in_BSP_Rapid_review_consultation_2021.pdf&lt;/a>&lt;/li>
&lt;li>Li, J., Zhou, Z., Dong, J., Fu, Y., Li, Y., Luan, Z., &amp;amp; Peng, X. (2021). Predicting breast cancer 5-year survival using machine learning: A systematic review. PloS one, 16(4), e0250370.&lt;/li>
&lt;li>Mendelsonm, Ellen B., Artificial Intelligence in Breast Imaging: Potentials and Limitations. American Journal of Roentgenology 2019 212:2, 293-299&lt;/li>
&lt;li>Seely, J. M., &amp;amp; Alhassan, T. (2018). Screening for breast cancer in 2018-what should we be doing today?. Current oncology (Toronto, Ont.), 25(Suppl 1), S115â€“S124.&lt;/li>
&lt;li>Shamout, F. E., Shen, A., Witowski, J., Oliver, J., &amp;amp; Geras, K. (2021, June 24). Improving Breast Cancer Detection in Ultrasound Imaging Using AI. NVIDIA Developer Blog. &lt;a href="https://developer.nvidia.com/blog/improving-breast-cancer-detection-in-ultrasound-imaging-using-ai/">https://developer.nvidia.com/blog/improving-breast-cancer-detection-in-ultrasound-imaging-using-ai/&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Report: Increasing Cervical Cancer Risk Analysis</title><link>/report/su21-reu-369/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-369/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-369/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-369/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Theresa Jean-Baptistee, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-369">su21-reu-369&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-369/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Cervical Cancer is an increasing matter that is affecting various women across the nation, in this project we will be analyzing risk factors that are producing
higher chances of this cancer. In order to analyize these risk factors a machine learning technique is implemented to help us understand the leading factors of
cervical cancer.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#model">Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. DataSets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#iud-visulaization">IUD Visulaization&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tabacoo-visulization-affect-on-cervixs">Tabacoo Visulization Affect On Cervixs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correlation-of-age-and-start-of-sexual-activity">Correlation of Age and Start Of sexual activity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-other-people-works">3. Other People Works&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4--explantion-of-confusion-matrix">4. Explantion of Confusion Matrix&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-benchmark">5. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgments">7. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Cervical, Cancer, Diseases, Data, conditions&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Cervical cancer is a disease that is increasing in various women nationwide. It occurs within the cells of the cervix (can be seen in stage 1 of the image below).
This cancer is the fourth leading cancer, where there are about 52,800 cases found each year, predominantly being in lower developed countries. Cervical cancer
occurs most commonly in women who are within their 50&amp;rsquo;s and who has symptoms such as watery and bloody discharge, bleeding, and painful intercourse. Two other
common causes can be an early start on sexual activity and multiple partners. The most common way to determine if one may be affected by this disease is through a
pap smear. When witnessed early it, can allow a better chance of results and treatment.&lt;/p>
&lt;p>Cervical cancer is so important for the future of reproduction, being the cause of a successful or unsuccessful birth with completions like premature a child. The
cervix help keeps the fetus stable within the uterus during this cycle, towards the end of development, it softens and dilates for the birth of a child. If
diagnosed with this cancer, a miracle would be needed to conceive a child after having treatment. Most treatments begin with a biopsy removing affected areas of
cervical tissue. As it continues, to spread radiotherapy might be recommended to treat the cancer where may affect the womb. lastly, one may need to have a
hysterectomy which is the removal of the womb.&lt;/p>
&lt;p>In this paper, we will study the exact cause and risk factors that may place someone in this position. If spotted early it wouldnâ€™t affect someoneâ€™s dream chance
of conceiving or affect their reproductive parts. Using various data sets we will study the way everything may alignes in causes and machine leaning would be the
primary technique to used interpretate the relation between variables and risk factor on cervical cancer.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/raw/main/project/images//Cervical-Cancer-1024x624.jpg" alt="Figure 2">&lt;/p>
&lt;h2 id="model">Model&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/raw/main/project/images/Screen%20Shot%202021-07-27%20at%207.40.36%20PM.png" alt="Figure 1">&lt;/p>
&lt;h2 id="2-datasets">2. DataSets&lt;/h2>
&lt;p>The Data sets obatained shows the primary risk factors that affect women ages 15 and above. The few factors that sticked out the most were age, start of sexual
activity, tabacoo intake, and IUD. The age and start of sexual activity maybe primary factor because a person is more liable to catch an STD and get this diease
from mutiple parnters never really knowing what the other person may be doing outside of the encounterment. Tabcoo intake causes an affect making a person by
weaking the immune system and making somone more septable to the disease. The IUD has the highest number on the data set being a primary factor that may put a
person at risk, this device aids the prevention of pregency by thickneing the mucos of the cervix that could later cause infection or make your more spetiable to
them.&lt;/p>
&lt;h2 id="iud-visulaization">IUD Visulaization&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/raw/main/project/images/cervical%20iud%20.jpg" alt="figure 2">&lt;/p>
&lt;h2 id="tabacoo-visulization-affect-on-cervixs">Tabacoo Visulization Affect On Cervixs&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/raw/main/project/images/tab.png" alt="Figure 3">&lt;/p>
&lt;h2 id="correlation-of-age-and-start-of-sexual-activity">Correlation of Age and Start Of sexual activity&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/raw/main/project/images/download-2021-06-29T15-34-01-628Z.png" alt="Figure 1">&lt;/p>
&lt;h2 id="3-other-people-works">3. Other People Works&lt;/h2>
&lt;p>The research of others work has made a huge imapact to this project starting from data to important knowledge needed to conduct the project. With the various
research sites, we were able to witness what the affects various day to day activtie affect women long term. The Cervical Cancer Diagnosis Using a Chicken Swarm
Optimization Based Machine Learning Method, was a big aid throught the project explaing the stages of cervical cancer, ways it can be treated, and the affects it
may cause. With the data that was used from UCI Machine Learning, we were able to find efficent correlation into the data, helping the implented machine learning algorithm for the classification task.&lt;/p>
&lt;h2 id="4--explantion-of-confusion-matrix">4. Explantion of Confusion Matrix&lt;/h2>
&lt;p>The confusion matrix generated by multilayer perceptron can be explained as the perdicted summary results from the data obtained. Zero is when no cervical cancer is witnessed, one is when cervical cancer is seen. A hundrend and sixty-two is the highest number of this disease seen on the chart and the lowest number being winessed is two and eight being quiet of a jump.&lt;/p>
&lt;h2 id="5-benchmark">5. Benchmark&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/raw/main/project/images/graph%5C.png" alt="figure4">&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>In conclusion it can be found as women partake in their first sexual activity and continue they are more at risk. 162 is a dramatic number not necessarily being affected by age and 0 is only seen when a person does not partake in it. In the future I hope to keep furthering my Knowledge on Cervical Cancer, hopefully coming up with a realistic method to cure this disease where one can continue to live their life with as a human being.&lt;/p>
&lt;h2 id="7-acknowledgments">7. Acknowledgments&lt;/h2>
&lt;p>The author would like to thank Yohn, Carlos, Gregor, Victor, and Jacques for all of their Help. Thank you!&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2></description></item><item><title>Report: Report: Dentronics: Classifying Dental Implant Systems by using Automated Deep Learning</title><link>/report/su21-reu-376/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-376/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-376/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-376/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-376/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-376/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Jamyla Young, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-376">su21-reu-376&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-376/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Artificial intelligence is a branch of computer science that focuses on building and programming machines to think like humans and mimic their actions. The proper concept definition of this term cannot be achieved simply by applying a mathematical, engineering, or logical approach but requires an approach that is linked to a deep cognitive scientific inquiry. The use of machine-based learning is constantly evolving the dental and medical field to assist with medical decision making process.In addition to diagnosis of visually confirmed dental caries and impacted teeth, studies applying machine learning based on artificial neural networks to dental treatment through analysis of dental magnetic resonance imaging, computed tomography, and cephalometric radiography are actively underway, and some visible results are emerging at a rapid pace for commercialization.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-dental-implant-classification">2.1 Dental implant classification&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-deep-convulutional-neural-network">2.2 Deep Convulutional Neural Network&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-results">3. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-conclusion">4. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-acknowledgments">5. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-references">6. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Dental implants, Deep Learning, Prosthodontics, Implant classificiation, Artificial Intelligence, Neural Networks.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Dental implants are ribbed oral protheses typically made up of biocompatible titanium to replace the missing root(s) of an absent tooth. These dental protheses are used to support the jaw bone to prevent deterioration due to an absent root&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This is referred to as bone resorption which can result to facial malformation as well as reduced oral function such as biting and chewing. These devices are composed of three elements that imitates a natural tooth function and structure.The implant which are typically ribbed and threaded to promote stability while integrating within the bone tissue. The osseointegration process usually takes 6-8 months to rebuild the bone to support the implant. An implant abutment is fixed on top of the implant to act as a base for prosthetic devices &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Prefabricated abutments are manufactured in many shapes, sizes and angles depending on the location of the implant and the types of prothesis that will be attached. Dental abutments support a range of prothetic devices such as dental crowns, bridges, and dentures &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Osseointegrated dental implants depend on various factors that affect the anchorage of the implant to the bone tissue. Successful surgical anchoring techniques can contribute to long term success of implant stability. Primary stability plays a role 2 week postoperatively by achieving mechanical retention of the implant. It helps establish a mechanical microenvironment for gradual bone healing, or osseointegration-This is secondary implant stability. Bone type, implant length, implant and diameter influences primary and secondary implant stability. Implant length can range from 6mm to 20mm; however, the most common lengths are between 8mm to 15mm. Many studies suggest that implant length contribute to decreasing bone stress and increasing implant stability. Bone stress can occur at both the cortical and cancellous part of the bone. Increasing implant length will decrease stress in the cancellous part of the bone while increasing the implant diameter can decrease stress in the cortical part of the bone&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Bone type can promote positive bone stimulation around an implant improving the overall function. There are four different types: Type I, Type II, Type III, and Type IV. Type I is the most dense of them which provides more cortical anchorage but has limited vascularity. Type II is the best for osseointegration because it provides good cortical anchorage and has better vascularity than type I. Type III and IV have a thin layer of cortical bone which decrease the success rate of primary stability&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Implant stability can be measured using the Implant Stability Quotient (ISQ) as an indirect indicator to determine the time frame for implant loading and prognostic indicator for implant failure &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This can be measured by resonance frequency analysis (RFA) immediately after the implant has been placed. Resonance frequency analysis is the measurement in which a device vibrates in response to frequencies in the range of 5-15 kHz. The peak amplitude of the response is then encoded into the implant stability quotient (ISQ). The clinical range of ISQ is from 55-80. High stability is &amp;gt;70 ISQ while medium stability is between 60-69 ISQ. Low stability is &amp;lt;60 ISQ&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>There are over 2000 types of dental implant systems (DIS) that differs in diameter, length, shape, coating, and surface material properties. These devices have more than a 90% long termed survival rate which ranges more than 10 years. Inevitably, biological and mechanical complications such as fractures, low implant stability, and screw loosening can occur. Therefore, identifying the correct Dental Implant System is essential to repair or replace the existing system. Methods and techniques that enables clear identification is insufficient &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Artificial intelligence is a branch of computer science that focuses on building and programming machines to think like humans and mimic their actions. A deep convolutional neural network (DCNN) is a brach of artificial intelligence that applies multiple layers of nonlinear processing units for feature extraction, transformation, and classification of high dimensional datasets. Deep convolutional neural networks are commonly used to identify patterns in images and videos. The structure typically consist of four types of layers: convolution, pooling, activation, and fully connected. These neural networks use images as an input to train a classifier which employs a mathematical operation called a convolution. Deep neural networks have been successfully applied in the dental field and demonstrated advantages in terms of diagnosis and prognosis. Using automated deep convolutional neural networks is highly efficient in classifying different dental implant systems compared to most dental professionals&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="2-data-sets">2. Data sets&lt;/h2>
&lt;p>Researchers at Daejon Dental Hospital used automated deep convolutional neural networks to evaluate the efficacy of its ability to classify dental implant systems and compare the performance with dental professionals using radiographic images.&lt;/p>
&lt;p>11,980 raw panoramic and periapical radiographic images of dental implant systems were collected. these images were then randomly divided into 2 groups: 9584 (80%) images were selected for the training dataset and the remaining 2396 (20%) images were used as the testing dataset.&lt;/p>
&lt;h2 id="21-dental-implant-classification">2.1 Dental implant classification&lt;/h2>
&lt;p>Dental implant systems were classified into six different types with a diameter of 3.3-5.0mm and a length of 7-13mm.&lt;/p>
&lt;ul>
&lt;li>Astra OsseoSpeed TX (Dentsply IH AB, Molndal, Sweden), with a diameter of 4.5â€“5.0 mm and a length of 9â€“13 mm;&lt;/li>
&lt;li>Implantium (Dentium, Seoul, Korea), with a diameter of 3.6â€“5.0 mm and a length of 8â€“12 mm;&lt;/li>
&lt;li>Superline (Dentium, Seoul, Korea), with a diameter of 3.6â€“5.0 mm and a length of 8â€“12 mm;&lt;/li>
&lt;li>TSIII (Osstem, Seoul, Korea), with a diameter of 3.5â€“5.0 mm and a length of 7â€“13 mm;&lt;/li>
&lt;li>SLActive BL (Institut Straumann AG, Basel, Switzerland), with a diameter of 3.3â€“4.8 mm and a length of 8â€“12 mm;&lt;/li>
&lt;li>SLActive BLT (Institut Straumann AG, Basel, Switzerland), with a diameter of 3.3â€“4.8 mm and a length of 8â€“12 mm.&lt;/li>
&lt;/ul>
&lt;h2 id="22-deep-convulutional-neural-network">2.2 Deep Convulutional Neural Network&lt;/h2>
&lt;p>Using Neuro-T to automatically select the model and optimize hyper-parameter. During training and inference, the automated DCNN automatically creates effective deep learning models and searches the optimal hyperparameters. An Adam optimizer with L2 regularization was used for transfer learning. The batch size was set to 432, and the automated DCNN architecture consisted of 18 layers with no dropout.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-376/raw/main/project/images/DCNN.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1&lt;/strong>: Overview of an automated deep convolutional neural network &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-results">3. Results&lt;/h2>
&lt;p>For the evaluation, the following statistical parameters were taken into account: receiver operating characteristic (ROC) curve, area under the ROC curve (AUC), 95% confidence intervals (CIs), standard error (SE), Youden index (sensitivity + specificity âˆ’ 1), sensitivity, and specificity, which were calculated using Neuro-T and R statistical software . Delongâ€™s method was used to compare the AUCs generated from the test dataset, and the significance level was set at p &amp;lt; 0.05.&lt;/p>
&lt;p>The accuracy of the automated DCNN abased on the AUC, Youden index, sensitivity, and specificity
for the 2,396 panoramic and periapical radiographic images were 0.954(95% CI = 0.933â€“0.970,
SE = 0.011), 0.808, 0.955, and 0.853, respectively. Using only panoramic radiographic images (n = 1429),
the automated DCNN achieved an AUC of 0.929 (95% CI = 0.904â€“0.949, SE = 0.018, Youden index = 0804,
sensitivity = 0.922, and specificity = 0.882), while the corresponding value using only periapical
radiographic images (n = 967) achieved an AUC of 0.961 (95% CI = 0.941â€“0.976, SE = 0.009, Youden
index = 0.802, sensitivity = 0.955, and specificity = 0.846). There were no significant differences in
accuracy among the three ROC curves.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-376/raw/main/project/images/results.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2&lt;/strong>: The accuracy of the automated DCNN for the test dataset did not show a significant difference among the three ROC three ROC curves based on DeLongâ€™s method &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The Straumann SLActive BLT implant system has a relatively large tapered shape compared to other types of DISs. Thus, the automated DCNN (AUC = 0.981, 95% CI = 0.949â€“0.996). However, for the Dentium Superline and Osstem TSIII implant systems that do not have conspicuous characteristic elements with a tapered shape, the automated DCNN classified correctly with an AUC of 0.903 (95% CI = 0.850â€“0.967) and 0.937 (95% CI = 0.890â€“0.967)&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-376/raw/main/project/images/results2.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3 (a-f)&lt;/strong>: Performance of the automated DCNN and comparison with dental professionals for
classification of six types of DIS &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="4-conclusion">4. Conclusion&lt;/h2>
&lt;p>Nonetheless, this study has certain limitations. Although six types of DISs were selected from three different dental hospitals and categorized as a dataset, the training dataset was still insufficient for clinical practice. Therefore, it is necessary to build a high-quality and large-scale dataset containing different types of DISs. If time and cost are not limited, the automated DCNN can be continuously trained and optimized for improved accuracy. Additionally, the automated DCNN regulates the entire process, including appropriate model selection and optimized hyper-parameter adjustment. The automated DCNN can help clinical dental practitioners to classify various types of DISs based on dental radiographic images. Nevertheless, further studies are necessary to determine the efficacy and feasibility of applying the automated DCNN in clinical practice.&lt;/p>
&lt;h2 id="5-acknowledgments">5. Acknowledgments&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Carlos Theran, REU Instructor&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Yohn Jairo Parra, REU Instructor&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Gregor von Laszewski, REU Instructor&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Victor Adankai, Graduate Student&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Jacques Fleischer, REU peer&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Florida Agricultural and Mechanical University&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="6-references">6. References&lt;/h2>
&lt;p>[^3] Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Karras, Spiro, Look at the structure of dental implants.(2020, September 2).
&lt;a href="https://www.drkarras.com/a-look-at-the-structure-of-dental-implants/">https://www.drkarras.com/a-look-at-the-structure-of-dental-implants/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Ghidrai, G. (n.d.). Dental implant abutment. Stomatologia pe intelesul tuturor. &lt;a href="https://www.infodentis.com/dental-implants/abutment.php">https://www.infodentis.com/dental-implants/abutment.php&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Bataineh, A. B., &amp;amp; Al-Dakes, A. M. (2017, January 1). The influence of length of implant on primary stability: An in vitro study using resonance frequency analysis. Journal of clinical and experimental dentistry. &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5268121/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5268121/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Huang, H., G, Wu., &amp;amp; E, Hunziker. (2020). The clinical significance of implant Stability QUOTIENT (ISQ) MEASUREMENTS: A literature review. Journal of oral biology and craniofacial research. &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7494467/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7494467/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Li, J., Yin, X., Huang, L., Mouraret, S., Brunski, J. B., Cordova, L., Salmon, B., &amp;amp; Helms, J. A. (2017, July). Relationships among Bone QUALITY, IMPLANT Osseointegration, and WNT SIGNALING. Journal of dental research &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5480808/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5480808/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>MÃ¶hlhenrich, S. C., Heussen, N., Modabber, A., Bock, A., HÃ¶lzle, F., Wilmes, B., Danesh, G., &amp;amp; Szalma, J. (2020, July 24). Influence of bone density, screw size and surgical procedure on orthodontic mini-implant placement â€“ part b: Implant stability. International Journal of Oral and Maxillofacial Surgery. &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0901502720302496">https://www.sciencedirect.com/science/article/abs/pii/S0901502720302496&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Lee JH, Kim YT, Lee JB, Jeong SN. A Performance Comparison between Automated Deep Learning and Dental Professionals in Classification of Dental Implant Systems from Dental Imaging: A Multi-Center Study. Diagnostics (Basel). 2020 Nov 7;10(11):910. doi: 10.3390/diagnostics10110910. PMID: 33171758; PMCID: PMC7694989.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Analyzing Hashimoto disease causes, symptoms and cases improvements using Topic Modeling</title><link>/report/su21-reu-372/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-372/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-372/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-372/workflows/Status/badge.svg" alt="Status">&lt;/a>
&lt;code>Status: final&lt;/code>, Type: Project&lt;/p>
&lt;p>Sheimy Paz, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-372">su21-reu-372&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Code:
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/blob/main/project/code/requirements.txt">Install documentation requirements.txt&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/blob/main/Tyroidhitis_Project.ipynb">Tyroidhitis_Project.ipynb&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This project proposes a new view of Hashimotoâ€™s disorder, its association with other pathologies, possible causes, symptoms, diets, and recommendations. The intention is to explore the association of Hashimoto disorder with disease like h pylori bacteria, inappropriate diet, environmental factors, and genetic factors. To achieve this, we are going to utilize AI in particular
topic modeling which is a technic used to process large collection of data to identifying topics.
Topic modeling is a text-mining tool that help to correlate words with topics making the research process easy and organized with the purpose to get a better understanding of the disorder and the relationship that this has with other health issues hoping to find clear information about the causes and effect that can have on the human body.
The dataset was collected from silo breaker software, which contains information about news, reports, tweets, and blogs. The program will organize our findings highlighting key words related to symptoms, causes, cures, anything that can apport clarification to the disorder.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-summary-tables">2. Summary Tables&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-datasets">3. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-results">4. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-hashimoto-findings">5. Hashimoto Findings&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Thyroid disease, Hashimoto, H Pylori, Implants, Food Sensitivity, Diary sensitivity, Healthy Diets, Exercise, topic modeling, text mining, BERT model.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Hashimoto thyroiditis is an organ-specific autoimmune disorder. its symptoms were first described 1912 but the disease was not recognized until 1957. Hashimoto is an autoimmune disorder that destroys thyroid cells and is antibody-mediated &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. In a female-to-men radio at least 10:4 women are more often affected than men. The diagnostic is often called between the ages of 30 to 50 years &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Pathologically speaking, Hashimoto stimulates the formation of antithyroid antibodies that attack the thyroid tissue, causing progressive fibrosis. Hashimoto is believe to be the concequence of a combination of mutated genes and eviromental factors&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The disorder is difficult to diagnose since in the early course of the disease the patients may or may not exhibit symptoms or laboratory findings of hyperthyroidism, it may show normal values because the destruction of the gland cells may be intermittent &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Clinical and epidemiological studies suggest worldwide that the most common cause of hypothyroidism is an inadequate dietary intake of iodine.&lt;/p>
&lt;p>Due to the arduous labor to identify this disorder a Machine Learning algorithm based on prediction would help to identify Hashimoto in early stages as well as any other health issues related to it &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This will be helpful for patients that would be able to get the correct treatment in an early stage of the illness avoiding future complications. This research algorithm was mainly intended to find patient testimonies of improvements, completed healed cases, early symptoms, trigger factors or any useful information about the disorder.&lt;/p>
&lt;p>Hashimoto autoimmune diseases have been linked to the infection caused by H pylori bacteria. H pylori is until the date the most common chronic bacterial infection, affecting half of the world&amp;rsquo;s population and is known for the presence of Caga antigens which are virulent strains that have been found in organ and non-organ specific autoimmune diseases &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Another important trigger of Hashimoto disorder is the inadequate modern diet patterns and the environmental factors that are closely related to it &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. For instance, western diet consumption is an essential factor that trigs the disorder since this food is highly preserved and predominate the consumption of artificial flavors and sugars which have dramatically increase in the past years, adding to it the use of chemicals and insecticides in the fruits and vegetables and the massive introduction of hormones for meat production, all this can be the cause of the rise of autoimmune diseases &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We utilize deep learning BERT model to train our dataset. BERT is a superior performer Bidirectional Encoder, which superimposes 12 or 24 layers of multiheaded attention in a Transformer &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Bert stands for Bidirectional(read from left to right and vice versa with the purpose of an accurate understanding of the meaning of each word in a sentence or document) Encoder Representations from Transformers(the used of transformers and bidirectional models allows the learning of contextual relations between words). Notice that BERT uses two training strategies MLM and NSP.&lt;/p>
&lt;p>Masked Lenguage Model (MLM) process is made by masking around 15% of token making the model predict the meaning or value of each of the masked words. In technical word it requires 3 steps Adding a classification layer on top of the encoder output, Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension. And lastly calculating the probability of each word in the vocabulary with SoftMax. Here we can see an image of the process &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/MLMBertexPic1.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>MLM Bert Figure:&lt;/strong> &amp;ldquo;Masked Language Model Figure Example&amp;rdquo; &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Next Sentence Prediction (NSP) process is based in sentence prediction. The model obtains pair of sentences as inputs, and it is train to predict which is the second sentence in the pair. In The training process 50% of the input sentences are in fact first and second sentence and in the other 50% the second sentences are random sentences used for training purposes.
The model is able to distinguish if the second sentence is connected to the first sentence by a 3-step process. An CLS (the reserved token to represent the start of sequence) is inserted at the beginning of the first sentence while the SEP (separate segments or sentence) is inserted at the end of each sentence. And embedding indicating sentence A or B is added to each token, and lastly a positional embedding is added to each token to indicate its position in the sequence like is shown on the image &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/NSPBertexpic.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>NSP Bert Figure:&lt;/strong> &amp;ldquo;Next Sentence Prediction Figure Example&amp;rdquo; &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>By the trained model Parameter learning we obtains the word embeddings of the input sentence or input sentence pair in the unsupervised learning framework proceeds by solving the following two tasks: Masked Language Model and Next Sentence Prediction.&lt;/p>
&lt;p>We try to use Bert model in the small dataset Hashimoto without any success because the BERT model was overfitting the data points. We use LDA model to train the Hashimoto dataset which allow us to find topic probabilities that we compare with the thyroiditis dataset that was trained with the BERT model-framework.&lt;/p>
&lt;p>We used Natural Language Toolkit (NLTK) which is a module that uses the process of splitting sentences from paragraph, split words, recognizing the meaning of those words, to highlighting the main subjects, with the purpose to help to understand the meaning of the document &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. For instance, in our NLTK model we used two data sets &lt;a href="https://drive.google.com/drive/u/0/folders/1Omtnn5e-yH3bbhW0-5fIbLgi8SEyfYBP">Hashimoto and thyroiditis&lt;/a> and we were able to identify the top 30 topics connected to these disorders. From the information collected we were able to identify general information like association of the disorder with other health issues. The impact of Hashimoto patient with covid19, long term consequences of untreated Hashimoto, recommendation for advance cases, and diet suggestion for improvement. The used of Natural Language tool kit made a precise and less time consuming research process.&lt;/p>
&lt;h2 id="2-summary-tables">2. Summary Tables&lt;/h2>
&lt;p>We can observe in this table the differences between this two similar disorders that are frequently misunderstood.&lt;/p>
&lt;p>&lt;strong>Summary Table 1:&lt;/strong> &amp;ldquo;Differences Between Hashimoto&amp;rsquo;s Thyroiditis and Grave&amp;rsquo;s Disease&amp;rdquo; &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/hertoghe-table-2.jpg" alt="Summary Table 1">&lt;/p>
&lt;p>&lt;strong>Summary Table 2:&lt;/strong> &amp;ldquo;Hashimotoâ€™s thyroiditis is associated with other important disorders&amp;rdquo; &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/Thyroiditis%20Associated%20Pathologies.jpg" alt="Summary Table 2">&lt;/p>
&lt;p>&lt;strong>Summary Table 3:&lt;/strong> &amp;ldquo;Overview of the main dietary recommendations for patients with Hashimoto&amp;rdquo; &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/Dietary%20changes%20that%20reduce%20Antithyroid%20Antibody%20levels.jpg" alt="Summary Table 3">&lt;/p>
&lt;h2 id="3-datasets">3. Datasets&lt;/h2>
&lt;p>Silobreaker software was used to obtain scientific information related to the Hashimoto disease coming from different sources such as journals, proceedings, tweets, and news. Our date consists in the fallowing feature: ID, cluster Id, Description, publication date, Source URL, publisher. And the purpose is to analyze the preform of the proposed approach to discover the hiding semantic structures related with Hashimoto and thyroiditis the description from the gather data is used to study the frequency of Hashimoto and thyroiditis appears in the documents and detecting words and phrases patterns within them to automatically clustering work groups.&lt;/p>
&lt;p>The dataset was obtained from Silobreaker database which is a commercial database. We got access through Florida A&amp;amp;M University who provided me the right to query the data. the link for the silobreaker information is [Here] (&lt;a href="https://www.silobreaker.com/">https://www.silobreaker.com/&lt;/a>) &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This data was preprocessed dropping the columns &amp;lsquo;Id&amp;rsquo;, &amp;lsquo;ClusterId&amp;rsquo;, &amp;lsquo;Language&amp;rsquo;, &amp;lsquo;LastUpdated&amp;rsquo;,&amp;lsquo;CreatedDate&amp;rsquo;,&amp;lsquo;FirstReported&amp;rsquo;. Also, stop words and punctuation were removed, we convert to lower case all the titles.&lt;/p>
&lt;p>The dataset already query can be download in my personal drive [Here](&lt;a href="https://drive.google.com/drive/u/0/folders/1Omtnn5e-yH3bbhW0-5fIbLgi8SEyfYBP">https://drive.google.com/drive/u/0/folders/1Omtnn5e-yH3bbhW0-5fIbLgi8SEyfYBP&lt;/a>.&lt;/p>
&lt;h2 id="4-results">4. Results&lt;/h2>
&lt;p>The following figures were creating with the help of libraries like gensim. Gensim stands for Generate similar and is an unsupervised library wide used for topic modeling and natural language processing based on modern statistical machine learning &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. it can handle large text collections of data and can preforms task like corpora, building document, word vectors and topic identification, which is one of the technics we used here, and we can observe it in some of the images. Each figure is described and explains the method we used to created it along with the relationship of the key word or major topic to the Hashimoto disorder.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/wordCloudObject.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> &amp;ldquo;Example of a Word Cloud Object&amp;rdquo;&lt;/p>
&lt;p>On Figure 1 we observed an example of a word cloud object and represent the difference words found in our dataset and the size of the words means the frequency of the given words in the document. Meaning that the size of the words is proportional to the frequency of its used.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/IntertopicDistanceMap.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> &amp;ldquo;Example of a Intertopic Distance Map&amp;rdquo;&lt;/p>
&lt;p>Figure 2 shows an Intertopic Distance Map which is a two-dimensional space filled with circles representing the proportional number of words that belongs to each topic making the distance to each other represent the relation between the topics, meaning that topics that are closer together have more words in common. For instance, in topic 1 we observed word like hypothyroidism, Morgan, symptoms after a small search we were able to find that Morgan is a well-known writer that presented thyroiditis symptoms after giving birth which is something that happen to some womenâ€™s and then recover after a couple of months, however this increments the risk of developing the syndrome later in their lives &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. On topic 4 we see words like food, levothyroxine, liothyronine, selenium and dietary. the relationship between these words is symptom control, symptoms relive, some natural remedies and supplements &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>&lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/topic%20modeling%20picture.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> &amp;ldquo;Top 30 major Topics&amp;rdquo;&lt;/p>
&lt;p>On figure 3 we observed a bar chart that shows 30 major terms. The bars indicate the total frequency of the term across the entire corpus. The size of the bubble measures the importance of the topics, relative to the data. for example, for visualization purposes we used the first topic that include Hashimoto, thyroiditis, and selenium. Saliency is a measure of how much the term talks about the topic. And in terms of findings is important to mentions the relationship between Hashimoto thyroiditis and selenium. Selenium is a suplement recomended for patients with this disorder that have shown a reduction on antibody levels &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/HierarchicalClustering.png" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> &amp;ldquo;Example of Hierarchical Clustering chart&amp;rdquo;&lt;/p>
&lt;p>On figure 4 we can see that the dendrograms have been created joining points 4 with 9, 0 with 2, 1 with 6, and 12 with 13. The vertical height of the dendrogram shows the Euclidean distances between points. It is easy to see that Euclidean distance between points 12 and 13 is greater than the distance between point 4 and 9. This is because the algorithm is clustering by similarity, differences, and frequency of words. We observed in the dark green dendrogram topic 7,3,4,9 which are all related to an advance stage of the disorder. we can find the information about certain treatments, causes of the disorder, level of damage at certain stages. On the reds dendrograms we observe topics 0,2,1,6 which are closely related to diagnosis, early symptoms and procedures used for the diagnosis of the disorder.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/SimilarityMatrix.png" alt="Figure 5">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> &amp;ldquo;Exaple of Similarity Matrix Chart&amp;rdquo;&lt;/p>
&lt;p>On figure 5 we can see a similarity matrix chart, the graph is build based on similarity reached from the volume of topic and association by document, therefore the graph show groups of documents that are cluster together based on similarities. in this case the blue square is an indication of a strong similarity, and the green and light green is an indication of different topics. for instance, we are able to derive as a conclusion that carcinoma cancer, carcinoma therapy, lymph papillary metastasis and hypothyroidism are closely related. in facts they are advance stages of the disorder. E.g. Carcinoma therapy is a type of treatment that can be used for this disorder &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/TermScoreDeclinePerTopic.png" alt="Figure 6">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> &amp;ldquo;Example of Term Score Decline Per Topic Chart&amp;rdquo;&lt;/p>
&lt;p>On figure 6 we observed TF-IDF which is an interesting technic used on machine learning that have the ability to give weight to those words that are not frequent in the document but can carry important information. In this example we can see how topic 12, covid19 pandemic patients is the at the top of the chart and then start declining when the rank term increase. The science behind this behave is explain by the TF-IDF which is term frequency - Inverse document frequency. Therefore, covid 19 was a relative new disease, and we do not expect to have a high frequency used in the document. In this case we were able to find information about Hashimoto patients and covid19 which it seems not to causes any extreme symptoms for patient with this disorder others than the ones expected from a healthy person in other words Hashimoto patients have the same risk of a healthy person &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/TopicProbability.png" alt="Figure 7">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> &amp;ldquo;Example of Topic Probability chart&amp;rdquo;&lt;/p>
&lt;p>On figure 7 we see a probability distribution chart based on each topic frequency and its relationship with the main topic: Hashimoto thyroiditis causes or cure. We can see that topic 12 is the least frequent or least related since most of its content is about covid19. Then we have topic 11 zebrafish which is related to the investigation of the disorder but most of its content is about the research made on zebrafish and how had help researchers to understand thyroid diseases in other no mammalsâ€™ animals, but is not closely related to the major point of this project, however, is an interesting research which have provide useful information about thyroiditis &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/topicwordscore.png" alt="Figure 8">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> &amp;ldquo;Example of a Topic Word Score Chart&amp;rdquo;&lt;/p>
&lt;p>On figure 8 we have Topic Word Scores chart that provides a deep understanding of large corpus of texts trough topic extraction. for instance, the data used in this project provide 5 fundamental topics from 0 to 4. Essentially each topic provided closely related words with deep information about the disorder itself, treatments, diagnosis, and symptoms. E.g. in topic number 4 we find a specific word &amp;ldquo;eye&amp;rdquo; which it does not seem to have a close relationship with Hashimoto thyroiditis but in facts is related to one of the early symptoms that the human body experiment most likely when is still undiagnosed [16]. In the same topic we also find the word teprotumumab which is an eye relieve medication recommended from doctors to relive the symptoms, in other word is not the cure but it helps &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="5-hashimoto-findings">5. Hashimoto Findings&lt;/h2>
&lt;p>As we can see our findings are wide in aspects of causes which is one of the main keys, because if we know the cause of something most likely we will be able to avoid it. However, this disorder is considered relative knew and have been around for some decades only, but it is necessary to point out the relation of diseases with the environment. Environmental changes are a fact and are affecting us every day even when we donâ€™t notice it. We have seen an exponential increase of Hashimoto cases in the last five decades, and at the same time the last five decades have been potentially related to climate change, high levels of pollution, less fertile soils, increased use of pesticides on food, etc. It would be a good idea to think about our environment and how to help it heal since it will bring benefits for all of us&lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Table Summary:&lt;/strong> &amp;ldquo;Finding summary on causes, descriptions and recommendations.&amp;rdquo;&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Possible Causes&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Recomendations&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Genetic predispositions&lt;/td>
&lt;td>Genetically linked&lt;/td>
&lt;td>Manage stress&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dietary errors&lt;/td>
&lt;td>Imbalance of iodine intake&lt;/td>
&lt;td>Balance is key&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Nutritional deficiencies&lt;/td>
&lt;td>not enough veggies, vitamins and minerals&lt;/td>
&lt;td>eat more veggies&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hormone deficiencies&lt;/td>
&lt;td>lover levels of vit D&lt;/td>
&lt;td>Enough sleep, Take some sun light&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Viral, bacterial, yeast, and parasitic infections.&lt;/td>
&lt;td>H pylori, Bad guts microbes&lt;/td>
&lt;td>food hygiene&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Enviromental Factors&lt;/td>
&lt;td>Pollution, Pesticides used etc.&lt;/td>
&lt;td>Human footprint on environment&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Possible causes&lt;/strong>&lt;/p>
&lt;p>Genetic predispositions, Dietary errors, Nutritional deficiencies, Hormone deficiencies, Viral, bacterial, yeast, and parasitic infections &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Hashimoto Trigger Food&lt;/strong>&lt;/p>
&lt;p>Some Food that can trigger Hashimoto are gluten, dairy, some type of grains, eggs, nuts or nightshades, sugar, sweeteners, sweet fruits, including honey, agave, maple syrup, and coconut sugar and high-glycemic fruits like watermelon, mango, pineapple, grapes, canned and dried fruits. Vegetable oil, specially hydrogenated oils, ad trans-fat. Patient with this disorder may experience symptoms of fatigue, rashes, joint pain, digestive issues, headaches, anxiety, and depression after eating some of these foods &lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Hashimoto recommended Diets&lt;/strong>&lt;/p>
&lt;p>The recommended foods are healthy fats like coconut, avocado, and olive oil, ghee, grass-fed and organic meat, wild fish, healthy fats, fermented foods like coconut yogurt, kombucha, fermented cucumbers and pickle ginger, and plenty of vegetable like Asparagus, spinach, lettuce, broccoli, beets, cauliflower, carrots, celery, artichokes, garlic, onions &lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Environmental causes of Hashimoto&lt;/strong>&lt;/p>
&lt;p>There have been an increase in the number of Hashimoto cases in the United States since 1950s. These is one of the reason research explain that Hashimoto disorder can be closely related to environmental causes since the rapid increase of cases can not only be related to family gens as it takes at least two generations to acquire and transfer gen mutation. Adding to this that for generation thru history human have been fitting microorganisms than enter our body but for the past centuries our environment has become very hygienic consequently our immune system suddeling was left without aggressors therefore humane start developing more allergies and autoimmune diseases.
Another important factor is the balance of iodine intake because too much is as dangerous for people with genetic Hashimoto predisposition but too littler can be also dangerous for patients with the disorder to reduce goiter which is the enlargement of the thyroid glands &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>It is still not enough research to state that low vitamin D levels are a cause or a consequence of the Hashimoto disorder, but it is a fact that most patients with this disorder have low levels of vitamin D this insufficient this is closely related to insufficient sun exposure &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The exposure to certain synthetic pesticide. An important fact is that 9 out of 12 pesticides are dangerous and persistent pollutants &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Symptoms of Hashimotoâ€™s&lt;/strong>&lt;/p>
&lt;p>Some of the symptoms are fatigue and sluggishness, sensitivity to cold, constipation, pale and dry skin, dry eyes, puffy face, brittle nails, hair loss, enlargement of the tongue, unexplained weight gain, muscle aches, tenderness and stiffness, joint pain and stiffness, muscle weakness, excessive or prolonged menstrual bleeding, depression, memory lapses, Another symptom reported by some patients was ablation, some patient described as an acceleration of the heart rhythm &lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Complications&lt;/strong>&lt;/p>
&lt;p>Tissue damage, Abnormal look of the thyroid gland (figure 2), goiter, Heart problems, mental health issues, myxedema, birth defects &lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup>, Nodule (figure 4 Similarity Matrix topic 3), and High antibody level. It is important to mention an association between high levels of thyroid autoantibodies and the increased of mood disorders, thyroid autoimmunity disease, celiac disease, panic disorder and major depressive disorder &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Recomendations&lt;/strong>&lt;/p>
&lt;p>Healthy diets, exercising, selenium supplementation [8], healthy sun exposure at an adequate time, getting enough sleep is primordial for the human body, in special for the metabolism regulation and the creation of normal hormones that the human body needs, &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> lowering stress levels by physical exercise is a good idea, exercise like yoga and reiki are valuable because it also exercise you brain with meditation which is a great stress reliever.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>We used benchmark to perform the process time to get topics frequency in parallel using google colab with run type: GPU and TPU. We can observe that TPU machines take less time to classify topic 1. Tensor Processor Unit (TPU) is designed to run cutting-edge machine learning models with AI services on Google Cloud &lt;sup id="fnref:21">&lt;a href="#fn:21" class="footnote-ref" role="doc-noteref">21&lt;/a>&lt;/sup>&lt;/p>
&lt;p>&lt;strong>Benchmark Topics Frequency:&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>parallel Topic&lt;/th>
&lt;th>Status&lt;/th>
&lt;th>Time&lt;/th>
&lt;th>processor&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>164 1_cancer_follicular_carcinoma_autoimmune&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.53&lt;/td>
&lt;td>GPU&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>190 1_cancer_follicular_carcinoma_autoimmune&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.002&lt;/td>
&lt;td>TPU&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>As expected, we were able to derive helpful information of the Hashimoto thyroiditis disorder. we attempted to summarize our findings concerning Hashimoto thyroiditis in aspects of causes, symptoms, recommended diets and supplements and used medication.&lt;/p>
&lt;p>Our findings highlight the great potential of the model we used. certainly, topic modeling method was a precise idea for the optimization of the research process. We also used various features of genism, which allows to manipulate data texts on NPL projects. The use of clustering technics was very useful to label our findings on the large datasets. Each used graph provided useful details and key words that later help us to review each important topic in a faster manner and develop the research project with accurate results.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Gregor von Laszewski&lt;/p>
&lt;p>Yohn J Parra&lt;/p>
&lt;p>Carlos Theran&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, [Online resource]
&lt;a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Helicobacter pylori infection in women with Hashimoto thyroiditis, [Online resource]
&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5265752/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5265752/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>I. Voloshyna, V.I Krivenko, V.G Deynega, M.A Voloshyn, Autoimmune thyrod disease related to helicobacterer pylori contamination, [Online resource]
&lt;a href="https://www.endocrine-abstracts.org/ea/0041/eposters/ea0041gp213_eposter.pdf">https://www.endocrine-abstracts.org/ea/0041/eposters/ea0041gp213_eposter.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>How your diet can trigger Hashimoto&amp;rsquo;s, [Online resource]
&lt;a href="https://www.boostthyroid.com/blog/2019/4/5/how-your-diet-can-trigger-hashimotos">https://www.boostthyroid.com/blog/2019/4/5/how-your-diet-can-trigger-hashimotos&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Hypothyroidism in Context: Where Weâ€™ve Been and Where Weâ€™re Going, [Online resource]
&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822815/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822815/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>BERT Explained: State of the art language model for NLP
&lt;a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Pavan Sanagapati, Knowledge Graph &amp;amp; NLP Tutorial-(BERT,spaCy,NLTK), [Online resource]
&lt;a href="https://www.kaggle.com/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk">https://www.kaggle.com/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Hashimotoâ€™s Thyroiditis, A Common Disorder in Women: How to Treat It, [Online resource]
&lt;a href="https://www.townsendletter.com/article/441-hashimotos-thyroiditis-common-disorder-in-women/">https://www.townsendletter.com/article/441-hashimotos-thyroiditis-common-disorder-in-women/&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Silobreaker: Intelligent platform for the data era
&lt;a href="https://www.silobreaker.com">https://www.silobreaker.com&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Gensim Tutorial â€“ A Complete Beginners Guide, [Onile resource]
&lt;a href="https://www.machinelearningplus.com/nlp/gensim-tutorial/">https://www.machinelearningplus.com/nlp/gensim-tutorial/&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Julia Haskins, Thyroid Conditions Raise the Risk of Pregnancy Complications, [Online resource]
&lt;a href="https://www.healthline.com/health-news/children-thyroid-conditions-raise-pregnancy-risks-052913">https://www.healthline.com/health-news/children-thyroid-conditions-raise-pregnancy-risks-052913&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>How your diet can trigger Hashimoto&amp;rsquo;s, [Online resource]
&lt;a href="https://www.boostthyroid.com/blog/2019/4/5/how-your-diet-can-trigger-hashimotos">https://www.boostthyroid.com/blog/2019/4/5/how-your-diet-can-trigger-hashimotos&lt;/a>&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Selenium Supplementation for Hashimoto&amp;rsquo;s Thyroiditis, [Online resource]
&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4005265/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4005265/&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Thyroid Cancer Treatment, [Online resource]
&lt;a href="https://www.cancer.gov/types/thyroid/patient/thyroid-treatment-pdq">https://www.cancer.gov/types/thyroid/patient/thyroid-treatment-pdq&lt;/a>&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>Hashimoto&amp;rsquo;s Disease And Coronavirus (COVID-19), [Online resource]
&lt;a href="https://www.palomahealth.com/learn/coronavirus-and-hashimotos-disease">https://www.palomahealth.com/learn/coronavirus-and-hashimotos-disease&lt;/a>&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16" role="doc-endnote">
&lt;p>How zebrafish research has helped in understanding thyroid diseases, [Online resource]
&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5730863/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5730863/&lt;/a>&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17" role="doc-endnote">
&lt;p>Teprotumumab for the Treatment of Active Thyroid Eye Disease, [Online resource]
&lt;a href="https://www.nejm.org/doi/full/10.1056/nejmoa1910434">https://www.nejm.org/doi/full/10.1056/nejmoa1910434&lt;/a>&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18" role="doc-endnote">
&lt;p>11 environmental triggers of Hashimotoâ€™s, [Online research]
&lt;a href="https://www.boostthyroid.com/blog/11-environmental-triggers-of-hashimotos">https://www.boostthyroid.com/blog/11-environmental-triggers-of-hashimotos&lt;/a>&amp;#160;&lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19" role="doc-endnote">
&lt;p>Hashimotoâ€™s low thyroid autoimmune, [Online research]
&lt;a href="https://www.redriverhealthandwellness.com/diet-hashimotos-hypothyroidism/">https://www.redriverhealthandwellness.com/diet-hashimotos-hypothyroidism/&lt;/a>&amp;#160;&lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20" role="doc-endnote">
&lt;p>Hashimoto&amp;rsquo;s disease, [Online research]
&lt;a href="https://www.mayoclinic.org/diseases-conditions/hashimotos-disease/symptoms-causes/syc-20351855">https://www.mayoclinic.org/diseases-conditions/hashimotos-disease/symptoms-causes/syc-20351855&lt;/a>&amp;#160;&lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:21" role="doc-endnote">
&lt;p>TPU: Tensor Processor Unit
&lt;a href="https://cloud.google.com/tpu">https://cloud.google.com/tpu&lt;/a>&amp;#160;&lt;a href="#fnref:21" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Detecting Multiple Sclerosis Symptoms using AI</title><link>/report/su21-reu-371/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-371/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-371/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-371/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-371/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-371/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Raeven Hatcher, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-371">su21-reu-371&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-371/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Multiple sclerosis (M.S.) is a chronic central nervous system disease that potentially affects the brain, spinal cord, and optic nerves in the eyes. People that suffer from M.S had their immune system attacks the myelin (protective sheath) that covers nerve fibers, resulting in communication problems between the brain and the body. The cause of M.S. is unknown; however, researchers believe that genetic and environmental factors play a role in those affected. Symptoms differ significantly from person to person due to varying nerves involved. The most common symptoms include tremors, numbness or weakness in limbs, vision loss, blurry vision, double vision, slurred speech, fatigue, dizziness, involuntary movement, and muscle paralysis. There is currently no cure for Multiple sclerosis and treatment focuses on slowing the progression of the disease and managing symptoms.&lt;/p>
&lt;p>There is no proven way to predict how an individual with M.S will progress certainly. However, researchers established four phenotypes that will assist in identifying those who are more inclined to have disease progression and help aid in more effective treatment targeting. In this experiment, Artificial Intelligence (AI) will be applied by ascertaining what causes these different phenotypes and which phenotype is at most risk for disease progression using a Magnetic Resonance Scan.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-datasets">4. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-benchmark">5. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgments">7. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>MS or Multiple sclerosis is a potentially disabling autoimmune disease that can damage the brain, spinal cord, and optic nerves located in the eyes. It is the most common progressive neurological disability that affects adolescents. The immune system attacks the central nervous system in this disease, specifically myelin (sheth that covers and protects nerve fibers), oligodendrocytes (myelin-producing cells), and the nerve fibers located under myelin. Myelin enables nerves to send and receive electrical signals swiftly and effectively. The myelin sheath becomes scarred from being attacked. These attacks make the myelin sheath inflamed in little patches, observable on an MRI scan. These little inflamed patches potentially disrupt messages moving along the nerves, which ultimately lead to the symptoms of Multiple sclerosis. If these attacks happen frequently, permanent damage can occur to the involved nerve.
Because Multiple sclerosis affects the central nervous system, which control all of the actions carried out in the body, symptoms can affect any part of the body and vary. The most common symptoms of this progressive disease include muscle weakness, pins and needle sensation, electrical shock sensation, loss of bladder control, muscle spasms, tremors, double or blurred vision, partial or total vision loss, to name a few. Researchers are not sure what causes Multiple sclerosis but believe those between the ages of 20 and 40, women, smoke, are exposed to certain infections, have a vitamin D and B12 deficiency, and related to someone affected by this disease are more susceptible.&lt;/p>
&lt;p>It can be difficult to diagnose MS due to the symptoms usually being vague or very similar to other conditions. There is no single test to diagnose it positively. However, doctors can choose a neurological examination, MRI scan, evoked potential test, lumbar puncture, or a blood test to diagnose a patient properly. Currently, clinical practices divide MS into four phenotypes: clinically isolated syndrome (CIS), relapsing-remitting MS (RRMS), primary-progressive MS (PPMS), and secondary progressive MS (SPMS). Two factors define these phenotypes; disease activity (evidenced by relapses or new activity on MRI scan) and progression of disability. Phenotypes are routinely used in clinical trials to choose patients and conduct treatment plans.&lt;/p>
&lt;p>New technologies, such as artificial intelligence and machine learning, help assess multidimensional data to recognize groups with similar features. When implemented in apparent abnormalities on MRI scans, these new technologies have assured promising results in classifying patients who share similar pathobiological mechanisms rather than the typical clinical features.&lt;/p>
&lt;p>Researchers at UCL work with the Artificial intelligence (AI) tool SuStain (Subtype and Stage Inference) to ask whether AI can find Multiple sclerosis subtypes that follow a particular pattern on brain images? The results uncovered three data-driven MS subtypes defined by pathological abnormalities seen on brain images (Skylar). The three data-driven MS subtypes are cortex-led, normal-appearing WM-led, Lesion-led. Cortex-led MS is characterized by early tissue shrinkage (atrophy) in the outer layer of the brain. Normal-appearing WM-led is identified by irregular diffused tissue located in the middle of the brain. Lastly, a lesion-led subtype is characterized by early extension accumulation of brain damage areas that lead to severe atrophy in numerous brain regions. All three of these subtypes correlate to the earliest abnormalities observed on an MRI scan within each pattern.&lt;/p>
&lt;p>In this experiment, researchers utilized the SuStain tool to capture MRI scans of 6,332 patients. The unsupervised SuStain taught itself and identified those three patterns that were previously undiscovered.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://user-images.githubusercontent.com/85815818/126948339-7723b810-83e4-463b-a2b0-bf417fac4458.jpg" alt="projectpic2">
The above image shows the MRI-based subtypes. The color shades range from blue to pink, representing the probability of abnormality mild to severe, respectively. (Eshaghi)
&lt;img src="https://user-images.githubusercontent.com/85815818/127168907-4d7e444f-14bd-4ee3-8027-a7b0e5b7d213.jpg" alt="INSERTPIC">&lt;/p>
&lt;h2 id="4-datasets">4. Datasets&lt;/h2>
&lt;p>MRI brain scans of 6,322 MS patients. look if you can find figures descrbing the data.&lt;/p>
&lt;h2 id="5-benchmark">5. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A vital barrier in distinguishing subtypes in Multiple sclerosis is to stitch observations together from cross-sectional or longitudinal studies. Grouping individuals based wholly on their MRI scan is ineffective because patients belonging to the same subgroup could show ranging abnormalities as the disease progresses and would appear different. SuStaIn, Subtype and Staging Inference, a newly developed unsupervised machine learning algorithm aids in uncovering data-driven disease subtypes that have distinct temporal progression patterns. &amp;ldquo;The ability to disentangle temporal and phenotypic heterogeneity makes SuStain different from other unsupervised learning or clustering algorithms&amp;rdquo; (Eshaghi). SuStaIn identifies subtypes given the data, defined by a particular pattern of variation in a set of features, such as MRI abnormalities. Once the SuStain subtypes and their MRI trajectories are adequately identified, the disease model can conclude how approximately a patient, whose MRI is unseen, belongs to each of the three subtypes and stages.&lt;/p>
&lt;p>A total of 9,390 patients participated in this research study. Six thousand three hundred twenty-two patients were utilized in training, and 3,068 patients were used for the validation dataset. Patient characteristics such as sex, age, disease duration, and expanded disability status scale (EDSS) were similar between the training and validation dataset. There were 18 MRI features measured, 13 of those differed dramatically from those between the MS training dataset and control group and were maintained in the SustaIn model. Three subtypes, with very distinct patterns, were identified in the training dataset and validated in the validation dataset. The early abnormalities noticed by SuStain helped define the three subtypes: cortex-led, normal-appearing white matter-led, and lesion-led.&lt;/p>
&lt;p>There was a statistically significant difference in the rate of the disease progression between the subtypes in the training dataset and validation datasets. The lesion-led subtype held a 30% higher risk of developing 24-week confirmed disability progression (CDP) than the cortex-led subtype in the training dataset. The lesion-led validation dataset had a 32% higher risk of confirmed disability progression than the cortex-led subtype. No other differences in the advancement of disability between subtypes were noted. When SuStaIn was applied to the training and validation dataset, it was pointed out that there were differences in the risk of disability progression between SuStaIn stages.&lt;/p>
&lt;p>Each MRI-based subtype had a different response to treatment, comparing those on treatment and those on placebo. The lesion-led subtype showed a remarkable response to the treatment. Patients on the lesion-led active treatment subtype showed a significantly slower worsening of EDSS than those on the placebo. No differences in the rate of EDSS were observed in those on the placebo compared to active treatment in the NAWM-led and cortex-led subtypes.&lt;/p>
&lt;p>When SuStain was applied to a large set of Multiple sclerosis scans, it identified three subtypes. Researchers found out the patient&amp;rsquo;s baseline subtype and stage were associated with an increased risk of disease progression. Combining clinical information with the MRI-based three subtypes increased the predictive accuracy of just using the MRI scan information alone. The patterns of MRI abnormality in these subtypes provide perspicacity into disease mechanisms, and, alongside clinical phenotypes, they may aid the stratification of patients for future studies.&lt;/p>
&lt;h2 id="7-acknowledgments">7. Acknowledgments&lt;/h2>
&lt;p>The author likes to thank Gregor von Laszewski, Yohn Jairo, and Carlos Theran.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;p>[^6] What Is MS? National Multiple Sclerosis Society. (n.d.). &lt;a href="https://www.nationalmssociety.org/What-is-MS">https://www.nationalmssociety.org/What-is-MS&lt;/a>.&lt;/p></description></item><item><title>Report: Report: AI in Orthodontics</title><link>/report/su21-reu-363/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-363/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-363/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-363/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-363/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-363/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Whitney McNair, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-363">su21-reu-363&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-363/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>In this effort we are analyzing X-ray images in AI and identifying cavitites&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data Sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-figures">3. Figures&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-example-of-a-ai-algorighm-in-orthodontics">4. Example of a AI algorighm in Orthodontics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-benchmark">5. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgments">7. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ai, orthodontics, x-rays.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Dental field technology capability has increased over the past 25 years, and has helped reduce time, cost, medical errors, and dependence on human expertise. Intelligence in orthodontics can learn, build, remember, understand and recognize designs from techniques used in correcting the teeth like retainers. Dental field can create alternatives, adapt to change and explore experiences with sub-groups of patients. AI has taken part of the dental field by accurately and efficiently processing the best data from treatments. For smart use of Health Data, machine learning and artificial intelligence are expected to promote further development of the digital revolution in (dental) medicine, like x-rays, using algorithms to simulate human cognition in the analysis of complex data. The performance is better, the higher the degree of repetitive pattern and the larger the amount of accessible data&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="2-data-sets">2. Data Sets&lt;/h2>
&lt;p>We found a dataset on a kaggle website that is about dental images. The data was collected by Mr. Parth Chokhra. The name of the dataset is Dental Images of kjbjl. The dataset did not have metadata and an explanation of how they collected the data. The data set supports how x-rays of teeth in dentistry becomes artificial intelligence. The Dental Images of kjbjl dataset was used in AI already using autoencoders. Autoencoders are an freely artificial neural network (located in the nervous system) that learns how to accurately encode data and reconstruct the data back from the reduced encoded depiction to a representation that is closes to the original. For some challenges with Orthodontics data sets with privacy, size, avalibility were surprisingly hard to find than we thought.&lt;/p>
&lt;h2 id="3-figures">3. Figures&lt;/h2>
&lt;p>Below we observed actual dental x-rays. These dental x-rays images below came from Parth Chorkhra on kaggle.com &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The images are patient x-rays taken by Parth in his dental imagery data set. In the images we can see the caps and nerves of the teeth. Using these x-rays, we may can also find cavities if there are some. We can also identify other issues with patients teeth by taking and using x-rays.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-363/raw/main/project/images/figure-1.jpg" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> First x-ray&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-363/raw/main/project/images/figure-2.jpg" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Second x-ray&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-363/raw/main/project/images/figure-3.jpg" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Third x-ray&lt;/p>
&lt;h2 id="4-example-of-a-ai-algorighm-in-orthodontics">4. Example of a AI algorighm in Orthodontics&lt;/h2>
&lt;p>On a separate kaggle website, we found a code for DENTAL PANORAMIC KNN &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The kaggle site shows dental codes taken place in Orthodontics.&lt;/p>
&lt;h2 id="5-benchmark">5. Benchmark&lt;/h2>
&lt;p>Here is an algorithm/code from one of the researchers we found. are using to study the performance of their algorithms or code.&lt;/p>
&lt;p>Lateral and frontal facial images of patients who visited the Orthodontic department (352 patients) were employed as the training and evaluation data. An experienced orthodontist examined all the facial images for each patient and identified as many clinically used facial traits during the orthodontic diagnosis process as possible (e.g., deviation of the lips, deviation of the mouth, asymmetry of the face, concave profile, upper lip retrusion, presence of scars). A sample patientâ€™s image, a list of sample assessments (i.e., labels), and the multi-label data used in the work by Murata et al. are shown in Figure 4.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-363/raw/main/project/images/figure-4.jpg" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Sample patientâ€™s image and a list of sample assessments (i.e., labels) including the region of interest, evaluation, etc. In a previous study by Murata et al., they employed labels representing only the facial part (mouth, chin, and whole face), distorted direction (right and left), and its severity (severe, mild, no deviation).&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>Artificial intelligence is rapidly expanding into multiple facets of society. Orthodontics may be one of the fastest branches of dentistry to adapt AI for three reasons. First, patient encounters during treatment generate many types of data. Second, the standardization in the field of dentistry is low compared to other areas of healthcare. A range of valid treatment options exists for any given case. Using AI and large datasets (that include diagnostic results, treatments, and outcomes), one can now measure the effectiveness of different treatment modalities given very specific clinical findings and conditions. Third, orthodontics is largely practiced by independent dentists in their own clinics. Despite the promise of AI, the volume of orthodontic research in this field is relatively low. Further, the clinical accuracy of AI must be improved with an increased number and variety of cases. Before AI can take on a more important role in making diagnostic recommendations, the volume and quality of research data will need to increase&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="7-acknowledgments">7. Acknowledgments&lt;/h2>
&lt;p>Dr. Gregor von Laszewski, Carlos and Yohn guided me throughout this process.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Hasnitadita. (2021, July 10). DENTAL panoramic knn. Kaggle. &lt;a href="https://www.kaggle.com/hasnitadita/dental-panoramic-knn">https://www.kaggle.com/hasnitadita/dental-panoramic-knn&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Chokhra, P. (2020, June 29). Medical image dataset. Kaggle. &lt;a href="https://www.kaggle.com/parthplc/medical-image-dataset">https://www.kaggle.com/parthplc/medical-image-dataset&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analysis of Covid-19 Vaccination Rates in Different Races</title><link>/report/su21-reu-375/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-375/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-375/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-375/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Ololade Latinwo, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-375">su21-reu-375&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-375/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>With the ready availability of COVID-19 vaccinations, it is concerning that a suprising large portion of the U.S. population still refuses to recieve one. In order to control the spread of the pandemic and possibly even erradicate it completely, it is integral that the United States vaccinate as much of the population as possible. Not only does this require ensuring that everyone who wishes to be vaccinated recieves a vaccine, it also requires that those who are unwilling to recieve the vaccine are persuaded to take it. The goal of this report is to analyze the demographics of those who are hesitant to recieve the vaccine and find the reasoning behind their decision. This will make it easier to properly persuade them to recieve the vaccine and aid in raising the United States' vaccination rates.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data Sets&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-covid-19-vaccination-hesitancy-in-all-50-states-by-county">2.1 COVID-19 Vaccination Hesitancy in all 50 States by County&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-vaccination-rates-by-demographic">2.2 Vaccination Rates by Demographic&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-covid-19-vaccination-rates-in-all-50-states">2.3 COVID-19 Vaccination Rates in all 50 States&lt;/a>&lt;/li>
&lt;li>&lt;a href="#24-vaccination-rates-in-all-50-states">2.4 Vaccination Rates in all 50 States&lt;/a>&lt;/li>
&lt;li>&lt;a href="#25-results-of-the-study-done-by-the-journal-of-community-health">2.5 Results of the Study Done by the Journal of Community Health&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-results">3. Results&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-race">3.1 Race&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-sex">3.2 Sex&lt;/a>&lt;/li>
&lt;li>&lt;a href="#33-age">3.3 Age&lt;/a>&lt;/li>
&lt;li>&lt;a href="#34-education-level">3.4 Education Level&lt;/a>&lt;/li>
&lt;li>&lt;a href="#35-location">3.5 Location&lt;/a>&lt;/li>
&lt;li>&lt;a href="#36-political-affiliation">3.6 Political Affiliation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-conclusion">4. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgments">6. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>It has been shown by several economic and health institutions that rates COVID-19 in the United States have been among the highest in the world. Estimates show that about 10 million people have been infected and over a quarter of a million have died in the U.S. by the end of November 2020 &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Fortunately, several pharmaceutical companies such as Pfizer, Moderna, and Johnson &amp;amp; Johnson have managed to create a vaccine by the end of 2020, with several million Americans being given the vaccine by early March. Interestingly, it appears that despite the ready availability of vaccines, a sizeable portion of the population has no intention of receiving either their second does or either dose at all. Voluntarily receiving the COVID-19 vaccine is integral to putting the pandemic to an end, so it is important to explore which demographics are hesitant to receive their vaccine and explore their reasons for doing so. In this project, the variables that will be examined are age, sex, race, education level, location within the United States, and political affiliation.&lt;/p>
&lt;h2 id="2-data-sets">2. Data Sets&lt;/h2>
&lt;p>The first of these larger data sets are the results from a study done in the Journal of Community of Health done by Jagdish Khubchandani, Sushil Sharma, James H. Price, Michael J. Wiblishauser, Manoj Sharma, and Fern J. Webb. In this study, participants of a variety of backgrounds were given a questionnaire regarding whether or not they were likely or unlikely to receive the COVID vaccine. The variables included in the study and explored in this project are sex, age group, race, education level, regional location, and political affiliation. The raw data for this study is not available, however, the study has published the results as percentages. The second and third data sets are two maps of the United States that are provided by the CDC and illustrate the estimated rates of vaccine hesitancy and vaccination progress. Additionally, the United States Census Bureau provided a breakdown of vaccine hesitancy and progress by certain variables, of which age, sex, race and ethnicity, and education level are explored in this project. Like the study done by the Journal of Community Health, the raw data is not available, but percentages are available for each variable.&lt;/p>
&lt;h3 id="21-covid-19-vaccination-hesitancy-in-all-50-states-by-county">2.1 COVID-19 Vaccination Hesitancy in all 50 States by County&lt;/h3>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/b624e0213bad00132fe7ec9762730466aa4210b3/Pictures/USA%20Vaccine%20Hesitancy.jpg" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> An image of the United States illustrating the percentage of adults who are hesitant to recieve a vaccine for COVID-19&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="22-vaccination-rates-by-demographic">2.2 Vaccination Rates by Demographic&lt;/h3>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/b89d46fedf16c94b543512c2e1999a1d6e2d4baa/Pictures/Vaccination%20Rate%20by%20Age.jpg" alt="Figure 2a">&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/b89d46fedf16c94b543512c2e1999a1d6e2d4baa/Pictures/Vaccination%20Rate%20by%20Education%20Level.jpg" alt="Figure 2b">&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/b89d46fedf16c94b543512c2e1999a1d6e2d4baa/Pictures/Vaccination%20Rate%20by%20Race%20and%20Ethnicity.jpg" alt="Figure 2c">&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/b89d46fedf16c94b543512c2e1999a1d6e2d4baa/Pictures/Vaccination%20Rate%20by%20Sex.jpg" alt="Figure 2d">&lt;/p>
&lt;p>&lt;strong>Figures 2a-d&lt;/strong>: Images of vaccine hesitancy rates by certain demographics &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="23-covid-19-vaccination-rates-in-all-50-states">2.3 COVID-19 Vaccination Rates in all 50 States&lt;/h3>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/e13597076f290e67ddc888ec8ac2a7f6fbf8a3ad/Pictures/USA%20Vaccine%20.jpg" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> An image of the United States illustrating the percentage of adults who have recieved at least one dose of the COVID-19 vaccine&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="24-vaccination-rates-in-all-50-states">2.4 Vaccination Rates in all 50 States&lt;/h3>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/4c274d9eff61ea7ed38e57202150b8f1222eef09/Pictures/Hesitancy%20Rate%20by%20Age.jpg" alt="Figure 4a">&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/4c274d9eff61ea7ed38e57202150b8f1222eef09/Pictures/Hesitancy%20Rate%20by%20Education.jpg" alt="Figure 4b">&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/4c274d9eff61ea7ed38e57202150b8f1222eef09/Pictures/Hesitancy%20Rate%20by%20Race%20and%20Ethnicity.jpg" alt="Figure 4c">&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/4c274d9eff61ea7ed38e57202150b8f1222eef09/Pictures/Hesitancy%20Rate%20by%20Sex.jpg" alt="Figure 4d">&lt;/p>
&lt;p>&lt;strong>Figures 4a-d&lt;/strong>: Images of vaccine hesitancy rates by certain demographics&lt;/p>
&lt;h3 id="25-results-of-the-study-done-by-the-journal-of-community-health">2.5 Results of the Study Done by the Journal of Community Health&lt;/h3>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/35296bc3e854e979a0798c57e3d8557e430519a6/Pictures/1.jpg" alt="Figure 5a">
&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/35296bc3e854e979a0798c57e3d8557e430519a6/Pictures/2.jpg" alt="Figure 5b">
&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/35296bc3e854e979a0798c57e3d8557e430519a6/Pictures/3.jpg" alt="Figure 5c">&lt;/p>
&lt;p>&lt;strong>Figure 5a-c&lt;/strong>: Relevant results for the explored variables from a study in vaccine hesitancy rates done by the Journal of Community Health&lt;/p>
&lt;h2 id="3-results">3. Results&lt;/h2>
&lt;h3 id="31-race">3.1 Race&lt;/h3>
&lt;p>In the study done by the Journal of Community Health, Black Americans are shown to have the highest rate of vaccine hesitancy at 34% with White Americans not very far behind at 22%&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Additionally, Asian Americans have the lowest rate of hesitancy at 11%&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Similar results can be seen in the data provided by the US Census Bureau, with Black and White Americans having the highest two vaccination hesitancy rates, however they are shown to be very close, at 10.6% and 11.9%, respectively&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. In Addition, Asian Americans are once again shown to have the lowest vaccination rates at as low as 2.3%&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This information correlates with the data provided by the US Census Bureau, which shows that Black Americans have the lowest rate of vaccination at 72.7% and Asian Americans have the highest rate of vaccination of 94.1%&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="32-sex">3.2 Sex&lt;/h3>
&lt;p>In the study done by the Journal of Community Health, men and women are shown to have very similar rates of vaccine hesitancy at 22% for both groups&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This can also be seen in the data provided by the US Census Bureau with women having a hesitancy rate of 10.3% and men having a barely higher rate of 11.3%&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This goes along well with the vaccination rates provided by the US Census Bureau, which shows that men and women have vaccination rates of 81.4% and 80.5%&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="33-age">3.3 Age&lt;/h3>
&lt;p>The age ranges for both studies are grouped slightly differently, yet, interestingly enough, both data sets show two completely different age groups having the highest rate of hesitancy. In the Journal of Community Health study, the 41-60-year-old group is shown to have the highest rate at 24%&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Yet, in the US Census Bureau&amp;rsquo;s data, ages 25-39 have the highest hesitancy rate at 15.9%&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Because it has a much larger sample size, it is safer to assume that the US Census Bureaus data is more correct in this case. However, both data sets show that seniors have the lowest hesitancy rates, which makes sense as this age group has the highest vaccination rate of 93%&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="34-education-level">3.4 Education Level&lt;/h3>
&lt;p>Once again, the education levels are grouped slightly differently with the US Census Bureau not taking account for those with a Master&amp;rsquo;s degree above and the Journal of Community Health grouping together those who have a high school education and those who have not completed high school. However, unlike the previous variable, very similar results are yielded despite the difference. In both studies, those who had a high school education or below had the highest hesitancy rate at 31% in the Journal of Community Health study and 15.7% for those who have less than a high school education and 13.8% for those who have one in the data provided by the US Census Bureau&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This correlates with the vaccination rate data as those who have a high school education or less have the two lowest vaccination rates of 74.3% and 70.9%, respectively&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="35-location">3.5 Location&lt;/h3>
&lt;p>The Journal of Community Health is the only data set that assigns specific percentages to the hesitancy rates to regions of the United States with the Northeast having the highest hesitancy rate of 25%, followed very closely by the West and South at 24% and 23%, respectively&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This leaves the Midwest with the lowest hesitancy rate of 18%&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This is partially reflected by the data displayed in the map of the United States that illustrates the rates of hesitancy made by the Census Bureau, which shows that the rates of vaccine hesitancy are slightly higher in the Northeast, West, and Midwest, with the lowest hesitancy rate in the South&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Once again, due to the much larger sample size, it is safer to assume that the Census Bureau is more correct.&lt;/p>
&lt;h3 id="36-political-affiliation">3.6 Political Affiliation&lt;/h3>
&lt;p>This variable is only seen in the Journal of Community Health study. It shows that vaccine hesitancy is highest in Republicans at 29% and lowest in Democrats at 16%.&lt;/p>
&lt;h2 id="4-conclusion">4. Conclusion&lt;/h2>
&lt;p>After analyzing the data, a person&amp;rsquo;s sex appears to have no influence regarding a person&amp;rsquo;s willingness to recieve a COVID vaccine. However, the other variables looked at during this study appear to have an influence. One&amp;rsquo;s level of education appears to have great influence over their willingness to take the vaccine. This makes sense as those who are less educated are less likely to understand the significance of vaccinations and how they protect the U.S. population and may also be more vulnerable to false information about the vaccine as they may not have the skills to deduce whether a source is reliable or not. Additionally, the data shows that younger adults are less likely to recieve the vaccine. This is possibly due to the common misconception that young people are not adversely affected by the virus and as a result the vaccine is not necessary, which could be why , according to the US Census Bureau, 34.9% of those who are hesitant to take the vaccine don&amp;rsquo;t believe that they need it&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. One&amp;rsquo;s political affiliation had a great influence on whether or not someone would be receptive to the vaccine as there is a 13% difference between the hesitancy rates in Republicans and Democrats&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This is very interesting considering that the region with the greatest Republican presence, the South, has the lowest hesitancy rate while the rest of the regions, which have a smaller Republican presence, have the highest hesitancy rates &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Race is also a significant variable, with Black Americans having the lowest vaccination rates and among the highest vaccination hesitancy rates. This could be due to a number of overlapping factors, such as level of education and past historical events. For example, a majority of Black Americans only have a high school diploma, while a relatively small amount of them have a Bachelor&amp;rsquo;s Degree and as we&amp;rsquo;ve seen before, those with a lower level of education are more likely to be hesitant about taking the vaccine&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. Additionally, Black Americans have a fairly long history of being used as medical guinea pigs, with the most infamous example being the Tuskeegee syphillis study, which would understandably result in some aversion to new vaccinations from the community&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>After analyzing this data, it is very apparent that, in order to persuade more Americans to take it, that a significant amount of time and effort be put towards having easily accesible education to the real facts about the COVID vaccine, especially in poorer areas where there people are more likely to have lower levels of education. Additionally, to reduce vaccine hesitancy amongst Republicans, we must stop politicizing the vaccine as well as the severity of COVID-19 and frame the pandemic as a bipartisan matter of public health. Regarding the high vaccination hesitancy amongst Black Americans, there are no short-term fixes as a significant portion of the community has not forgotten about the several transgressions that the United States has committed against them, however, the sooner the United States manages to regain the trust of the Black community, the closer we come to ending this pandemic.&lt;/p>
&lt;h2 id="6-acknowledgments">6. Acknowledgments&lt;/h2>
&lt;p>Special thanks to Yohn J Parra, Carlos Theran, and Gregor Lasweski for supporting this project.&lt;/p>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Khubchandani, J., Sharma, S., Price, J.H. et al.
COVID-19 Vaccination Hesitancy in the United States: A Rapid National Assessment.
J Community Health 46, 270â€“277 (2021).
&lt;a href="https://doi.org/10.1007/s10900-020-00958-x">https://doi.org/10.1007/s10900-020-00958-x&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Estimates of vaccine hesitancy for COVID-19
Center for Disease Control and Prevention
&lt;a href="https://data.cdc.gov/stories/s/Vaccine-Hesitancy-for-COVID-19/cnd2-a6zw">https://data.cdc.gov/stories/s/Vaccine-Hesitancy-for-COVID-19/cnd2-a6zw&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Party Affiliation by Region Pew Research Center
&lt;a href="https://www.pewforum.org/religious-landscape-study/compare/party-affiliation/by/region/-trend">https://www.pewforum.org/religious-landscape-study/compare/party-affiliation/by/region/-trend&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Household Pulse Survey COVID-19 Vaccination Tracker
United States Census Bureau
&lt;a href="https://www.census.gov/library/visualizations/interactive/household-pulse-survey-covid-19-vaccination-tracker.html">https://www.census.gov/library/visualizations/interactive/household-pulse-survey-covid-19-vaccination-tracker.html&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Black High School Attainment Nearly on Par With National Average
United States Census Bureau
&lt;a href="https://www.census.gov/library/stories/2020/06/black-high-school-attainment-nearly-on-par-with-national-average.html">https://www.census.gov/library/stories/2020/06/black-high-school-attainment-nearly-on-par-with-national-average.html&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Washington, Harriet A.
Medical Apartheid: The Dark History of Medical Experimentation on Black Americans from Colonial Times to the Present
&lt;a href="https://www.jstor.org/stable/25610054?seq=1#metadata_info_tab_contents">https://www.jstor.org/stable/25610054?seq=1#metadata_info_tab_contents&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: COVID-19 Analysis</title><link>/report/fa20-523-342/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-342/project/project/</guid><description>
&lt;h1 id="covid-19-analysis">COVID-19 Analysis&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-342/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-342/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final approved, Type: Project&lt;/p>
&lt;p>Hany Boles, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-342/">fa20-523-342&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-342/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>By the end of 2019, healthcare across the world started to see a new type of Flu and they called it Coronavirus or Covid-19. This new type of Flu developed across the world and it appeared there is no one treatment could be used to treat it yet, scientists found different treatments that apply to different age ranges. In this project, We will try to work on comparison analysis between USA and China on number of new cases and new deaths and trying to find factors played big roles in this spread.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data-Sets&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-preparing-covid-19-data-set">2.1 Preparing COVID 19 Data-Set&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-preparing-weather-data-set">2.2 preparing Weather Data-Set&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-methodology">3. Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-processing-the-data">4. Processing the Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion-and-future-work">5. Conclusion and Future Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgements">6. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Corona Virus, Covid 19, Health&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>While the world is ready to start 2020 we heard about a new type of the Flu that it appears to be started in China and from there it went to the entire world. It appeared to affect all ages but its severity did depend on other factors that related to age, health conditions if the patient is a smoker or not?&lt;/p>
&lt;p>This new disease attacked aggressively the respiratory system for the patient and then all the human body causing death. The recovery from this disease appeared to vary from area to area across the globe, Also the death percentage as well was and still vary from area to area. and then we decided to perform the analysis on weather temperature from one side and the covid 19 new cases and new death on the other side to see if the weather temperature plays a role or not with it.&lt;/p>
&lt;h2 id="2-data-sets">2. Data-Sets&lt;/h2>
&lt;p>After observing many datasets &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> to get a better understanding if there are common factors in the areas that have the larger number of new Covid 19 cases, we decided to proceed with the dataset provided by the World Health Organization &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> because this dateset is being updated on a daily basis and has the most accurate data. Currently it appears that we are getting a second wave of coronavirus and so we will try to get the most recent data. We were able to use Webscraping to get the data we need from the World Health Organization website which is updated daily.&lt;/p>
&lt;p>For the weather datasets, we looked at several datasets &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> and we decided to use the data provided by visualcrossing website &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This website helped us in getting the data we need which is daily average temperatures in the United States of America and China. We started to collect the data from 1/3/2020.&lt;/p>
&lt;h3 id="21-preparing-covid-19-data-set">2.1 Preparing COVID 19 Data-Set&lt;/h3>
&lt;p>We started to work on Covid 19 dataset and we found that it is better to use webscraping to gather the dataset so every time we run the python script, we will get the most recent data and then we opened the CSV file and added it to a dataframe.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/webscrap.jpg" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1&lt;/strong>: Downloading the Covid 19 dataset.&lt;/p>
&lt;p>We then filtered only on United States of America so we can get all data belong to United States of America.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/usa.jpg" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2&lt;/strong>: Capturing only USA data&lt;/p>
&lt;p>Then we made 2 seperate graphs to depict the number of new cases and the number of new deaths in the USA throughout 2020 as shown below.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/usa_new_cases_deaths1.jpg" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3&lt;/strong>: USA New Covid 19 cases and new deaths.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/china.jpg" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4&lt;/strong>: Capturing China data.&lt;/p>
&lt;p>We also made 2 seperate graphs to depict the number of new cases and the number of new deaths in China throughout 2020 as shown below.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/china_new_cases_deaths.jpg" alt="Figure 5">&lt;/p>
&lt;p>&lt;strong>Figure 5&lt;/strong>: China New Covid 19 cases and new deaths&lt;/p>
&lt;h3 id="22-preparing-weather-data-set">2.2 preparing Weather Data-Set&lt;/h3>
&lt;p>For the weather temperature dataset, we cleaned all the un-needed data from the csv file we received from Visualcrossing website &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>, then we merged the temperature column with the csv file we prepared for the covid 19, so now we have one file per country that contains Date, Temperature, New cases, new deaths.&lt;/p>
&lt;h2 id="3-methodology">3. Methodology&lt;/h2>
&lt;p>We utilized the Indiana University system to process the collected data as it will need a strong system to process it. Also, we utilized Python, matplotlib for visualization purposes, and Jupyter notebook as programming software and platform.&lt;/p>
&lt;h2 id="4-processing-the-data">4. Processing the Data&lt;/h2>
&lt;p>We started to process the final dataset we prepared and we examined the data for any correlation between the temperature and new cases for the United States of America and we found that there is no correlation there.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/usa_new_dataset.jpg" alt="Figure 6">&lt;/p>
&lt;p>&lt;strong>Figure 6&lt;/strong>: USA Dataset after merging the data.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/usacorr.jpg" alt="Figure 7">&lt;/p>
&lt;p>&lt;strong>Figure 7&lt;/strong>: Correlation between the temperature and the new covid 19 cases for the United States of America.&lt;/p>
&lt;p>Then we also processed the data for China and got the same results as the United States of America. We then looked at the analysis and in the case of the United States of America we found there is a correlation between the new covid cases and the current (cumulative) cases. On the contrary, for China we found no correlation between the new Covid cases and the current (cumulative) cases.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/usacorrheat.jpg" alt="Figure 8">&lt;/p>
&lt;p>&lt;strong>Figure 8&lt;/strong>: Heat map depicting the correlations between new cases, cumulative cases, new deaths, and cumulative deaths, respectively for the United States of America.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/chinacorr.jpg" alt="Figure 9">&lt;/p>
&lt;p>&lt;strong>Figure 9&lt;/strong>: Correlation between the new covid 19 cases and the current cases for China.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/chinacorrheat.jpg" alt="Figure 10">&lt;/p>
&lt;p>&lt;strong>Figure 10&lt;/strong>: Heat map depicting the correlations between new cases, cumulative cases, new deaths, and cumulative deaths, respectively for China.&lt;/p>
&lt;p>We started to observe more data from another country so we chose the United Kingdom, and we found a correlation between the new covid 19 cases and the current (cumulative) cases.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/uk_correlation.jpg" alt="Figure 11">&lt;/p>
&lt;p>&lt;strong>Figure 11&lt;/strong>: Correlation between the new covid 19 cases and the current cases for the United Kingdom.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-342/raw/main/project/images/ukcorrheat.jpg" alt="Figure 12">&lt;/p>
&lt;p>&lt;strong>Figure 12&lt;/strong>: Heat map depicting the correlations between new cases, cumulative cases, new deaths, and cumulative deaths, respectively for the United Kingdom.&lt;/p>
&lt;h2 id="5-conclusion-and-future-work">5. Conclusion and Future Work&lt;/h2>
&lt;p>After processing all the data gathered in search of a correlation between the weather, more specifically temperature, and the number of new cases in both China and the United States of America, the results clearly indicate that the number of new cases and temperature are uncorrelated. Nonetheless, the results suggest that there is a strong positive correlation (correlation coefficient &amp;gt; 0.8) between the number of new cases and the cumulative number of current cases in United states of America and United Kingdom. Hence, it appears that, in the absence of other mitigating factors, the number of the new cases will increase as long as the cumulative number of current cases keeps increasing.&lt;/p>
&lt;p>While the new covid 19 cases and the current cases at China are uncorrelated, this might be due to false reporting or due to different factors are being used at China to reduce the number of the new cases.&lt;/p>
&lt;p>Given the more recent developments pertaining to the discovery and distribution of vaccines it is suggested that the model be modified to include the number of vaccinations administered. The objective in this case will be to discover any correlation between the number of new cases and both the number of the current cases as well as the number of vaccinations being given across at least the United Sates. Depending on the outcome it maybe possible to determine how effective the vaccines are and maybe predict, if possible, if ever the number of cases will diminish to zero.&lt;/p>
&lt;h2 id="6-acknowledgements">6. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Geoffrey Fox, Dr. Gregor von Laszewski, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their assistance, suggestions, and aid provided during working on this project.&lt;/p>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Covid19.who.int. 2020. [online] Available at: &lt;a href="https://covid19.who.int/table">https://covid19.who.int/table&lt;/a> [Accessed 19 December 2020].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Datatopics.worldbank.org. 2020. Understanding The Coronavirus (COVID-19) Pandemic Through Data | Universal Health Coverage Data | World Bank. [online] Available at: &lt;a href="http://datatopics.worldbank.org/universal-health-coverage/coronavirus/">http://datatopics.worldbank.org/universal-health-coverage/coronavirus/&lt;/a> [Accessed 19 December 2020].&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Kaggle.com. 2020. COVID-19 Open Research Dataset Challenge (CORD-19). [online] Available at: &lt;a href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge">https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge&lt;/a> [Accessed 19 December 2020].&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Visualcrossing.com. 2020. Weather Data Services | Visual Crossing. [online] Available at: &lt;a href="https://www.visualcrossing.com/weather/weather-data-services#/editDataDefinition">https://www.visualcrossing.com/weather/weather-data-services#/editDataDefinition&lt;/a> [Accessed 19 December 2020].&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>The Weather Channel. 2020. National And Local Weather Radar, Daily Forecast, Hurricane And Information From The Weather Channel And Weather.Com. [online] Available at: &lt;a href="https://weather.com/">https://weather.com/&lt;/a> [Accessed 22 December 2020].&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Climate.gov. 2020. Dataset Gallery | NOAA Climate.Gov. [online] Available at: &lt;a href="https://www.climate.gov/maps-data/datasets/formats/csv/variables/precipitation">https://www.climate.gov/maps-data/datasets/formats/csv/variables/precipitation&lt;/a> [Accessed 22 December 2020].&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Deep Learning in Drug Discovery</title><link>/report/sp21-599-359/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/sp21-599-359/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Anesu Chaora, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/">sp21-599-359&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Code: &lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/blob/main/project/code/predicting_molecular_activity.ipynb">predicting_molecular_activity.ipynb&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Machine learning has been a mainstay in drug discovery for decades. Artificial neural networks have been used in computational approaches to drug discovery since the 1990s [^1]. Under traditional approaches, emphasis in drug discovery was placed on understanding chemical molecular fingerprints, in order to predict biological activity. More recently however, deep learning approaches have been adopted instead of computational methods. This paper outlines work conducted in predicting drug molecular activity, using deep learning approaches.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#11-de-novo-molecular-design">1.1. De novo molecular design&lt;/a>&lt;/li>
&lt;li>&lt;a href="#12-bioactivity-prediction">1.2. Bioactivity prediction&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#2-related-work">2. Related Work&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-merck-molecular-activity-challenge-on-kaggle">2.1. Merck Molecular Activity Challenge on Kaggle&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-the-dataset">2.2. The Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-a-deep-learning-algorithm">2.3. A Deep Learning Algorithm&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-project-implementation">3. Project Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-tools-and-environment">3.1. Tools and Environment&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-implementation-overview">3.2. Implementation Overview&lt;/a>&lt;/li>
&lt;li>&lt;a href="#33-benchmarks">3.3. Benchmarks&lt;/a>&lt;/li>
&lt;li>&lt;a href="#34-findings">3.4. Findings&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-discussion">4. Discussion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgments">6. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-appendix">7. Appendix&lt;/a>&lt;/li>
&lt;li>&lt;a href="#references">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Deep Learning, drug discovery.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;h3 id="11-de-novo-molecular-design">1.1. De novo molecular design&lt;/h3>
&lt;p>Deep learning (DL) is finding uses in developing novel chemical structures. Methods that employ variational autoencoders (VAE) have been used to generate new chemical structures. Approaches have involved encoding input string molecule structures, then reparametrizing the underlying latent variables, before searching for viable solutions in the latent space by using methods such as Bayesian optimizations. The results are then decoded back into simplified molecular-input line-entry system (SMILES) notation, for recovery of molecular descriptors. Variations to this method involve using generative adversarial networks (GAN)s, as subnetworks in the architecture, to generate the new chemical structures &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Other approaches for developing new chemical structures involve recurrent neural networks (RNN), to generate new valid SMILES strings, after training the RNNs on copious quantities of known SMILES datasets. The RNNs use probability distributions learned from training sets, to generate new strings that correspond to molecular structures &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Variations to this approach incorporate reinforcement learning to reward models for new chemical structures, while punishing them for undesirable results &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="12-bioactivity-prediction">1.2. Bioactivity prediction&lt;/h3>
&lt;p>Computational methods have been used in drug development for decades &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The emergence of high-throughput screening (HTS), in which automated equipment is used to conduct large assays of scientific experiments on molecular compounds in parallel, has resulted in generation of enormous amounts of data that require processing. Quantitative structure activity relationship (QSAR) models for predicting the biological activity responses to physiochemical properties of predictor chemicals, extensively use machine learning models like support vector machines (SVM) and random decision forests (RF) for processing &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>While deep learning (DL) approaches have an advantage over single-layer machine learning methods, when predicting biological activity responses to properties of predictor chemicals, they have only recently been used for this &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The need to interpret how predictions are made through computationally oriented drug discovery, is seen - in part - as a factor to why DL approaches have not been adopted as quickly in this area &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. However, because DL models can learn complex non-linear data patterns, using their multiple hidden layers to capture patterns in data, they are better suited for processing complex life sciences data, than other machine learning approaches &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Their applications have included profiling tumors at molecular level and predicting drug responses, based on pharmacological and biological molecular structures, functions, and dynamics. This is attributed to their ability to handle high dimensionality in data features, making them appealing for use in predicting drug response &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>For example, deep neural networks were used in models that won NIHâ€™s Toxi21 Challenge &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup> on using chemical structure data only to predict compounds of concern to human health &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. DL models were also found to perform better than standard RF models &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> in predicting the biological activities of molecular compounds in the Merck Molecular Activity Challenge on Kaggle &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. Details of the challenge follow.&lt;/p>
&lt;h2 id="2-related-work">2. Related Work&lt;/h2>
&lt;h3 id="21-merck-molecular-activity-challenge-on-kaggle">2.1. Merck Molecular Activity Challenge on Kaggle&lt;/h3>
&lt;p>A challenge to identify the best statistical techniques for predicting molecular activity was issued by Merck &amp;amp; Co Pharmaceutical, through Kaggle in October of 2012. The stated goal of the challenge was to â€˜help develop safe and effective medicines by predicting molecular activityâ€™ for effects that were both on and off target &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="22-the-dataset">2.2. The Dataset&lt;/h3>
&lt;p>A &lt;a href="https://www.kaggle.com/c/MerckActivity/data">dataset&lt;/a> was provided for the challenge &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. It consisted of 15 molecular activity datasets. Each dataset contained rows corresponding to assays of biological activity for chemical compounds. The datasets were subdivided into training and test set files. The training and test dataset split was done by dates of testing &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>, with test set dates consisting of assays conducted after the training set assays.&lt;/p>
&lt;p>The training set files each had a column with molecular descriptors that were formulated from chemical molecular structures. A second column in the files contained numeric values, corresponding to raw activity measures. These were not normalized, and indicated measures in different units.&lt;/p>
&lt;p>The remainder of the columns in each training dataset file indicated disguised substructures of molecules. Values in each row, under the substructure (atom pair and donor-acceptor pair) codes, corresponded to the frequencies at which each of the substructures appeared in each compound. Figure 1 shows part of the head row for one of the training dataset files, and the first records in the file.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/raw/develop/project/images/training_set.jpg" alt="Figure 1">
&lt;strong>Figure 1&lt;/strong>: Head Row of 1 of 15 Training Dataset files&lt;/p>
&lt;p>The test dataset files were similar (Figure 2) to the training files, except they did not include the column for activity measures. The challenge presented was to predict the activity measures for the test dataset.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/raw/develop/project/images/test_set.jpg" alt="Figure 2">
&lt;strong>Figure 2&lt;/strong>: Head Row of 1 of 15 Test Dataset files&lt;/p>
&lt;h3 id="23-a-deep-learning-algorithm">2.3. A Deep Learning Algorithm&lt;/h3>
&lt;p>The entry that won the Merck Molecular Activity Challenge on Kaggle used an ensemble of methods that included a fully connected neural network as the main contributor to the high accuracy in predicting molecular activity &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Evaluations of predictions for molecular activity for the test set assays were then determined using the mean of the correlation coefficient (R2) of the 15 data sets. Sample code in R was provided for evaluating the correlation coefficient. The code, and formula for R2 are appended in Appendix 1.&lt;/p>
&lt;p>An approach of employing convolutional networks on substructures of molecules, to concentrate learning on localized features, while reducing the number of parameters in the overall network, was also proposed in literature on improving molecular activity predictions. This methodology of identifying molecular substructures as graph convolutions, prior to further processing, was discussed by authors &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>, &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In line with the above research, an ensemble of networks for predicting molecular activity was planned for this project, using the Merck dataset, and hyperparameter configurations found optimal by the cited authors. Recognized optimal activation functions, for different neural network types and prediction types &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>, were also earmarked for use on the project.&lt;/p>
&lt;h2 id="3-project-implementation">3. Project Implementation&lt;/h2>
&lt;p>Implementation details for the project were as follows:&lt;/p>
&lt;h3 id="31-tools-and-environment">3.1. Tools and Environment&lt;/h3>
&lt;p>The Python programming language (version 3.7.10) was used on Google Colab (&lt;a href="https://colab.research.google.com">https://colab.research.google.com&lt;/a>).&lt;/p>
&lt;p>A subscription account to the service was employed, for access to more RAM (High-RAM runtime shape) during development, although the free standard subscription will suffice for the version of code included in this repository.&lt;/p>
&lt;p>Google Colab GPU hardware accelerators were used in the runtime configuration.&lt;/p>
&lt;p>Prerequisites for the code included packages from &lt;a href="http://cloudmesh.github.io/">Cloudmesh&lt;/a>, for benchmarking performance, and from &lt;a href="https://www.kaggle.com/">Kaggle&lt;/a>, for API access to related data.&lt;/p>
&lt;p>Keras libraries were used for implementing the molecular activity prediction model.&lt;/p>
&lt;h3 id="32-implementation-overview">3.2. Implementation Overview&lt;/h3>
&lt;p>This project&amp;rsquo;s implementation of a molecular activity prediction model consisted of a fully connected neural network. The network used the Adam &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup> optimization algorithm, at a learning rate of 0.001 and beta_1 calibration of 0.5. Mean Squared Error (MSE) was used for the loss function, and R-Squared &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup> for the metric. Batch sizes were set at 128. These parameter choices were selected by referencing the choices of other prior investigators &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The network was trained on the 15 datasets separately, by iterating through the storage location containing preprocessed data, and sampling the data into training, evaluation and prediction datasets - before running the training. The evaluation and prediction steps, for each dataset, where also executed during the iteration of each molecular activity dataset. Running the processing in this way was necessitated by the fact that the 15 datasets each had different feature set columns, corresponding to different molecular substructures. As such, they could not be readily processed through a single dataframe.&lt;/p>
&lt;p>An additional compounding factor was that the data was missing the molecular activity results (actual readings) associated with the dataset provided for testing. These were not available through Kaggle as the original competition withheld these from contestants, reserving them as a means for evaluating the accuracy of the models submitted. In the absence of this data, for validating the results of this project, the available training data was split into samples that were then used for the exercise. The training of the fully connected network was allocated 80% of the data, while the testing/evaluation of the model was allocated 10% of the data. The remaining data (10%) was used for evaluating predictions.&lt;/p>
&lt;h3 id="33-benchmarks">3.3. Benchmarks&lt;/h3>
&lt;p>Benchmarks captured during code execution, using cloudmesh-common &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>, were as follows:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The data download process from Kaggle, through the Kaggle data API, took 29 seconds.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Data preprocessing scripts took 8 minutes and 56 seconds to render the data ready for training and evaluation. Preprocessing of data included iterating through the issued datasets separately, since each file contained different combinations of feature columns (molecular substructures).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The model training, evaluation and prediction step took 7 minutes and 45 seconds.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="34-findings">3.4. Findings&lt;/h3>
&lt;p>The square of the correlation coefficient (R^2) values obtained (coefficient of determination) &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup> during training and evaluation were considerably low (&amp;lt; 0.1). A value of one (1) would indicate a goodness of fit for the model that implies that the model is completely on target with predicting accurate outcomes (molecular activity) from the independent variables (substructures/feature sets). Such a model would thus fully account for the predictions, given a set of substructures as inputs. A value of zero (0) would indicate a total lack of correlation between the input feature values and the predicted outputs. As such, it would imply that there is a lot of unexplained variance in the outputs of the model. The square of the correlation coefficient values obtained for this model (&amp;lt;0.1) therefore imply that it either did not learn enough, or other unexplained (by the model) variance caused unreliable predictions.&lt;/p>
&lt;h2 id="4-discussion">4. Discussion&lt;/h2>
&lt;p>An overwhelming proportion of the data elements provided through the datasets were zeros (0)s, indicating that no frequencies of the molecular substructures/features were present in the molecules represented by particular rows of data elements. This disproportionate representation of absent molecular substructure frequencies, versus the significantly lower instances where there were frequencies appears to have had an effect of dampening the learning of the fully connected neural network.&lt;/p>
&lt;p>This supports approaches that advocated for the use of convolutional neural networks &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>, &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup> as auxiliary components to help focus learning on pertinent substructures. While the planning phase of this project had incorporated inclusion of such, the investigator ran out of time to implement an ensemble network that would include the suggestions.&lt;/p>
&lt;p>Apart from employing convolutions, other preprocessing approaches for rescaling, and normalizing, the data features and activations &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup> could have helped the learning, and subsequently the predictions made. This reinforces the fact that deep learning models, as is true of other machine learning approaches, rely deeply on the quality and preparation of data fed into them.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>Deep learning is a very powerful new approach to solving many machine learning problems, including some that have eluded solutions till now. While deep learning models offer robust and sophisticated ways of learning patterns in data, they are still only half the story. The quality and appropriate preparation of the data fed into models is equally important when seeking to have meaningful results.&lt;/p>
&lt;h2 id="6-acknowledgments">6. Acknowledgments&lt;/h2>
&lt;p>Acknowledgements go to Dr. Geoffrey Fox for his excellent guidance on ways to think about deep learning approaches, and for his instructorship of the course &amp;lsquo;ENG-E599: AI-First Engineering&amp;rsquo;, for which this project is a deliverable. Acknowledgements also go to Dr. Gregor von Laszewski for his astute tips and recommendations on technical matters, and on coding and documention etiquette.&lt;/p>
&lt;h2 id="7-appendix">7. Appendix&lt;/h2>
&lt;p>Square of the Correlation Coefficient (R2) Formula:&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/raw/develop/project/images/correlation_coefficient.jpg" alt="Figure 3">
&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>&lt;/p>
&lt;p>Sample R2 Code in the R Programming Language:&lt;/p>
&lt;pre>&lt;code>Rsquared &amp;lt;- function(x,y) {
# Returns R-squared.
# R2 = \frac{[\sum_i(x_i-\bar x)(y_i-\bar y)]^2}{\sum_i(x_i-\bar x)^2 \sum_j(y_j-\bar y)^2}
# Arugments: x = solution activities
# y = predicted activities
if ( length(x) != length(y) ) {
warning(&amp;quot;Input vectors must be same length!&amp;quot;)
}
else {
avx &amp;lt;- mean(x)
avy &amp;lt;- mean(y)
num &amp;lt;- sum( (x-avx)*(y-avy) )
num &amp;lt;- num*num
denom &amp;lt;- sum( (x-avx)*(x-avx) ) * sum( (y-avy)*(y-avy) )
return(num/denom)
}
}
&lt;/code>&lt;/pre>&lt;p>&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Hongming Chen, O. E. (2018). The rise of deep learning in drug discovery. Elsevier.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Marwin H. S. Segler, T. K. (2018). Generating Focused Molecule Libraries for Drug Discovery with Recurrent Neural Networks. America Chemical Society.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>N Jaques, S. G. (2017). Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control. Proceedings of the 34th International Conference on Machine Learning, PMLR (pp. 1645-1654). MLResearchPress.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Gregory Sliwoski, S. K. (2014). Computational Methods in Drug Discovery. Pharmacol Rev, 334 - 395.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Delora Baptista, P. G. (2020). Deep learning for drug response prediction in cancer. Briefings in Bioinformatics, 22, 2021, 360â€“379.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Erik Gawehn, J. A. (2016). Deep Learning in Drug Discovery. Molecular Informatics, 3 - 14.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>National Institute of Health. (2014, November 14). Tox21 Data Challenge 2014. Retrieved from [tripod.nih.gov:] &lt;a href="https://tripod.nih.gov/tox21/challenge/">https://tripod.nih.gov/tox21/challenge/&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Andreas Mayr, G. K. (2016). Deeptox: Toxicity Prediction using Deep Learning. Frontiers in Environmental Science.&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Junshui Ma, R. P. (2015). Deep Neural Nets as a Method for Quantitative Structure-Activity Relationships. Journal of Chemical Information and Modeling, 263-274.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Kaggle. (n.d.). Merck Molecular Activity Challenge. Retrieved from [Kaggle.com:] &lt;a href="https://www.kaggle.com/c/MerckActivity">https://www.kaggle.com/c/MerckActivity&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Kearnes, S., McCloskey, K., Berndl, M., Pande, V., &amp;amp; Riley, P. (2016). Molecular graph convolutions: moving beyond fingerprints. Switzerland: Springer International Publishing .&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Mikael Henaff, J. B. (2015). Deep Convolutional Networks on Graph-Structured Data.&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Bronlee, J. (2021, January 22). How to Choose an Activation Function for Deep Learning. Retrieved from [https://machinelearningmastery.com:] &lt;a href="https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/">https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Keras. (2021). Adam. Retrieved from [https://keras.io:] &lt;a href="https://keras.io/api/optimizers/adam/">https://keras.io/api/optimizers/adam/&lt;/a>&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>Keras. (2021). Regression Metrics. Retrieved from [https://keras.io:] &lt;a href="https://keras.io/api/metrics/regression_metrics/">https://keras.io/api/metrics/regression_metrics/&lt;/a>&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16" role="doc-endnote">
&lt;p>RuwanT (2017, May 16). Merk. Retrieved from [https://github.com:] &lt;a href="https://github.com/RuwanT/merck/blob/master/README.md">https://github.com/RuwanT/merck/blob/master/README.md&lt;/a>&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17" role="doc-endnote">
&lt;p>Wikipedia (2021). Coefficient of Determination. Retrieved from [https://wikipedia.org:] &lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">https://en.wikipedia.org/wiki/Coefficient_of_determination&lt;/a>&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data on Gesture Recognition and Machine Learning</title><link>/report/fa20-523-315/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-315/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-315/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-315/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-315/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-315/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Sunny Xu, Peiran Zhao, Kris Zhang, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-315/">fa20-523-315&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-315/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Since our technology is more and more advanced as time goes by, traditional human-computer interaction has become increasingly difficult to meet people&amp;rsquo;s demands. In this digital era, people need faster and more efficient methods to obtain information and data. Traditional and single input and output devices are not fast and convenient enough, it also requires users to learn their own methods of use, which is extremely inefficient and completely a waste of time. Therefore, artificial intelligence comes out, and its rise has followed the changeover times, and it satisfied people&amp;rsquo;s needs. At the same time, gesture is one of the most important way for human to deliver information. It is simple, efficient, convenient, and universally acceptable. Therefore, gesture recognition has become an emerging field in intelligent human-computer interaction field, with great potential and future.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background">2. Background&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-gesture-recognition">3. Gesture Recognition&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-hand-gesture">3.1 Hand Gesture&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#311-hand-gesture-recognition-and-big-data">3.1.1 Hand Gesture Recognition and Big Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#312-principles-of-hand-gesture-recognition">3.1.2 Principles of Hand Gesture Recognition&lt;/a>&lt;/li>
&lt;li>&lt;a href="#313-gesture-segmentation-and-algorithm-the-biggest-difficulty-of-gesture-recognition">3.1.3 Gesture Segmentation and Algorithm, The Biggest Difficulty of Gesture Recognition.&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#32-body-gesture">3.2 Body Gesture&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#321-introduction-to-body-gesture">3.2.1 Introduction to Body Gesture&lt;/a>&lt;/li>
&lt;li>&lt;a href="#322-body-gesture-and-big-data">3.2.2 Body Gesture and Big Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#323-random-forest-algorithm-in-body-gesture-recognition">3.2.3 Random Forest Algorithm in Body Gesture Recognition&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#33-face-gesture">3.3 Face Gesture&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#331-introduction-to-face-gesture-facial-expression">3.3.1 Introduction to Face Gesture (Facial Expression)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#332-sense-organs-on-the-face">3.3.2 Sense Organs on The Face&lt;/a>&lt;/li>
&lt;li>&lt;a href="#333-facial-expression-and-big-data">3.3.3 Facial Expression and Big Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#334-the-problem-with-detecting-emotion-for-technology-nowadays">3.3.4 The Problem with Detecting Emotion for Technology Nowadays&lt;/a>&lt;/li>
&lt;li>&lt;a href="#335-classification-algorithms">3.3.5 Classification Algorithms&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-conclusion">4. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-references">5. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> gesture recognition, human, technology, big data, artificial intelligence, body language, facial expression&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Technology is probably one of the most attracting things for people nowadays. Whether it is the new iPhone coming out or some random new technology that is bring into our life. It is a matter of fact that technology has become one of the essential parts of our life and our society. Simply, our life will change a lot without technology. As of today, since technology is improving so fast, there are many things that can be related to AI and machine learning. A lot of the ordinary things around our life becomes data. And the reason why they become data is because there is a need for them in having better technology to improve our life. For example, language was stored into data to produce technology like translator to provide convenience for people that does not speak the language. Another example is that roads were stored into data to produce GPS to guide direction for people. Nowadays, people values communication and interaction between others. Since gesture recognition is one of the most important ways to understand people and know their emotion, it becomes a popular field of study for many scientists. There are multiply field of study in gesture recognition and each require a lot of amount of time to know them well. For the report, we do research about hand gesture, body gesture and facial expression. Of course, there will be a lot of other fields related to gesture recognition, for example, like animal gestures. They all can be stored into data and get study in the research by scientists. Many people might have question about how gesture recognition are has anything to do with technology. They simply do not think that they can be related, but in fact, they are related. Companies like Intel and Microsoft have already created so many studies for new technology in that field. For example, Intel proposed combining facial recognition with device recognition to authenticate users. Studying gestures recognition will often reveal what the think. For example, when someone is lying, their eye will tend to look around and they tend to touch their nose with their hand, etc. So, studying gesture recognition will not only help people understand much more about human beings and it can also help our technology grow. For example, in AI and machine learning, studying gestures recognition will make or improve AI and machine learning to better understand humans and be more human-like.&lt;/p>
&lt;h2 id="2-background">2. Background&lt;/h2>
&lt;p>Nowadays, people are doing more and more high-tech research, which also makes various high-tech products appear in society. For people, electricity is as important as water and air. Can&amp;rsquo;t imagine life without electricity. We can realize that technology is changing everything about people from all aspects. People living in the high-tech era are also forced to learn and understand the usage of various high-tech products. As a representative of high technology, artificial intelligence has also attracted widespread attention from society. Due to the emergence of artificial intelligence, people have also begun to realize that maintaining human characteristics is also an important aspect of high technology.&lt;/p>
&lt;p>People&amp;rsquo;s living environment is inseparable from high technology. As for the use of human body information, almost every high-tech has different usage &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. For example, face recognition is used in many places to check-in. This kind of technology enables the machine to store the information of the human face and determine whether it is indeed the right person by judging the five senses. We are most familiar with using this technology in airports, customs, and companies to check in at work. Not only that, but the smartphones we use every day are also unlocked through this face recognition technology. Another example is the game console that we are very familiar with. Game consoles such as Xbox and PS already have methods for identifying people&amp;rsquo;s bodies. They can identify several key points of people through the images received by their own cameras, thus inputting this line of action into the world of the game.&lt;/p>
&lt;p>Many researchers are now studying other applications of human movements, gestures, and facial expressions. One of the most influential ones is that Googleâ€™s scientists have developed a new computer vision method for hand perception. Google researchers identified the movement of a hand through twenty-one 3D points on the hand. Research Engineers Valentin Bazarevsky and Fan Zhang stated that &amp;ldquo;The ability to perceive the shape and motion of hands can be a vital component in improving the user experience across a variety of technological domains and platforms &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&amp;rdquo; This model can currently identify many common cultural features. gesture. They have done experiments. When people play a game of &amp;ldquo;rock, paper, scissors&amp;rdquo; in front of the camera, this model can also judge everyone&amp;rsquo;s win or loss by recognizing gestures.&lt;/p>
&lt;p>More than that, many artificial intelligences can now understand people&amp;rsquo;s feelings and intentions by identifying people&amp;rsquo;s facial expressions. This also allows us to know how big a database is behind this to support the operation of these studies. But collecting these data about gesture recognition is not easy. Many times we need to worry about not only whether the data we input is correct, but also whether the target identified by artificial intelligence is clear and the output information is accurate.&lt;/p>
&lt;h2 id="3-gesture-recognition">3. Gesture Recognition&lt;/h2>
&lt;p>Gesture recognition is mainly divided into two categories, one is based on external device recognition, the specific application is data gloves, wearing it on user&amp;rsquo;s hand, to obtain and analysis information through sensors. This method has obvious shortcomings, though it is accurate and has excellent response speed, but it is costly and is not good for large-scale promotion. The other one is the use of computer vision. People do not need to wear gloves. As its name implies, this method collects and analyzes information through a computer. It is convenient, comfortable, and not so limited based on external device identification. In contrast, it has greater potential and is more in line with the trend of the times. Of course, this method needs more effective and accurate algorithms to support, because the gestures made by different people at different times, in different environments and at different angles also represent different meanings. So, if we want more accurate information feedback. Then the advancement of algorithms and technology is inevitable. The development of gesture recognition is also the development of artificial intelligence, a process of the development of various algorithms from data gloves to the development of computer vision-based optical technology plays a role in promoting it.&lt;/p>
&lt;h3 id="31-hand-gesture">3.1 Hand Gesture&lt;/h3>
&lt;h4 id="311-hand-gesture-recognition-and-big-data">3.1.1 Hand Gesture Recognition and Big Data&lt;/h4>
&lt;p>Hand gesture recognition is commonly used in gesture recognition because fingers are the most flexible and it is able to create different angles that will represent different meanings. The hand gesture itself is also an easy but efficient way for us human beings to communicate and send messages to each other. The existence of hand gestures can be considered easy but powerful. However, if we are using the application of hand gesture recognition, it is a much more complicated process. In real life, we can just ben our finger or simply make a fist so that other people will understand our message. But when using hand gesture recognition there are many processes that are being involved. Hand gesture is commonly used in geesture recoginitaion As we did our research and based on our life experiences, hand gesture recognition is a very hot topic and has all the potential to be the next wave. Hand gesture recognition has recently achieved big success in many fields. The advancement and development of hand gesture recognition is also the development of other technology such as the advancement of computer chips, the advancement of algorithms, the advancement of machine learning even advancement of deep learning, and the advancement of cameras from 2D to 3D. The most important part of hand gesture recognition is big data and machine learning. Because of the development of big data and machine learning, data scientists are able to have better datasets, build a more accurate and successful model and be able to process the information and predict the most accurate results. Hand gesture recognition is a significant link in Gesture recognition.However gesture recognition is also not only about hand gesture recognition, it also includes other body parts such as facial expression recognition and body gesture recognition. With the help of the whole system of different gesture recognitions, the data can be recorded and processed by AIs. The results or predictions can be used currently or later on for different purposes in different areas &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="312-principles-of-hand-gesture-recognition">3.1.2 Principles of Hand Gesture Recognition&lt;/h4>
&lt;p>Hand gesture recognition is a complicated process involving many steps. And in order to get the most accurate result, it will need a large amount of quality data and a scientific model with high performance. Hand gesture recognition is also at a developing stage simply because there are so many possible factors that can influence the result. Possible factors include skin color, background color, hand gesture angle, and Bending angle, etc. To simplify the process of gesture recognition, AIs will use 3-D cameras to capture images. After that, the data of the image will be collected and processed by programs and built models. And lastly, AIs will be able to use that model to get an accurate result in order to have a corresponding response or predict future actions. To explain all processes of hand gesture recognition in detail, it includes graphic gathering, retreatment, skin color segmentation, hand gesture segmentation, and finally hand gesture recognition. Hand Gesture Recognition can not achieve the best accuracy without all any of these steps. Within all these steps, skin color segmentation is the most crucial step in order to increase accuracy and this process will be explained in the next session &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="313-gesture-segmentation-and-algorithm-the-biggest-difficulty-of-gesture-recognition">3.1.3 Gesture Segmentation and Algorithm, The Biggest Difficulty of Gesture Recognition.&lt;/h4>
&lt;p>If someone actually asks us a question which is what kind of recognition is going to have the maximum potential in the future? We will have Hand Gesture recognition as my answer without a doubt. Because in my opinion, Hand Gesture Recognition is really the next wave, as our technology is getting better and better, it will be a much easier and more efficient type of recognition that could possibly change our lives. If you compare Hand Gesture Recognition with Voice Recognition, you will see the biggest difference because everyone is using Hand Gesture all over the world in different ways while Voice is limited to people that are unable to make a sound and sound is a much more complicated type of data that in my opinion is not efficient enough to deliver a message, at least with lots of evidence indicating it is not easier than Hand Gesture Recognition. However, it doesnâ€™t mean hand gesture doesnâ€™t have any limit. Instead, Hand Gesture Recognition is influenced by the color in many ways including skin colors and the colors of the background. But skin color is also a great characteristic of recognition at the same time. So if we could overcome this shortcoming or obstacle, the biggest disadvantage of Hand Gesture Recognition could also become its biggest advantage since skin color has so many amazing characteristics that could be used as a huge benefit for Hand Gesture Recognition. Firstly, skin color is a unique attribute which means it has a similar meaning all over the world. For example, Asian people mostly have yellow skins, Western people mostly have white skins while African American people mostly have black skins. People might form different regions from all over the world but since their skins are similar in many ways, they are most likely to have at least similar hand gesture meanings according to different scientific studies. However, you might ask another realistic question which is what about many people who have similar skin colors but are coming from different groups of people who have a completely different cultural background which results in different Hand Gestures and people who have similar Hands Gesture but have much different skin colors. These are all realistic Hand Gesture Recognition problems and these are the problems that Hand Gesture Recognition already solved or is going to solve in the future. Firstly, for people who have similar skin colors but are coming from different groups of people who have a completely different cultural background, this is when skin color comes to play its role. Even though those people have similar skin color, their skin color canâ€™t be exactly the same. Most of the time, it will be either darker or lighter and we might say itâ€™s all yellow or white, but the machine will see it as its data format so even if it is all white, the type of white is still completely different. And this is when gesture segmentation or more accurately skin color segmentation makes a difference. Overall, us human read skin colors as the simple color we have learned from different textbooks but the computer or machine see the different color in the different color spaces and the data they receive and going to process will be much more accurate. In addition to that, scientists will need to do more in-depth research and studies in order to get the most accurate result. And for people who have similar Hands Gesture but have many different skin colors, scientists will need to collect more accurate data not only about the color and about the size, angles, etc. This more detailed information will help the machine read Hand Gesture more accurately in order to get the most beneficial feedback. The background color will undoubtedly provide lots of useless information and potentially negatively influence the result. In this way, Hand Gesture Recognition has developed its own color and gesture recognition algorithm and method to remove the most useless data or color and leave the valid ones. Lighting in different background settings will have a huge influence to and in most ways, it will negatively influence the result too. There are five most important steps in Hand Gesture Recognition which are Graphic Gathering, Pretreatment, Skin Color Segmentation, Gesture Segmentation, and lastly Gesture Recognition. All these different steps are all very crucial in order to get the most accurate feedback or result. It is pretty similar to the most data treatment process especially the first two steps where you first build a model, gather different types of data, clean the data after that, and use skin color segmentation and gesture segmentation before the last Gesture Recognition process &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="32-body-gesture">3.2 Body Gesture&lt;/h3>
&lt;h4 id="321-introduction-to-body-gesture">3.2.1 Introduction to Body Gesture&lt;/h4>
&lt;p>Body gestures, which can also be called body language, refer to humans expressing their ideas through the coordinated activities of various body parts. In our lives, body language is ubiquitous. It is like a bridge for our human communication. Through body language expression, it is often easier for us to understand what the other person wants to express. At the same time, we can express ourselves better. The profession of an actor is a good example of body language. This is a compulsory course for every actor because actors can only use their performances to let us know what they are expressing &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. At this time, body language becomes extremely important. Different characters have different body movements in different situations, and actors need to make the right body language at a specific time to let the audience know their inner feelings. Yes, the most important point of body language is to convey mood through movement.&lt;/p>
&lt;p>In many cases, certain actions will make people feel emotions. For us who communicate with all kinds of people every day, there are also many body languages that we are more familiar with. For example, when a person hangs his head, it means that he is unhappy, walking back and forth is a sign of a person&amp;rsquo;s anxiety, and body shaking is caused by nervousness, etc.&lt;/p>
&lt;h4 id="322-body-gesture-and-big-data">3.2.2 Body Gesture and Big Data&lt;/h4>
&lt;p>As a piece of big data, body language requires data collected by studying human movements. Scientists found that when a person wants to convey a complete message, body language accounts for half. And because body language belongs to a person&amp;rsquo;s actions subconsciously, it is rarely deceptive. All of your nonverbal behaviorsâ€”the gestures you make, your posture, your tone of voice, how many eyes contact you makeâ€”send strong messages &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. In many cases, these unconscious messages from our bodies allow the people who communicate with us to feel our intentions. Even when we stop talking, these messages will not stop. This also explains why scientists want to collect data to let artificial intelligence understand human behavior. In order for artificial intelligence to understand human mood or intention from people&amp;rsquo;s body postures and actions, scientists have collected a lot of human body actions that show intentions in different situations through research. The music gesture artificial intelligence developed by MIT-IBM Watson AI Lab is a good example &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. The music gesture artificial intelligence developed by MIT-IBM Watson AI Lab can enable artificial intelligence to judge and isolate the sounds of individual instruments through body and gesture movements. This success is undoubtedly created by the big data of the entire body and gestures. The research room collects a large number of human structure actions to provide artificial intelligence with a large amount of information so that the artificial intelligence can judge what melody the musician is playing through body gestures and key points of the face. This can improve its ability to distinguish and separate sounds when artificial intelligence listens to the entire piece of music.&lt;/p>
&lt;p>Most of the artificial intelligence&amp;rsquo;s analysis of the human body requires facial expressions and body movements. This recognition cannot be achieved only by calculation. What is needed is the collection of the meaning of different body movements of the human body by a large database. The more situations are collected, the more accurate the analysis of human emotions and intentions by artificial intelligence will be. The easiest way is to include more. Just like humans, broadening your horizons is a way to better understand the world. The way of recording actions is not complicated. Just set several key movable joints of the human body to several points, and then connect the red dots with lines to get the approximate shape of the human body. At this time, the actions made by the human body will be included in the artificial intelligence. In the recording process, the upper body and lower body can be recorded separately. In order to avoid in some cases, the existence of obstructions will cause artificial intelligence to fail to recognize correctly.&lt;/p>
&lt;h4 id="323-random-forest-algorithm-in-body-gesture-recognition">3.2.3 Random Forest Algorithm in Body Gesture Recognition&lt;/h4>
&lt;p>Body gesture recognition is pretty useful but pretty hard to achieve because of its limitations and harsh requirements. Without the development of all kinds of 3D cameras, body gesture recognition is just an unrealistic dream. In order to get important and precise data for the body gesture recognition to process, different angles, light, background all needs to be captured &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. For body gestures, the biggest difficulty is that if you only capture data in the front, it will not give you the correct information and result in most of the time. In this way, you will need precise data from different angles. A Korean team has done an experiment using three 3D cameras and three stereo cameras to capture images and record data from different angles. The data were recorded in a large database that includes captured data both from outside and inside. One of the most popular algorithms used in body gesture recognition is the random forest algorithm. It is very famous and useful in all types of machine learning projects. It is a type of supervised learning algorithm. Because there are all types of data are needed to be a record and process. The random forest algorithm is perfect for that, the biggest advantage of this algorithm is that it can let each individual tree mainly focus on one part or one characteristic of body gesture data because of this algorithmâ€™s ability to combine all weak classifiers into a strong one &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. It is simple but so powerful and efficient. In addition to that, it works really well with body gesture recognition. With the algorithm and advanced cameras, precise data could be collected and AIs will be able to get useful information at different levels.&lt;/p>
&lt;h3 id="33-face-gesture">3.3 Face Gesture&lt;/h3>
&lt;h4 id="331-introduction-to-face-gesture-facial-expression">3.3.1 Introduction to Face Gesture (Facial Expression)&lt;/h4>
&lt;p>Body language is one of the ways that we can express ourselves without saying any words. It has been suggested that body language may account for between 60 to 65% of all communication &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. According to expert, body language is used every day for us to communicate with each other. During our communication, we not only use words but also use body gestures, hand gestures and most importantly, we use facial expression most. During communication with different people, our face communicate different thoughts, idea, and emotion and the reason why we use facial expression more than any other body gestures is that when we have certain emotion, it is express in our face automatically. Facial expression is often not under our control. That is why people often say that the word that come out of mouth cannot always be true, but their facial expression will reveal what those people are thinking about. So, what is facial expression exactly? According to Jason Matthew Harley, Facial expressions are configurations of different micromotor movements in the face that are used to infer a personâ€™s discrete emotional state &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Some example of common facial expression will be: Happiness, Sadness, Anger, Surprise, Disgust, Fear, etc &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Each facial expression will have some deep meaning behind it. For example, A simple facial expression like smiling can be translated into a sign of approval, or it can be translated into a sign of friendly. If we put all those emotion into big data, it will help us to understand ourselves much better.&lt;/p>
&lt;h4 id="332-sense-organs-on-the-face">3.3.2 Sense Organs on The Face&lt;/h4>
&lt;p>The facial expression expresses our emotion during the communication by micro movement of our sense organs. The most used organs are the eyes and mouth and sometimes, the eyebrows.&lt;/p>
&lt;h5 id="3321-eye">3.3.2.1 Eye&lt;/h5>
&lt;p>The eyes are one of the most important communication tools in our ways of communication with each other. When we communicate with each other, the eye contact will be inevitable. The signal in your eye will tell people what you are think.
Eye gaze is a sample of paying attention when communicating with others. When you are talking to a person and if his eye is directly on you and both of you keep having eye contact. In this situation, this mean that he is interested in what you say and is paying attention to what you say. On the other hand, if the action of breaking eye contact happens very frequently, it means that he is not interested, distracted, or not paying attention to you and what you are saying.&lt;/p>
&lt;p>Blinking is another eye signal that is very often and will happen in communicating with other people. When talking to other people, blinking is very usual and will happen every time when you are going to communicate with different people. But the frequency of blanking can give away what are you feeling right now. People often blink more rapidly when they are feeling distressed or uncomfortable. Infrequent blinking may indicate that a person is intentionally trying to control his or her eye movements &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. For example, when A person is lying, he might try to control his blinking frequency to make other people feel like he is calm and saying the truth. In order to persuade other people that he is calm and telling the truth, he will need to blink less frequently.&lt;/p>
&lt;p>Pupil size is a very important facial expression. Pupil size can be a very subtle nonverbal communication signal. While light levels in the environment control pupil dilation, sometimes emotions can also cause small changes in pupil size &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. For example, when you are surprised by something, your pupil size will become noticeably larger than before. When having a communication, dilated eyes can also mean that the person is interesting in the communication.&lt;/p>
&lt;h5 id="3322-mouth">3.3.2.2 Mouth&lt;/h5>
&lt;p>Mouth expression and movement will also be a huge part in communicating with other and reading body language. The easiest example will be smiling. A micro movement of your mouth and lip will give signal to others about what do you think or how are you feeling.
When you tighten your lips, it means that you either distaste, disapprove or distrust other people when having a conversation.
When you bite your lips, it means that you are worried, anxious, or stressed.
When someone tries to hide certain emotional reaction, they tend to cover their mouth in order not to display any facial expression through lip movement. For example, when you are laughing.
The simple movement of turning up or down of the lip will also indicate what a person is feeling. When the mouth is slightly turn up, it might mean that the person is either feeling happy or optimistic. On the other hand, a slightly down-turned mouth can be an indicator of sadness, disapproval, or even an outright grimace &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="333-facial-expression-and-big-data">3.3.3 Facial Expression and Big Data&lt;/h4>
&lt;p>Nowadays, since technology is so advance, everything around us can be turn into data and everything can be related to data. Facial expression is often study by different scientist in research because it allows us to understand more about human and communication between different people. One of the relatively new and promising trends in using facial expressions to classify learners' emotions is the development and use of software programs the automate the process of coding using advanced machine learning technologies. For example, FaceReader is a commercially available facial recognition program that uses an active appearance model to model participant faces and identifies their facial expression. The program further utilizes an artificial neural network, with seven outputs to classify learnerâ€™s emotions &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. Also, facial expression can be analyzed in other software programs like the Computer Expression Recognition Toolbox. Emotion is a huge study field in the technology field, and facial expression is one of the best ways to study and analyze people&amp;rsquo;s emotion. Emotion technology is becoming huge right now and will be even more popular in the future according to MIT Technology Review, Emotion recognition â€“ or using technology to analyze facial expressions and infer feelings-is, by one estimate, set to be a $25 billion business by 2023 &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. So back to the topic about big data and facial expression. Why are those things related? It is because, first everything is data around us. Your facial expression can be stored into data for other to learn and detect too. One of the examples is that, in 2003, The US Transportation Security Administration started training humans to spot potential terrorists by reading their facial expression. And by that, scientist believe that if human can do that, with data and AI technology, robot can detect facial expression more accurate than human.&lt;/p>
&lt;h4 id="334-the-problem-with-detecting-emotion-for-technology-nowadays">3.3.4 The Problem with Detecting Emotion for Technology Nowadays&lt;/h4>
&lt;p>Even though facial expression can reveal people&amp;rsquo;s emotion and what they think, but there has been &amp;ldquo;growing pushback&amp;rdquo; against the statement. A group of scientists brought together a research after reviewing more than 1,000 paper on emotion detection. After the research, the conclusion of it is hard to use facial expressions alone to accurately tell how someone is feeling is made. Human&amp;rsquo;s mind is very hard to predict. People do not always cry when they feel down and smile when they feel happy. The facial expression can not always reveal the true feeling the person is feeling. Not only that, because there is not enough data for facial expression, people will often mistakenly categorize other&amp;rsquo;s facial expression. For example, Kairos, which is a facial biometrics company, promise retailers that it can use a emotion recognition technology to figure out how their customers are feeling. But when they are labeling the data to feed the algorithm, one big problem reveals. An observer might read a facial expression as &amp;ldquo;surprised,&amp;rdquo; but without asking the original person, it is hard to know what the real emotion was &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. So the problems with technology that involves around facial expression are first, there is not enough data. Second is that facial expression sometimes can not be always true.&lt;/p>
&lt;h4 id="335-classification-algorithms">3.3.5 Classification Algorithms&lt;/h4>
&lt;p>Nowadays, since technology is growing so fast, there are a lot of interaction between humans and computer. Facial expression plays an essential role in social interaction with other people. It is not arguably one of the best ways to understand human. &amp;ldquo;It is reported that facial expression constitutes 55% of the effect of a communicated message while language and voice constitute 7% and 38% respectively. With the rapid development of computer vision and artificial intelligence, facial expression recognition becomes the key technology of advanced human computer interaction &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>.&amp;rdquo; This quote from the research shows that facial expression is one of the main tools that we are using to communicate with other people and interact with computer. So being able to recognize and identify the facial expression becomes relatively important. The main objective for facial expression recognitions is to use its conveying information automatically and correctly. As a result, feature extraction is very important to the facial expression recognition process. The process needs to be smooth and without any mistakes. So, algorithms are needed in the processes. Classification analysis is an important component of facial recognition, it is mainly used to find data distribution that is valuable and at the same time, find data models in the potential data. At present it has further study of the database, data mining, statistics, and other fields &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. In addition to that, one of the major obstacles and limitation of facial expression recognition is face detection. To detect the face, you will need to locate the faces in an image or a photograph. This is where scientists applicate classification algorithm, machine learning and deep learning. Recently, convolutional neural network model has become so successful that facial recognition is the next top wave &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-conclusion">4. Conclusion&lt;/h2>
&lt;p>With the development of artificial intelligence, human-computer interaction, big data, and machine learning even deep learning is getting more mature. Gesture Recognition including Hand Gesture Recognition, Body Gesture Recognition, and Face Gesture Recognition has finally come true into a real-life application and already achieved huge success in many areas. But it still has much more potential in all possible areas that could change people&amp;rsquo;s lives drastically in a good way. Gestures are the simplest and the most natural language of all human beings. It sends the clearest message for communicating between people, and even human and computers. Because of the more powerful cameras, better big data technology, and more efficient and effective algorithms from deep learning, Scientists are able to use color and the Gesture Segmentation method to remove useless color data in order to maximize the accuracy of the result. As we are doing our research, we also find out Hand Gesture Recognition is not the only Recognition in this area, Body Gesture Recognition and Face Gesture Recognition or facial expression are also very important, they can also deliver messages in the simplest way. They are also very effective when building relationships between humans and machines. Face Gesture or facial expression could not only deliver messages but even deliver emotions. Micromovements of facial expressions studied by different scientists could be very useful in predicting the emotions of humans. Body Gesture Recognition is also helpful as we did our research with the body gesture data scientists collected from different musicians with different instruments. They are able to predict the melodies or even the songs played by that musician. This is mind-blowing because with this type of technology and applications we are able to achieve more and use it in many possible fields. With all these combined, scientists could build a very successful and mature Gesture Recognition model to get the most accurate result or prediction. According to the research and our own analysis, we come up with a conclusion that Gesture Recognition will be the next hot trendy topic and are applicable in many possible areas including Security, AI, economics, manufacture, the game industry, and even medical services. With Gesture Recognition being applied, scientists are able to develop much smarter AIs and machines that can interact with humans more efficiently and more fluently. AIs will be able to receive and understand messages from humans more easily and will able to function better. This is also a great message for many handicapped people. With Hand Gesture Recognition being used, their life will also be easier and happier and thatâ€™s definitely something we are want to see because the overall goal of all the technologies is to make people&amp;rsquo;s life easier and bring the greatest amount of happiness to the greatest amount of people. However, the technology we have right now is not advanced enough yet, in order to get a more accurate result, we still need to develop better cameras, better algorithms, and better models. But we all believe that this era is the big data era, and everything could happen as big data and deep learning technology get more and more advanced and mature. We believe in and look forward to the beautiful future of Gesture Recognition. And we also think people should really pay more and closer attention to this field since Gesture Recognition is the next wave.&lt;/p>
&lt;h2 id="5-references">5. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Srilatha, Poluka, and Tiruveedhula Saranya. &amp;ldquo;Advancements in Gesture Recognition Technology.&amp;rdquo; IOSR Journal of VLSI and Signal Processing, vol. 4, no. 4, 2014, pp. 01â€“07, iosrjournals.org/iosr-jvlsi/papers/vol4-issue4/Version-1/A04410107.pdf, 10.9790/4200-04410107. Accessed 25 Oct. 2020.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Bazarevdsky, V., &amp;amp; Zhang, F. (2019, August 19). On-device, real-time hand tracking with MediaPipe. Google AI Blog. &lt;a href="https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html">https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>F. Zhan, &amp;ldquo;Hand Gesture Recognition with Convolution Neural Networks,&amp;rdquo; 2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI), Los Angeles, CA, USA, 2019, pp. 295-298, doi: 10.1109/IRI.2019.00054.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Di Zhang, DZ.(2019) Research on Hand Gesture Recognition Technology Based on Machine Learning, Nanjing University of Posts and Telecommunications.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>A. Choudhury, A. K. Talukdar and K. K. Sarma, &amp;ldquo;A novel hand segmentation method for multiple-hand gesture recognition system under complex background,&amp;rdquo; 2014 International Conference on Signal Processing and Integrated Networks (SPIN), Noida, 2014, pp. 136-140, doi: 10.1109/SPIN.2014.6776936.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Cherry, K. (2019, September 28). How to Read Body Language and Facial Expressions. Verywell Mind. Retrieved November 8, 2020, from &lt;a href="https://www.verywellmind.com/understand-body-language-and-facial-expressions-4147228">https://www.verywellmind.com/understand-body-language-and-facial-expressions-4147228&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Segal, J., Smith, M., Robinson, L., &amp;amp; Boose, G. (2020, October). Nonverbal Communication and Body Language. HelpGuide.org. &lt;a href="https://www.helpguide.org/articles/relationships-communication/nonverbal-communication.htm">https://www.helpguide.org/articles/relationships-communication/nonverbal-communication.htm&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Martineau, K. (2020, June 25). Identifying a melody by studying a musicianâ€™s body language. MIT News | Massachusetts Institute of Technology. &lt;a href="https://news.mit.edu/2020/music-gesture-artificial-intelligence-identifies-melody-by-musician-body-language-0625">https://news.mit.edu/2020/music-gesture-artificial-intelligence-identifies-melody-by-musician-body-language-0625&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>N. Normani et al., &amp;ldquo;A machine learning approach for gesture recognition with a lensless smart sensor system,&amp;rdquo; 2018 IEEE 15th International Conference on Wearable and Implantable Body Sensor Networks (BSN), Las Vegas, NV, 2018, pp. 136-139, doi: 10.1109/BSN.2018.8329677.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Bon-Woo Hwang, Sungmin Kim and Seong-Whan Lee, &amp;ldquo;A full-body gesture database for automatic gesture recognition,&amp;rdquo; 7th International Conference on Automatic Face and Gesture Recognition (FGR06), Southampton, 2006, pp. 243-248, doi: 10.1109/FGR.2006.8.&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Harley, J. M. (2016). Facial Expression. ScienceDirect. &lt;a href="https://www.sciencedirect.com/topics/computer-science/facial-expression">https://www.sciencedirect.com/topics/computer-science/facial-expression&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Chen, A. (2019, July 26). Computers canâ€™t tell if youâ€™re happy when you smile. MIT Technology Review. &lt;a href="https://www.technologyreview.com/2019/07/26/238782/emotion-recognition-technology-artifical-intelligence-inaccurate-psychology/">https://www.technologyreview.com/2019/07/26/238782/emotion-recognition-technology-artifical-intelligence-inaccurate-psychology/&lt;/a>&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Ou, J. (2012). Classification algorithms research on facial expression recognition. Retrieved from &lt;a href="https://www.sciencedirect.com/science/article/pii/S1875389212006438">https://www.sciencedirect.com/science/article/pii/S1875389212006438&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Brownlee, J. (2020, August 24). How to perform face detection with deep learning. Retrieved from &lt;a href="https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/">https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/&lt;/a>&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data in the Healthcare Industry</title><link>/report/fa20-523-352/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-352/report/report/</guid><description>
&lt;p>Cristian Villanueva, Christina Colon&lt;/p>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-352/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-352/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-352/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-352/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-352/">fa20-523-352&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-352/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Healthcare is an organized provision of medical practices provided to individuals or a community. Over centuries the application of innovative healthcare has been needed increasingly as humans expand their life span and become more aware of better preventative care practices. The application of Big Data within the industry of Healthcare is of the utmost importance in order to quantify the effects of wide scale efficient and safe solutions. Pharmaceutical and Bio Data Research companies can use big data to intake large facets of patient record data and use this collected data to iterate how preventative care can be implemented before diseases actually present themselves in stages that are beyond the point of potential recovery. Data collected in laboratory settings and statistics collected from medical and state institutions of healthcare facilitate time, money, and life saving initiatives as deep learning can in certain instances perform better than the average doctor at detecting malignant cells. Big data within healthcare has proven great results for the advancement and diverse application of informed reasoning towards medical solutions.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1--introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-patient-records">2. Patient Records&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-ehr-application-detecting-error-and-reducing-costs">2.1 EHR Application: Detecting Error and Reducing Costs&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-ai-models-in-cancer-detection">3. AI Models in Cancer Detection&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-early-detection-big-data-applications">3.1 Early Detection Big Data Applications&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-detecting-cervical-cancer">3.2 Detecting Cervical Cancer&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-artificial-intelligence-in-cardiovascular-disease">4. Artificial intelligence in Cardiovascular Disease&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-deep-learning-techniques-for-genomics">5. Deep Learning Techniques for Genomics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-discussion">6. Discussion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgements">8. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> EHR, Healthcare, diagnosis, application, treatment, AI, network, records&lt;/p>
&lt;h2 id="1--introduction">1. Introduction&lt;/h2>
&lt;p>Healthcare is a multi-dimensional system established with the aim of the prevention, diagnosis, and treatment of health-related issues or impairments in human beings&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The many dimensions of Healthcare can be characterized by the influx of information coming and going from each level as there are multiple different applications of Healthcare. These applications can include but are not limited to vaccines, surgeries, x-rays, medicines/treatments. Big data plays a pivotal role in Healthcare diagnostics, predictions, and accelerated results/outcomes of these applications. Big Data has the ability to save millions of dollars through automating 40% of radiologistâ€™s tasks, saving time on potential treatments through digital patients, and by providing improved outcomes&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. With higher accuracy rates of diagnosis and advanced AI is able to transform hypothetical analysis into data driven diagnosis and treatment strategies.&lt;/p>
&lt;h2 id="2-patient-records">2. Patient Records&lt;/h2>
&lt;p>EHR stands for &amp;lsquo;electronic health records&amp;rsquo; and is a digital version of a patientâ€™s paper chart.The Healthcare industry utilizes EHR for maintaining records of everything related in their institutions. EHR are real-time, patient centred records that make information available instantly and securely to authorized users&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. EHR is capable of holding even more information as it is possible to include such information such as medical history, diagnoses, medications, treatment plans, immunization dates, allergies, radiology images, and laboratory and test results. According to Definitive Healthcare data from 2020, more than 89 percent of all hospitals have implemented inpatient or ambulatory EHR systems &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. A network of information surrounding a patient&amp;rsquo;s health record and medical data allows for the research and production of such progressive advancement in treatment. To underline the potential of the resources, more than 110 million EHRs around the continents were inspected for genetic disease research&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This is the capability of EHRs as it holds information capable of diagnosing, preventing and treating other patients for early detection of an ailment or disease. Through the application of neural networks in Deep Learning models, EHRâ€™s could be compiled and analyzed to identify inconspicuous indicators of disease development of patients in early stages far before a human doctor would be able to make a clear diagnosis. The application has the ability to work far ahead for preventive measures as well as the allocation of resources to make sure that patients are paying for the care at minimum costs, the appropriate method of medical intervention is applied, and physiciansâ€™ workload can become less strenuous.&lt;/p>
&lt;h3 id="21-ehr-application-detecting-error-and-reducing-costs">2.1 EHR Application: Detecting Error and Reducing Costs&lt;/h3>
&lt;p>In order to understand the impact that Big Data such as EHRs has on the Healthcare industry an example of research is presented in the form of collection before and after implementation of EHR. The research study collected data for the period of 1 year before EHR (pre-EHR) and 1 year after EHR (post-EHR) implementation. What was noticed in the analyzes of the data was in the area of &amp;lsquo;Medication errors and near misses&amp;rsquo; the research stated &amp;lsquo;medication errors per 1000 hospital days decreased 14.0%-from 17.9% in the pre-EHR period to 15.4% in the 9 months after CPOE implementation&amp;rsquo;&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The research determined that with implementation of EHR with (CPOE) computerized provider order entry was able to reduce the costs of treatment and improvised upon the safety of their patients. Participants of the study mentioned that there was an increase in speed when it came to pharmacy, laboratory and radiology orders. The research also stated &amp;lsquo;our study demonstrated an 18% reduction in laboratory testing&amp;rsquo;. The study touched upon the rapidness that EHR can add to a process of treatment when orders are validated much quicker and hospitals and patients save money from the rapid diagnosis and treatment. This cuts out the middle-man of deliberate testing and examinations upon patients so they donâ€™t have to cover the costs or undergo wasteful testing from their own EHR and other extensive EHR that it utilizes for comparison. Examples of models used in this example study include data mining through phenotyping and natural language processing. In this way data mining allows large sets of patient data to be aggregated in order to make inferences over a population or theories regarding how a disease will progress in any given patient. Phenotyping categorizes features of patients' health and their DNA and ultimately their overall health. Association rule data mining helps automated systems in their predictions in order to predict behavioral and health outcomes of patientsâ€™ circumstances.&lt;/p>
&lt;h2 id="3-ai-models-in-cancer-detection">3. AI Models in Cancer Detection&lt;/h2>
&lt;p>AI is modifying early detection of cancer as models are capable of being more accurate and precise with the analysis of mass and cell images. The difficulty of diagnosing cancer is because of the possibilities of either the mass being benign or malignant. The amount of time overlooking the cell nuclei and its features to either determine if it is malignant or benign can be staggering for oncologists. Utilizing the information of what&amp;rsquo;s known about cancer can train AI to be calibrated to scour through several images and screenings of cell nuclei to find the key indicators. These key indicators can also be whittled down even further as there is AI to determine which indicators have the highest correlation with malignant cancer. As a dataset from Kaggle consisting of 569 cases of malignant and benign breast cancer, it represented 357 cases of benign and 212 of malignant. With that information there were initially 33 features that may have indicated malignancy in these cases. The 33 features were reduced to 10 features as not all of them equally displayed the same level of contribution to the diagnosis. Across the 10 features there were 5 features that demonstrated the highest correlation to the malignancy. Several models were adapted to find the highest accuracy and precision. This form of AI detection improves upon the efficacy of early cancer detection.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-352/raw/main/report/images/aimodels.PNG" alt="AI Models Demonstrate Accuracy &amp; Precision">&lt;/p>
&lt;p>&lt;strong>Figure 1.&lt;/strong> Demonstrates how AI in this study used images to cross-analyze features of a patient&amp;rsquo;s results to verify what model is the most accurate and precise to determine which model can best serve a physician in their diagnostic report.&lt;/p>
&lt;h3 id="31-early-detection-big-data-applications">3.1 Early Detection Big Data Applications&lt;/h3>
&lt;p>&amp;lsquo;An ounce of prevention is worth more than a pound of a cure&amp;rsquo; is a common philosophy held by medical professionals. The meaning behind this ideology is found in that if one can prevent a disease from ever taking its final form through performing small routine tasks and check ups, a plethora of harm and suffering from trying to recage a disease can be avoided. Many medical solutions for diseases such as cancer or degenerative brain diseases rely on the idea that outside medical intervention will strengthen the patient enough for the human body to heal itself through existing biological principles &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. For example, vaccines work by injecting dead cells into a patient so that its antibodies can be learned and immunity can be built up by white blood cells naturally. Intervening before one is infected must be completed for these measures to be effective. If preventative care such as routine screenings on individuals with family history of diseases or those with general genetic predispositions then the power truly lies in having the discernment knowledge to catch the disease early. In many diseases once a patient is presenting symptoms, it is too late or survival/recovery probability percentages are slashed. This places immense pressures on patients themselves to work to have access to routine screenings and even more pressure on physicians to intake these patients and make preliminary diagnosis with little more than a visual analysis of the patient. Big data automates these tasks and gives physicians an incredible advantage and discernment as to what is truly happening within a patientâ€™s circumstance.&lt;/p>
&lt;h3 id="32-detecting-cervical-cancer">3.2 Detecting Cervical Cancer&lt;/h3>
&lt;p>Cervical cancer in the past was one of the most common causes of cancer death for women in the United States. However preventive care in the form of pap test has been able to drop the death rate significantly. In the pap test images are taken of the womenâ€™s cervix to identify any changes that might indicate cancer is going to form. Cervical cancer has a much higher death rate without early detection as a cure is easier to take full effect in the early stages. Artificial Intelligence performs an algorithm and gives the computer the ability to act and reason based on a set of known information. Machine learning implements more data and allows the computer to work iteratively and make predictions and make decisions based on the massive amount of data provided. In this way, machines have had the ability to detect cervical cancer with greater precision and accuracy in some cases than gynecologists &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Imaging of cervical screenings targeted by a convolution neural network is the key to unlocking correlations behind the large sum of images. By implementing further reasoning into the data set, the CNN is able to classify enhanced recognition of cancer as or before it forms. This study using this method of machine learning has been able to perform with 90-96% accuracy and save lives. The CNN is able to identify the colors, shapes, sizes, edges and other features pertaining to cancerous cells.&lt;/p>
&lt;p>This is ground breaking for women in underdeveloped countries like India and Nigeria where the death rate for cervical cancer is much higher than the United States due to lack of access to routine pap smears. Women could get results on their cervical cancer status even if they do not get a pap smear every 3 years as recommended by doctors. For example if a woman in Nigeria has her first pap smear at the age of 40 when the recommended age to start pap smears is 21 she has gone unchecked for nearly 20 years and the early detection window is narrowed. However, if she is one of the 20% of women who get cervical cancer over the age of 65, a deep learning analysis of her pap smear at 40 could save her life and roadblock potential suffering. Early detection is key and big data optimizes early detection windows by providing a deeper analysis in the preventive care stages. From here doctors are able to implement the best care plan available on a case by case basis.&lt;/p>
&lt;h2 id="4-artificial-intelligence-in-cardiovascular-disease">4. Artificial intelligence in Cardiovascular Disease&lt;/h2>
&lt;p>AI in cardiovascular disease models are innovating disease detection by segmenting different types of analysis together for more efficient and accurate results. Being that cardiovascular diseases typically agitate/involve the heart and lungs there are numerous dynamics surrounding why a person is experiencing certain symptoms or at risk for development of a more critical diagnosis. Immense amount of labor is included in the diagnosis and treatment of individuals with cardiovascular disease on behalf of general physicians, specialists, nurses, and several other medical professionals. Artificial intelligence has the capability to add a layer of ease and accuracy that is involved in analyzing a patient&amp;rsquo;s status or risk for cardiovascular disease. AI is able to overcome the challenges of low quality pixelated images from analyzes and draw clearer and more accurate conclusions at a stage where more prevention strategies can be implemented. AI in this sense is able to analyze the systems of the human body as a whole as opposed to a doctor which might have several appointments with a patient to determine results from evaluations on lungs, heart, etc. By segmenting x-rays from numerous patients AI is able to learn and grow its data set to produce increasingly accurate and precise results[^8].
By using a combination of recurrent neural networks and convolutional neural networks artificial intelligence is able to go beyond what currently exists in terms of medical analysis and provide optimum results for patients in need. Recurrent neural networks function by building upon past data in order to create new output in series. They work hand in hand with Convolutional Neural networks which focus on analyzing advanced imagery based on qualitative data and can weigh biases on potential prescriptive outcomes.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-352/raw/main/report/images/cardioai.PNG" alt="AI Learning Wireframe">[^10]&lt;/p>
&lt;p>&lt;strong>Figure 2.&lt;/strong> Demonstrates a wireframe of how data is computed to draw relevant conclusions from thousands of images and pinpoint exact predictions of diagnosis. Risk analysis is crucial for heart attack prevention and understanding how suspeectable a person is to heart failure. Being that heart attacks can lead to strokes due to loss of blood and oxygen to the brain, these imaging tools serve as an invaluable life saving mechanism to help bring prevention to the forefront of these medical emergencies.&lt;/p>
&lt;h2 id="5-deep-learning-techniques-for-genomics">5. Deep Learning Techniques for Genomics&lt;/h2>
&lt;p>A digital patient is the idea that a patientâ€™s health record can be compiled with live and exact biometrics for the purpose of testing. Through this method medical professionals will have the ability to propose new solutions to patients and monitor potential effects of operations or medicines over a period of time in a condensed/rapid results format. Essentially if a patient would be able to see how their body reacts to medical procedures before they are performed. The digital copy of a patient would receive simulated trial treatments to better understand what would happen over a period of time if the solution was adopted. For example, a patient would be able to verify with their physician what type of diuretics, beta inhibitors, or angiotensin receptor blocker medication would be the most effective solution to their hypertension regulatory needs[^11]. Physicians would be able to mitigate the risks and side effects associated with a certain solution given a patients expected behavior in response to what has been uploaded to the model.
In order to produce deep learning results, models must be implemented by indicating genetic markers by which computational methods can traverse the genetics strands and draw relevant conclusions. In this way data can be processed to propose changes to disease carrying chains of DNA or fortify immune based responses in those who are immunocompromised[^9].&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-352/raw/main/report/images/genomics.PNG" alt="Genomics Illustration">&lt;/p>
&lt;p>&lt;strong>Figure 3.&lt;/strong> Illustrates how genes are analyzed through data collection methods such as EHR, personal biometric data, and family history in order to track what type of disease poses a threat and how to prevent, predict, and treat disease at the molecular level. Producing accurate methods of treatments, medications as well as predictions without having to put the patient through any trials.&lt;/p>
&lt;h2 id="6-discussion">6. Discussion&lt;/h2>
&lt;p>In considering the numerous innovations made possible by Big Data one can expect major impacts on society as we know it. Access to these types of data solutions should be made accessible to all those who are in need. Collectively an effort must be made to promote equitable access to life saving artificial intelligence discussed in this report. Processing power and lack of resources stand as a barrier to widespread access to proper testing. However, governments and industries in the private sector must work together to avoid monopolies and price gouging limitations to such valuable data and computing models.
With further investment into deep learning models error margins can be narrowed and risk percentages and be slimmed pertaining to prescriptive analysis in specific use cases. The more access to information and examples are available, the better and more advanced a deep learning system can become. With the addition of electronic health records and past analysis artificial intelligence has the power to exponentially revolutionize the healthcare industry. By providing patients with services that could save their lives there is more incentive to stay involved in personal health as computation is optimized targeting patients for more results focused visits to the doctor. Doctors themselves are able to be relieved of a portion of the workload and foster a greater work life balance through cutting down on testing time and having more time to interact with patients for educational informative appointments. Legally medical professionals will be able to use prediction errors as alternative signals to further analyze a patient and justify treatment measures. Using data visualization of potential outcomes via a specific treatment method will empower patients and doctors to choose the pathway with the most favorable outcome.Convolutional Neural Networks within deep learning is one of the if not the most essential form of algorithm for AI in healthcare. CNN allows images to be input in a way that allows for learnable weights and biases to be calculated for and differentiate and match aspects of images that would go unknown to the human eye. Through identifying the edges, shape, size, color, amount of scarring CNN is able to identify cancerous and non-cancerous cells into five categories: normal, mild, moderate, severe, and carcinoma. Accuracy in this space is above 95% and creates a new opportunity space for medical professionals to provide their patients with a high level of accuracy and timely action planning for treatment and recovery[^13]. Beyond human healthcare CNN modeling has the potential to transfer into the realm of veterinary medicine, agricultural engineering, and sustainable environment initiatives to detect invasive species and similar disease development. Dogs or cats with cancer or heart worm could be analyzed in order to determine that with their heredity/breed and life span what are the chances and timeline for disease development. Crop production could be amplified with the processing of plant genomes in combination with soil to foresee what combination will produce the most abundant and profitable harvest. Lastly, ecosystems distrubed by global warming have the capability of being studied with CNN in order to factor in changes to the environment and what solutions could be on the horizon. With enough sample collection the power of CNN has the capability of securing a brighter future for tomorrow.&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>Healthcare is an essential resource to living a long life and without it we can see our lifespan slashed nearly in half or even more for those who are hindered by hereditary ailments. Healthcare has been around as long as medicine and such other treatments have been around and that was centuries ago. The field has expanded well beyond what couldâ€™ve been expected for any medical professional or institution. Where the information and resources are available to save and care for the life before them even when a lack of training can hinder them the resources are present. It&amp;rsquo;s come to be such an accomplishment to mesh the medical practices of many medical professionals and Big Data to develop the largest compendium of medical practices in the world. By the allowance of such an asset many are able to collaborate with new findings and reinforcing old findings as these prevalent results allow physicians to work without faltering over inconclusive findings. The goal for this area of Big Data is to continue making the EHR system more secure and friendly towards medical professionals in different areas of practice as well as allowing easy access for patients who seek out their own medical history. The more advancements in this area of Healthcare can be applicable to other fields that must reference the compendium that maintains individuals and their history going forward. Such a structure will continue to aid generations of physicians and patients alike and can aid technological advancements along the way.&lt;/p>
&lt;h2 id="8-acknowledgements">8. Acknowledgements&lt;/h2>
&lt;p>We would like to thank Dr Gregor von Laszweski for allowing us to complete this report despite the delays there was as well as the lack of communication. We would also like to thank the AI team for their commitment to assisting in this class as even through a pandemic they continued to help the students complete the course. We would also like to thank Dr. Geoffrey Fox for teaching the course and making the class as informative as possible given his experience with the field of Big Data.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>[^8] Arslan, M., Owais, M., &amp;amp; Mahmood, T. Artificial Intelligence-Based Diagnosis of Cardiac and Related Diseases (2020, March 23). Retrieved December 13, 2020 from &lt;a href="https://www.mdpi.com/2077-0383/9/3/871/htm">https://www.mdpi.com/2077-0383/9/3/871/htm&lt;/a>&lt;/p>
&lt;p>[^9] Eraslan, G., Avsec, Z., Gagneur, J., &amp;amp; Theis, Fabian J.. Deep learning: new computational modelling techniques for genomics. (2019, April 10). Retrieved December 14, 2020 from &lt;a href="https://www.nature.com/articles/s41576-019-0122-6">https://www.nature.com/articles/s41576-019-0122-6&lt;/a>&lt;/p>
&lt;p>[^10] 1Regina. AI to Detect Cancer. (2019, November 22). Retrieved December 14, 2020 from &lt;a href="https://towardsdatascience.com/ai-for-cancer-detection-cadb583ae1c5">https://towardsdatascience.com/ai-for-cancer-detection-cadb583ae1c5&lt;/a>&lt;/p>
&lt;p>[^11] Koumakis, L. Deep learning models in genomics; are we there yet? (2020). Retrieved December 14, 2020 from &lt;a href="https://www.sciencedirect.com/science/article/pii/S2001037020303068">https://www.sciencedirect.com/science/article/pii/S2001037020303068&lt;/a>&lt;/p>
&lt;p>[^12] Ross, M.K., Wei, W., &amp;amp; Ohno-Machado, L., &amp;lsquo;Big Data&amp;rsquo; and the Electronic Health Record (2014, August 15). Retrieved 15, 2020 from &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4287068/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4287068/&lt;/a>&lt;/p>
&lt;p>[^13] P, Shanthi. B., Faruqi, F., K, Hareesha, K., &amp;amp; Kudva, R., Deep Convolution Neural Network for Malignancy Detection and Classification in Microscopic Uterine Cervex Cell Images (2019, November 1). Retrieved December 15, 2020 from &lt;a href="https://pubmed.ncbi.nlm.nih.gov/31759371/">https://pubmed.ncbi.nlm.nih.gov/31759371/&lt;/a>&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Laney, D., AD. Mauro, M., Gubbi, J., Doyle-Lindrud, S., Gillum, R., Reiser, S., . . . Reardon, S. Big data in healthcare: Management, analysis and future prospects (2019, June 19). Retrieved December 10, 2020, from &lt;a href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0217-0">https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0217-0&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>What is an electronic health record (EHR)? (2019, September 10). Retrieved December 10, 2020 from &lt;a href="https://www.healthit.gov/faq/what-electronic-health-record-ehr">https://www.healthit.gov/faq/what-electronic-health-record-ehr&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Moriarty, A. Does Hospital EHR Adoption Actually Improve Data Sharing? (2020, October 23) Retrieved December 10, 2020 from &lt;a href="https://blog.definitivehc.com/hospital-ehr-adoption">https://blog.definitivehc.com/hospital-ehr-adoption&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Cruciana, Paula A. The Implications of Big Data in Healthcare (2019, November 21) Retrieved December 11, 2020 from &lt;a href="https://ieeexplore.ieee.org/document/8970084">https://ieeexplore.ieee.org/document/8970084&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Zlabek, Jonathan A. Early cost and safety benefits of an inpatient electronic health record (2011, February 2) Retrieved December 10, 2020 from &lt;a href="https://academic.oup.com/jamia/article/18/2/169/802487">https://academic.oup.com/jamia/article/18/2/169/802487&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Artificial Intelligence-Oppurtunities in Cancer Research. (2020, August 31). Retrieved December 11, 2020 from &lt;a href="https://www.cancer.gov/research/areas/diagnosis/artificial-intelligence">https://www.cancer.gov/research/areas/diagnosis/artificial-intelligence&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Zhang, R., Simon, G., &amp;amp; Yu, F. Advancing Alzheimer&amp;rsquo;s research: A review of big data promises. (2017, June 4) Retrieved December 11, 2020 from &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5590222/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5590222/&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analysis of Various Machine Learning Classification Techniques in Detecting Heart Disease</title><link>/report/fa20-523-309/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-309/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-309/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-309/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Ethan Nguyen, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-309">fa20-523-309&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>As cardiovascular diseases are the number 1 cause
of death in the United States, the study of the factors and early detection and treatment could improve quality of life and lifespans. From investigating how the variety of factors related to cardiovascular health relate to a general trend, it has resulted in general guidelines to reduce the risk of experiencing a cardiovascular disease. However, this is a rudimentary way of preventative care that allows for those who do not fall into these risk categories to fall through. By applying machine learning, one could develop a flexible solution to actively monitor, find trends, and flag patients at risk to be treated immediately. Solving not only the risk categories but has the potential to be expanded to annual checkup data revolutionizing health care.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-dataset-cleaning">2.1 Dataset Cleaning&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-dataset-analysis">2.2 Dataset Analysis&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-machine-learning-algorithms-and-implementation">3. Machine Learning Algorithms and Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-scikit-learn-and-algorithm-types">3.1 Scikit-Learn and Algorithm Types&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-classification-algorithms">3.2 Classification Algorithms&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#321-support-vector-machines">3.2.1 Support Vector Machines&lt;/a>&lt;/li>
&lt;li>&lt;a href="#322-k-nearest-neighbors">3.2.2 K-Nearest Neighbors&lt;/a>&lt;/li>
&lt;li>&lt;a href="#323-gaussian-naive-bayes">3.2.3 Gaussian Naive Bayes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#324-decision-trees">3.2.4 Decision Trees&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#33-clustering-algorithms">3.3 Clustering Algorithms&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#331-k-means">3.3.1 K-Means&lt;/a>&lt;/li>
&lt;li>&lt;a href="#332-mean-shift">3.3.2 Mean-shift&lt;/a>&lt;/li>
&lt;li>&lt;a href="#333-spectral-clustering">3.3.3 Spectral Clustering&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#34-implementation">3.4 Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#341-dataset-preprocessing">3.4.1 Dataset Preprocessing&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-results--discussion">4. Results &amp;amp; Discussion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-algorithm-metrics">4.1 Algorithm Metrics&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#411-support-vector-machines">4.1.1 Support Vector Machines&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-k-nearest-neighbors">4.1.2 K-Nearest Neighbors&lt;/a>&lt;/li>
&lt;li>&lt;a href="#413-gaussian-naive-bayes">4.1.3 Gaussian Naive Bayes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#414-decision-trees">4.1.4 Decision Trees&lt;/a>&lt;/li>
&lt;li>&lt;a href="#415-k-means">4.1.5 K-Means&lt;/a>&lt;/li>
&lt;li>&lt;a href="#416-mean-shift">4.1.6 Mean-shift&lt;/a>&lt;/li>
&lt;li>&lt;a href="#417-spectral-clustering">4.1.7 Spectral Clustering&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#42-system-information">4.2 System Information&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-discussion">4.3 Discussion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgements">6. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#references">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> health, healthcare, cardiovascular disease, data analysis&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Since cardiovascular diseases are the number 1 cause of death in the United States, early prevention could help in extending oneâ€™s life span and possibly quality of life &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Since there are cases where patients do not show any signs of cardiovascular trouble until an event occurs, having an algorithm predict from their medical history would help in picking up on early warning signs a physician may overlook. Or could also reveal additional risk factors and patterns for research on prevention and treatment. In turn this would be a great tool to apply in preventive care, which is the type of healthcare policy that focuses in diagnosing and preventing health issues that would otherwise require specialized treatment or is not treatable &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This also has the potential to trickle down and increase the quality of life and lifespan of populations at a reduced cost as catching issues early most likely results in cheaper treatments &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This project will take a high-level overview of common, widely available classification algorithms and analyze their effectiveness for this specific use case. Notable ones include, Gaussian Naive Bayes, K-Nearest Neighbors, and Support Vector Machines. Additionally, two data sets that contain common features will be used to increase the training and test pool for evaluation. As well as to explore if additional feature types contribute to a better prediction. The goal of this project being a gateway to further research in data preprocessing, tuning, or development of specialized algorithms as well as further ideas on what data could be provided.&lt;/p>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.kaggle.com/johnsmith88/heart-disease-dataset">https://www.kaggle.com/johnsmith88/heart-disease-dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kaggle.com/sulianova/cardiovascular-disease-dataset">https://www.kaggle.com/sulianova/cardiovascular-disease-dataset&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The range of creation dates are 1988 and 2019 respectively with different features of which 4 are common between. This does bring up a small hiccup in preprocessing to consider. Namely the possibility of changing diet and culture trends resulting in significantly different trends/patterns within the same age group. As well as possible differences in measurement accuracy. However this large gap is within the scope of the project in exploring which features can help provide an accurate prediction.&lt;/p>
&lt;p>This possible phenomenon may be of interest to explore closely if time allows. Whether a trend itself is even present or there is an overarching trend across different cultures and time periods. Or to consider if this difference is significant enough that the data from the various sets needs to be adjusted to normalize the ages to present day.&lt;/p>
&lt;h3 id="21-dataset-cleaning">2.1 Dataset Cleaning&lt;/h3>
&lt;p>The datasets used have already been significantly cleaned from the raw data and has been provided as a csv file. These files were then imported into the python notebook as pandas dataframes for easy manipulation.&lt;/p>
&lt;p>An initial check was made to ensure the integrity of the data matched the description from the source websites. Then some preprocessing was completed to normalize the common features between the datasets. These features were gender, age, and cholesterol levels. The first two adjustments were trivial in conversion however, in the case of cholesterol levels, the 2019 set is on a 1-3 scale while the 1988 dataset provided them as real measurements. A conversion of the 1988 dataset was done based on guidelines found online for the age range of the dataset &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="22-dataset-analysis">2.2 Dataset Analysis&lt;/h3>
&lt;p>From this point on, the 1988 dataset will be referred to as &lt;code>dav_set&lt;/code> and 2019 data set will be referred to as &lt;code>sav_set&lt;/code>.&lt;/p>
&lt;p>To provide further insight on what to expect and how a model would be applied, the population of the datasets was analysed first. As depicted in Figure 2.1 the population samples of both datasets of gender vs age show the majority of the data is centered around 60 years of age with a growing slope from 30 onwards.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/agevssex.jpg" alt="Figure 2.1">&lt;/p>
&lt;p>&lt;strong>Figure 2.1&lt;/strong>: Age vs Gender distributions of the dav_set and sav_set.&lt;/p>
&lt;p>This trend appears to signify that the datasets focused solely on an older population or general trend in society of not monitoring heart conditions as closely in the younger generation.&lt;/p>
&lt;p>Moving on to Figure 2.2, we see an interesting trend with a significant growing trend in the sav_set in older population having more cardiovascular issues compared to the dav_set. While this cannot be seen in the dav_set. This may be caused by the additional life expectancy or a change in diet as noted in the introduction.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/agevstarget.jpg" alt="Figure 2.2">&lt;/p>
&lt;p>&lt;strong>Figure 2.2&lt;/strong>: Age vs Target distributions of the dav_set and sav_set.&lt;/p>
&lt;p>In Figure 2.3, the probability of having cardiovascular issues between the sets are interesting. In the dav_set the inequality of higher probability could be attributed to the larger female samples in the dataset. With the sav_set having a more equal probability between the genders.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/gendervsprobability.jpg" alt="Figure 2.3">&lt;/p>
&lt;p>&lt;strong>Figure 2.3&lt;/strong>: Gender vs Probability of cardiovascular issues of the dav_set and sav_set.&lt;/p>
&lt;p>Finally, in Figure 2.4 is the probability vs cholesterol levels. This one is very interesting between the two datasets in terms of trend levels. With the dav_set having a higher risk at normal levels compared to the sav_set. This could be another hint of a societal change across the years or may in fact be due to the low sample size. Especially since the sav_set matches the general consensus of higher cholesterol levels increasing risk of cardiovascular issues &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/cholesterolvsprobability.jpg" alt="Figure 2.4">&lt;/p>
&lt;p>&lt;strong>Figure 2.4&lt;/strong>: Cholesterol levels vs Probability of cardiovascular issues of the dav_set and sav_set.&lt;/p>
&lt;p>To close out this initial analysis is the correlation map of each of the features. From Figure 2.5 and 2.6 it can be concluded that both of these datasets are viable to conduct machine learning as the correlation factor is below the recommended value of 0.8 &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Although we do see the signs of a low sample amount in the dav_set with a higher correlation factor compared to the sav_set.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davsetcorrelation.jpg" alt="Figure 2.5">&lt;/p>
&lt;p>&lt;strong>Figure 2.5&lt;/strong>: dav_set correlation matrix.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savsetcorrelation.jpg" alt="Figure 2.6">&lt;/p>
&lt;p>&lt;strong>Figure 2.6&lt;/strong>: sav_set correlation matrix.&lt;/p>
&lt;h2 id="3-machine-learning-algorithms-and-implementation">3. Machine Learning Algorithms and Implementation&lt;/h2>
&lt;p>With many machine learning algorithms already available and many more in development. Selecting the optimal one for an application can be a challenging balance since each algorithm has both its advantages and disadvantages. As mentioned in the introduction, we will explore applying the most common and established algorithms available to the public.&lt;/p>
&lt;p>Starting off, is selecting a library from the most popular ones available. Namely Keras, Pytorch, Tensorflow, and Scikit-Learn. Upon further investigation it was determined that Scikit-Learn would be used for this project. The reason being Scikit-Learn is a great general machine learning library that also includes pre and post processing functions. While Keras, Pytorch, and Tensorflow are targeted for neural networks and other higher-level deep learning algorithms which are outside of the scope of this project at this time &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="31-scikit-learn-and-algorithm-types">3.1 Scikit-Learn and Algorithm Types&lt;/h3>
&lt;p>Diving further into the Scikit-Learn library, its key strength appears to be the variety of algorithms available that are relatively easy to implement against a dataset. Of those available, they are classified under three different categories based on the approach each takes. They are as follows:&lt;/p>
&lt;ul>
&lt;li>Classification
&lt;ul>
&lt;li>Applied to problems that require identifying the category an object belongs to.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Regression
&lt;ul>
&lt;li>For predicting or modeling continuous values.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Clustering
&lt;ul>
&lt;li>Grouping similar objects into groups.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For this project, we will be investigating the Classification and Clustering algorithms offered by the library due to the nature of our dataset. Since it is a binary answer, the continuous prediction capability of regression algorithms will not fair well. Compared to classification type algorithms which are well suited for determining binary and multi-class classification on datasets &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Along with Clustering algorithms being capable of grouping unlabeled data which is one of the key problem points mentioned in the introduction &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="32-classification-algorithms">3.2 Classification Algorithms&lt;/h3>
&lt;p>The following algorithms were determined to be candidates for this project based on the documentation available on the Scikit-learn for supervised learning &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="321-support-vector-machines">3.2.1 Support Vector Machines&lt;/h4>
&lt;p>This algorithm was chosen because classification is one of the target types and has a decent list of advantages that appear to be applicable to this dataset &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;ul>
&lt;li>Effective in high dimensional spaces as well as if the number dimensions out number samples.&lt;/li>
&lt;li>Is very versatile.&lt;/li>
&lt;/ul>
&lt;h4 id="322-k-nearest-neighbors">3.2.2 K-Nearest Neighbors&lt;/h4>
&lt;p>This algorithm was selected due to being a non-parametric method that has been successful in classification applications &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. From the dataset analysis, it is appears that the decision boundary may be very irregular which is a strong point of this type of method.&lt;/p>
&lt;h4 id="323-gaussian-naive-bayes">3.2.3 Gaussian Naive Bayes&lt;/h4>
&lt;p>Is an implementation of the Naive Bayes theorem that has been targeted for classification. The advantages of this algorithm is its speed and requires a small training set compared to more advanced algorithms &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="324-decision-trees">3.2.4 Decision Trees&lt;/h4>
&lt;p>This algorithm was chosen to investigate another non-parametric method to determine their efficacy against this dataset application. This algorithm also has some advantages over K-Nearest namely &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>.&lt;/p>
&lt;ul>
&lt;li>Simple to interpret and visualize&lt;/li>
&lt;li>Requires little data preparation
&lt;ul>
&lt;li>Handles numerical and categorical data instead of needing to normalize&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Can validate the model and is possible to audit from a liability standpoint.&lt;/li>
&lt;/ul>
&lt;h3 id="33-clustering-algorithms">3.3 Clustering Algorithms&lt;/h3>
&lt;p>The following algorithms were determined to be candidates for this project based on the table of clustering algorithms available on the Scikit-learn &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="331-k-means">3.3.1 K-Means&lt;/h4>
&lt;p>The usecase for this algorithm is general purpose with even and low number of clusters &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Of which the sav_set appears to have with the even distribution across most of the features.&lt;/p>
&lt;h4 id="332-mean-shift">3.3.2 Mean-shift&lt;/h4>
&lt;p>This algorithm was chosen for its strength in dealing with uneven cluster sizes and non-flat geometry &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Though it is not easily scalable the application of our small dataset size might be of interest.&lt;/p>
&lt;h4 id="333-spectral-clustering">3.3.3 Spectral Clustering&lt;/h4>
&lt;p>As an inverse, this algorithm was chosen for its strength with fewer uneven clusters &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. In comparison to Mean-shift, this maybe the better algorithm for this application.&lt;/p>
&lt;h3 id="34-implementation">3.4 Implementation&lt;/h3>
&lt;p>The implementation of these algorithms were done under the direction of the documentation page for each respective algorithm. The jupyter notebook used for this project is available at &lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/data_analysis/ml_algorithms.ipynb">https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/data_analysis/ml_algorithms.ipynb&lt;/a> with each algorithm having a corresponding cell. A benchmarking library is also included to determine the efficiency of each algorithm in processing time. One thing of note is the lack of functions used for the classification compared to the clustering algorithms. The justification for this discrepancy is due to inexperience in creating optimal implementations as well as determining that not being implemented in a function would not have a significant impact on performance. Additionally, graphs representing the test data were included to help visualize the performance of the clustering algorithms utilizing example code from the documentation &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="341-dataset-preprocessing">3.4.1 Dataset Preprocessing&lt;/h4>
&lt;p>Pre-processing of the cleaned datasets for the classification algorithms was done under guidance of the scikit learn documentation &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. Overall, each algorithm was trained and tested with the same split for each run. While the split data could have been passed directly to the algorithms, they were normalized further using the built-in fit_transform function for the best results possible.&lt;/p>
&lt;p>Pre-processing of the cleaned datasets for the clustering algorithms was done under guidance of the scikit learn documentation &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Compared to the classification algorithms, a dimensionality reduction was conducted using Principal component analysis (PCA). This step condenses the multiple features into a 2 feature array which the clustering algorithms were optimized for, increasing the odds for the best results possible. Another note is the dataset split was conducted during execution of the algorithm. Upon further investigation, it was determined that this does not have an effect on the ending results as the randomization was disabled due to setting the same random_state parameter for each call.&lt;/p>
&lt;h2 id="4-results--discussion">4. Results &amp;amp; Discussion&lt;/h2>
&lt;h3 id="41-algorithm-metrics">4.1 Algorithm Metrics&lt;/h3>
&lt;p>The metrics used to determine the viability of each of the algorithms are precision, recall, and f1-score. These are simple metrics based on the values from a confusion matrix which is a visualization of the False and True Positives and Negatives. Precision is essentially how accurate was the algorithm in classifying each data point. This however, is not a good metric to solely base performance as precision does not account for imbalanced distributions within a dataset &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This is where the recall metric comes in which is defined as how many samples were accurately classified by the algorithm. This is a more versatile metric as it can compensate for imbalanced datasets. While it may not be in our case as seen in the dataset analysis where we have a relatively balanced ratio. It still gives great insight on the performance for our application.&lt;/p>
&lt;p>Finally is the f1-score which is the harmonic mean of the precision and recall metric &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. This will be the key metric we will mainly focus on as it strikes a good balance between the two more primitive metrics. Since one may think in medical applications one would want to maximize recall, it is at the cost of precision which ends up in more false predictions which is essentially an overfitting scenario &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. Something that reduces the viability of the model to the application especially since we have a relatively balanced dataset, more customized weighting is not as necessary.&lt;/p>
&lt;p>The metrics for each algorithm implementation are as follows. The training time metric is provided by the cloudmesh.common benchmark library &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="411-support-vector-machines">4.1.1 Support Vector Machines&lt;/h4>
&lt;p>&lt;strong>Table 4.1:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.94&lt;/td>
&lt;td>0.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.95&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.97&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.038 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.2:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.94&lt;/td>
&lt;td>0.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.95&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.97&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>167.897 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="412-k-nearest-neighbors">4.1.2 K-Nearest Neighbors&lt;/h4>
&lt;p>&lt;strong>Table 4.3:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.88&lt;/td>
&lt;td>0.86&lt;/td>
&lt;td>0.87&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.87&lt;/td>
&lt;td>0.90&lt;/td>
&lt;td>0.88&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.025 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.4:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.62&lt;/td>
&lt;td>0.74&lt;/td>
&lt;td>0.67&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.67&lt;/td>
&lt;td>0.54&lt;/td>
&lt;td>0.60&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>10.116 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="413-gaussian-naive-bayes">4.1.3 Gaussian Naive Bayes&lt;/h4>
&lt;p>&lt;strong>Table 4.5:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.88&lt;/td>
&lt;td>0.81&lt;/td>
&lt;td>0.84&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.83&lt;/td>
&lt;td>0.90&lt;/td>
&lt;td>0.86&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.011 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.6:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.90&lt;/td>
&lt;td>0.69&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.72&lt;/td>
&lt;td>0.28&lt;/td>
&lt;td>0.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.057 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="414-decision-trees">4.1.4 Decision Trees&lt;/h4>
&lt;p>&lt;strong>Table 4.7:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.92&lt;/td>
&lt;td>0.97&lt;/td>
&lt;td>0.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.97&lt;/td>
&lt;td>0.93&lt;/td>
&lt;td>0.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.009 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.8:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.71&lt;/td>
&lt;td>0.80&lt;/td>
&lt;td>0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>0.66&lt;/td>
&lt;td>0.71&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.272 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="415-k-means">4.1.5 K-Means&lt;/h4>
&lt;p>&lt;strong>Figure 4.1:&lt;/strong> dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davkmeans.jpg" alt="Figure 4.1">&lt;/p>
&lt;p>&lt;strong>Table 4.9:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.22&lt;/td>
&lt;td>0.29&lt;/td>
&lt;td>0.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.12&lt;/td>
&lt;td>0.09&lt;/td>
&lt;td>0.10&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.376 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 4.2:&lt;/strong> sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savkmeans.jpg" alt="Figure 4.2">&lt;/p>
&lt;p>&lt;strong>Table 4.10:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.51&lt;/td>
&lt;td>0.69&lt;/td>
&lt;td>0.59&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.52&lt;/td>
&lt;td>0.34&lt;/td>
&lt;td>0.41&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>1.429 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="416-mean-shift">4.1.6 Mean-shift&lt;/h4>
&lt;p>&lt;strong>Figure 4.3:&lt;/strong> dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davmeanshift.jpg" alt="Figure 4.3">&lt;/p>
&lt;p>&lt;strong>Table 4.11:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.47&lt;/td>
&lt;td>1.00&lt;/td>
&lt;td>0.64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.461 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 4.4:&lt;/strong> sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savmeanshift.jpg" alt="Figure 4.4">&lt;/p>
&lt;p>&lt;strong>Table 4.12:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.50&lt;/td>
&lt;td>1.00&lt;/td>
&lt;td>0.67&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>193.93 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="417-spectral-clustering">4.1.7 Spectral Clustering&lt;/h4>
&lt;p>&lt;strong>Figure 4.5:&lt;/strong> dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davspectral.jpg" alt="Figure 4.5">&lt;/p>
&lt;p>&lt;strong>Table 4.13:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.86&lt;/td>
&lt;td>0.74&lt;/td>
&lt;td>0.79&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.79&lt;/td>
&lt;td>0.89&lt;/td>
&lt;td>0.84&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.628 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 4.6:&lt;/strong> sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savspectral.jpg" alt="Figure 4.6">&lt;/p>
&lt;p>&lt;strong>Table 4.14:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.57&lt;/td>
&lt;td>0.57&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.56&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>208.822 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="42-system-information">4.2 System Information&lt;/h3>
&lt;p>Google Collab was used to train and evaluate the models selected. The specifications of the system in use is provided by the cloudmesh.common benchmark library and is listed in Table 4.15 &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Table 4.15&lt;/strong>: Training and Evaluation System Specifications&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attribute&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BUG_REPORT_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://bugs.launchpad.net/ubuntu/%22">https://bugs.launchpad.net/ubuntu/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_DESCRIPTION&lt;/td>
&lt;td>&amp;ldquo;Ubuntu 18.04.5 LTS&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_ID&lt;/td>
&lt;td>Ubuntu&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_RELEASE&lt;/td>
&lt;td>18.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HOME_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://www.ubuntu.com/%22">https://www.ubuntu.com/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ID&lt;/td>
&lt;td>ubuntu&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ID_LIKE&lt;/td>
&lt;td>debian&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NAME&lt;/td>
&lt;td>&amp;ldquo;Ubuntu&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PRETTY_NAME&lt;/td>
&lt;td>&amp;ldquo;Ubuntu 18.04.5 LTS&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PRIVACY_POLICY_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy%22">https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SUPPORT_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://help.ubuntu.com/%22">https://help.ubuntu.com/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UBUNTU_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION&lt;/td>
&lt;td>&amp;ldquo;18.04.5 LTS (Bionic Beaver)&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION_ID&lt;/td>
&lt;td>&amp;ldquo;18.04&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu_count&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.active&lt;/td>
&lt;td>698.5 MiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.available&lt;/td>
&lt;td>11.9 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.free&lt;/td>
&lt;td>9.2 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.inactive&lt;/td>
&lt;td>2.6 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.percent&lt;/td>
&lt;td>6.5 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.total&lt;/td>
&lt;td>12.7 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.used&lt;/td>
&lt;td>1.6 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>platform.version&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python&lt;/td>
&lt;td>3.6.9 (default, Oct 8 2020, 12:12:24) [GCC 8.4.0]&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.pip&lt;/td>
&lt;td>19.3.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.version&lt;/td>
&lt;td>3.6.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sys.platform&lt;/td>
&lt;td>linux&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.machine&lt;/td>
&lt;td>x86_64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.node&lt;/td>
&lt;td>bc15b46ebcf6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.processor&lt;/td>
&lt;td>x86_64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.release&lt;/td>
&lt;td>4.19.112+&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.system&lt;/td>
&lt;td>Linux&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.version&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>user&lt;/td>
&lt;td>collab&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="43-discussion">4.3 Discussion&lt;/h3>
&lt;p>In analyzing the resulting metrics in section 4.1, two major trends between the algorithms are apparent.&lt;/p>
&lt;ol>
&lt;li>The classification algorithms perform significantly better than the clustering algorithms.&lt;/li>
&lt;li>Significant signs of overfitting for the dav_set.&lt;/li>
&lt;/ol>
&lt;p>Addressing the first point, it is obvious from the metric performance where on average the classification algorithms were higher than the clustering algorithms. At a lower training time cost as well, which indicates that classification algorithms are well suited for this application than clustering. Especially when looking at the results for Mean-Shift in section 4.1.6 where the algorithm failed to identify any patient with a disease. This also illustrates the discussion on the metrics used to determine performance as the recall was 100% at the cost of missing every patient that would have required treatment illustrated by Figure 4.3 and 4.4. On this topic, comparing the actual data graphs for each of the clustering algorithms and comparing them to the example clustering figures within the scikit documentation, it solidifies that this is not the correct algorithm type for this dataset &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Moving on to the next point, it can be seen that overfitting is occurring for the dav_set in comparing the performance to the sav_set for the same algorithm which can be seen in the corresponding tables in sections 4.1.2, 4.1.3, and 4.1.4. Here the performance gap is at least 20% between the two compared to what one would assume should be relatively close to each other. While this could also illustrate the affect the various features have on the algorithm, it was determined that this is most likely due to the small dataset size having a larger influence than anticipated.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>Reviewing these results, a clear conclusion cannot be accurately be determined due to the considerable amount of variables involved that were not able to be isolated to a desirable level. Namely the compromises that were mentioned in section 2.1 and general dataset availability. However, it was determined that the main goal of this project was accomplished where the Support Vector Machine algorithm was narrowed down as a viable candidate for future work. Due in part to the overall f1-score performance for both datasets, providing confidence that overfitting may not occur. While there is a downside in scalability due to the significant increase in training time between the smaller dav_set and larger sav_set. This could indicate that further research should be focused on either improving this algorithm or creating a new one based on the underlying mechanism.&lt;/p>
&lt;p>In relation to the types of features, it could be interpreted from this project that further efforts require a more expansive and modern dataset to perform to a level suitable for real world applications. As possible factors affecting the performance are in the accuracy and granularity of the measurements and factors available to learn from. This however, is seen to be a difficult challenge due to the nature of privacy laws on health data but, as proposed in the introduction. It would be very interesting to apply this project&amp;rsquo;s findings on more general health data that is retrieved in annual visits.&lt;/p>
&lt;h2 id="6-acknowledgements">6. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their assistance and suggestions with regard to this project.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Centers for Disease Control and Prevention. 2020. Heart Disease Facts | Cdc.Gov. [online] Available at: &lt;a href="https://www.cdc.gov/heartdisease/facts.htm">https://www.cdc.gov/heartdisease/facts.htm&lt;/a> [Accessed 16 November 2020].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Amadeo, K., 2020. Preventive Care: How It Lowers Healthcare Costs In America. [online] The Balance. Available at: &lt;a href="https://www.thebalance.com/preventive-care-how-it-lowers-aca-costs-3306074">https://www.thebalance.com/preventive-care-how-it-lowers-aca-costs-3306074&lt;/a> [Accessed 16 November 2020].&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>WebMD. 2020. Understanding Your Cholesterol Report. [online] Available at: &lt;a href="https://www.webmd.com/cholesterol-management/understanding-your-cholesterol-report">https://www.webmd.com/cholesterol-management/understanding-your-cholesterol-report&lt;/a> [Accessed 21 October 2020].&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>R, V., 2020. Feature Selection â€” Correlation And P-Value. [online] Medium. Available at: &lt;a href="https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf">https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf&lt;/a> [Accessed 21 October 2020].&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Stack Overflow. 2020. Differences In Scikit Learn, Keras, Or Pytorch. [online] Available at: &lt;a href="https://stackoverflow.com/questions/54527439/differences-in-scikit-learn-keras-or-pytorch">https://stackoverflow.com/questions/54527439/differences-in-scikit-learn-keras-or-pytorch&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.4. Support Vector Machines â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/svm.html#classification">https://scikit-learn.org/stable/modules/svm.html#classification&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 2.3. Clustering â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/clustering.html#clustering">https://scikit-learn.org/stable/modules/clustering.html#clustering&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1. Supervised Learning â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning">https://scikit-learn.org/stable/supervised_learning.html#supervised-learning&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.6. Nearest Neighbors â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/neighbors.html">https://scikit-learn.org/stable/modules/neighbors.html&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.9. Naive Bayes â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/naive_bayes.html">https://scikit-learn.org/stable/modules/naive_bayes.html&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.10. Decision Trees â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/tree.html">https://scikit-learn.org/stable/modules/tree.html&lt;/a> [Accessed 27 October 2020].&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. A Demo Of K-Means Clustering On The Handwritten Digits Data â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py">https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py&lt;/a> [Accessed 17 November 2020].&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 6.3. Preprocessing Data â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing">https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing&lt;/a> [Accessed 17 November 2020].&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Mianaee, S., 2020. 20 Popular Machine Learning Metrics. Part 1: Classification &amp;amp; Regression Evaluation Metrics. [online] Medium. Available at: &lt;a href="https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce">https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce&lt;/a> [Accessed 10 November 2020].&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: NBA Performance and Injury</title><link>/report/fa20-523-301/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-301/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;ul>
&lt;li>Gavin Hemmerlein, fa20-523-301&lt;/li>
&lt;li>Chelsea Gorius, fa20-523-344&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/blob/main/project/project.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Sports Medicine will be a $7.2 billion dollar industry by 2025. The NBA has a vested interest in predicting performance of players as they return from injury. The authors evaluated datasets available to the public within the 2010 decade to build machine and deep learning models to expect results. The team utilized Gradient Based Regressor, Light GBM, and Keras Deep Learning models. The results showed that the coefficient of determination for the deep learning model was approximately 98.5%. The team recommends future work to predicting individual player performance utilizing the Keras model.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-data-transformations-and-calculations">3.1 Data Transformations and Calculations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-development-of-models">4.1 Development of Models&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#411-evaluation-metrics">4.1.1 Evaluation Metrics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-gradient-boost-regression">4.1.2 Gradient Boost Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-lightgbm-regression">4.1.2 LightGBM Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#413-keras-deep-learning-models">4.1.3 Keras Deep Learning Models&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-inference">5. Inference&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#61-limitations">6.1 Limitations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#71-work-breakdown">7.1 Work Breakdown&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> basketball, NBA, injury, performance, salary, rehabilitation, artificial intelligence, convolutional neural network, lightGBM, deep learning, gradient based regressor.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The topic to be investigated is basketball player performance as it relates to injury. The topic of injury and recovery is a multi-billion dollar industry. The Sports Medicine field is expected to reach $7.2 billion dollars by 2025 &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The scope of this effort is to explore National Basketball Association(NBA) teams, but the additional uses of a topic such as this could expand into other realms such as the National Football League, Major League Baseball, the Olympic Committees, and many other avenues. For leagues with salaries, projecting an expected return on the investment can assist in contract negotiations and cater expectations. Competing at such a high level of intensity puts these players at a greater risk to injury than the average athlete because of the intense and constant strain on their bodies. The overall valuation of the NBA in recent years is over 2 billion dollars, meaning each team is spending millions of dollars in the pursuit of a championship every season. Injuries to players can cost teams not only wins but also significant profits. Ticket sales alone for a single NBA finals game have reported greater than 10 million dollars in profit for the home team, if a team&amp;rsquo;s star player gets injured just before the playoffs and the team does not succeed, that is a lot of money lost. These injuries can have an effect no matter the time of year, regular season ticket sales have been known to fluctuate with injuries from the team&amp;rsquo;s top performers. Besides ticket sales these injuries can also influence viewership, TV or streaming, and potentially lead to a greater loss in profits. With the health of the players and so much money at stake NBA team organizations as a whole do their best to take care of their players and keep them injury free.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>The assumptions were made based on current literature as well. The injury return and limitations upon return of Anterior Cruciate Ligament (ACL) rupture (ACLR) are well documented and known. Interesting enough, forty percent of the players in the study occurred during the fourth quarter &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This leads some credence to the idea that fatigue is a major factor in the occurrence of these injuries.&lt;/p>
&lt;p>The current literature also shows that a second or third injury can occur more frequently due to minor injuries. &lt;em>&amp;ldquo;When an athlete is recovering from an injury or surgery, tissue is already compromised and thus requires far more attention despite the recovery of joint motion and strength. Moreover, injuries and surgical procedures can create detraining issues that increase the likelihood of further injury&amp;rdquo;&lt;/em> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>To compare performance and injury, a minimum of two datasets will be needed. The first is a dataset of injuries for players &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This dataset created the samples necessary for review.&lt;/p>
&lt;p>Once the controls for injuries were established, the next requirement was to establish pre-injury performance parameters and post-injury parameters. These areas were where the feature engineering took place. The datasets needed had to include appropriate basketball performance stats to establish a metric to encompass a player&amp;rsquo;s performance. One example that ESPN has tried in the past is the Player Efficiency Rating (PER). To accomplish this, it was important to review player performance within games such as in the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> dataset because of how it allowed the team to evaluate the player performance throughout the season, and not just the average stats across the year. In addition to that the data from the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> dataset was valuable in order to compare the calculated performance metrics just before an injury or after recovery to the player&amp;rsquo;s overall performance that season or in seasons prior. That comparison provided a solid baseline to understand how injuries can effect a player&amp;rsquo;s performance. With in depth information about each game of the season, and not just the teams and players aggregated stats, added to the data provided from the injury dataset &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> the team was be able to compose new metrics to understand how these injuries are actually affecting the players performance.&lt;/p>
&lt;p>Along the way attempted to discover if there is also a causal relationship to the severity of some of the injuries, based on how the player was performing just before the injury. The term &lt;em>load management&lt;/em> has become popular in recent years to describe players taking rest periodically throughout the season in order to prevent injury from overplaying. This new practice has received both support for the player safety it provides and also criticism around players taking too much time off. Of course not all injuries are entirely based on the recent strain under the players body, but a better understanding about how that affects the injury as a whole could give better insight into avoiding more injuries. It is important to remember though that any pattern identification would not lead to an elimination of all injuries, any contact sport will continue to have injuries, especially one as high impact as the NBA. There is value to learn from why some players are able to return from certain injuries more quickly and why some return to almost equivalent or better playing performance than before the injury. This comparison of performance was attempted by deriving metrics based on varying ranges of games immediately leading up to injury and then immediately after returning from injury. In addition to that performed comparisons to the players known peak performance to better understand how the injury affected them. Another factor that was important to include is the length of time recovering from the injury. Different players take differing amounts of time off, sometimes even with similar injuries. Something will be said about the playerâ€™s dedication to recovery and determination to remain at peak performance, even through injury, when looking at how severe their injury was, how much time was taken for recovery, and how they performed upon returning.&lt;/p>
&lt;p>These datasets were chosen because they allow for a review of individual game performance, for each team, throughout each season in the recent decade. Aggregate statistics such as points per game (ppg) can be deceptive because duration of the metric is such a large period of time. The large sample of 82 games can lead to a perception issue when reviewing the data. These datasets include more variables to help the team determine effects to player injury, such as minutes per game (mpg) to understand how strenuous the pre-injury performance or how fatigue may have played a factor in the injury. Understanding more of the variables such as fouls given or drawn can help determine if the player or other team seemed to be the primary aggressor before any injury.&lt;/p>
&lt;h3 id="31-data-transformations-and-calculations">3.1 Data Transformations and Calculations&lt;/h3>
&lt;p>Using the Kaggle package the datasets were downloaded direct from the website and unzipped to a directory accessible by the â€˜project_dateEngineering.ipynbâ€™ notebook. The 7 unzipped datasets are then loaded into the notebook as pandas data frames using the â€˜.read_csv()â€™ function. The data engineering performed in the notebook includes removal of excess data and data type transformations across almost all the data frames loaded. This data transformation includes transforming the games details column â€˜MINâ€™, meaning minutes played, from a timestamp format to a numerical format that could have calculations like summation or average performed on it. This was a crucial transformation since minutes played have a direct correlation to player fatigue, which can increase a playerâ€™s chance of injury.&lt;/p>
&lt;p>One of the more difficult tasks was transforming the Injury dataset into something that would provide more information through machine learning and analysis. The dataset is loaded as one data set where 2 columns â€˜Relinquishedâ€™ and â€˜Acquiredâ€™ defined if the row in questions was a player leaving the roster due to injury or returning from injury, respectively. In this case for each for one of those two columns contained a players name and the other was blank. Besides that the data frame contained information like the date, notes, and the team name. In order to appropriately understand each injury as whole the data frame needs to be transformed into one where each row contains the player, the start date of the injury, and the end date of the injury. In order to do this first the original Injury dataset was separated into rows marking the start of an injury and those marking the end of an injury. Data frames from the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> data set were used to join TeamID and PlayerID columns to the Injury datasets. An â€˜iterrows():â€™ loop was then used on the data frame marking the start of an injury to specifically locate the corresponding row in the Injury End data frame with the same PlayerID and where the return date was the closest date after the injury date. As this new data frame was being transformed, it was noted that sometimes a Player would have multiple rows with the same Injury ending date but different injury start dates, this can happen if an injury worsens or the player did not play due to last minute decision. In order to solve this the table was grouped by the PlayerID and InjuryEnd Date while keeping the oldest Injury Start date, since the model will want to see the full length of the injury. From there it was simple to calculate the difference in days for each row between the Injury start and end dates. This data frame is called â€˜df_Injury_lengthâ€™ in the notebook and is much easier to use for improved understanding of NBA injuries than the original format of the Injury data set.&lt;/p>
&lt;p>Once created, the â€˜df_Injury_lengthâ€™ data frame was copied and built upon. Using â€˜iterrows():â€™ loop again to filter down the games details data frame rows with the same PlayerId, over 60 calculated columns are created to produce the â€˜df_Injury_statsâ€™ data frame. The data frame includes performance statistics specifically from the game the player was injured and the game the player returned from that injury. In addition to this aggregate performance metrics were calculated based on the 5 games prior to the injury and the 5 games post returning from injury. At this time the season of when the injury occurred and when the player returned is also stored in the dataframe. This will allow comparisons between the â€˜df_Injury_statsâ€™ data frame and the â€˜df_Season_statsâ€™ data frame which contains the players average performance metrics for entire seasons.&lt;/p>
&lt;p>A few interesting figures were generated within the Exploratory Data Analysis (EDA) stage. &lt;strong>Figure 1&lt;/strong> gave a view of the load of the player returning from injury. The load to the player will show how recovered the player is upon completion of rehab. Many teams decide to slowly work a returning player in. Additionally, the amount of time for an injury can be seen on this graph. The longer the injury, the more unlikely the player will return to action.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/avg_min_played_post5.png" alt="Average Minutes Played in First Five Games Upon Return over Injury Length in Days">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Average Minutes Played in First Five Games Upon Return over Injury Length in Days*&lt;/p>
&lt;p>&lt;strong>Figure 2&lt;/strong> shows the frequency in which a player is injured. The idea behind this graph is to see a relationship between the time leading up to the injury. Interesting enough, there is no key indication of where injury is more likely to occur. It can be assumed that there is a rarity of players who see playing time greater than 30 minutes. The histogram only shows a near flat relationship; which was surprising.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/frequencies_by_average_minutes.png" alt="Frequency of Injuries by Average Minutes Played in Prior Five Games">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Frequency of Injuries by Average Minutes Played in Prior Five Games*&lt;/p>
&lt;p>&lt;strong>Figure 3&lt;/strong> shows the length of injury over number of injuries. By reviewing this data, it can be seen that most injuries occur fewer rather than more often. A player that is deemed injury prone will be a lot more likely to be cut from the team. This data makes sense.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injury_length.png" alt="Injury Length in Days over Number of Injuries">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Injury Length in Days over Number of Injuries&lt;/p>
&lt;p>&lt;strong>Figure 4&lt;/strong> shows the injury length over average minutes played in the five games before injury. This graph attempts to show all of the previous games and the impacts to the players injury. The data looks evenly distributed, but the majority of plaers do not play close to 40 minutes per game. By looking at this data, it shows that minutes played does likely contribute to the injury severity.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injury_length_over_avg_min.png" alt="Injury Length in Days over Avg Minutes Played in Prior 5 Games">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Injury Length in Days over Avg Minutes Played in Prior 5 Games&lt;/p>
&lt;p>&lt;strong>Figure 5&lt;/strong> shows that in general the number of games played does not have a significant relationship to the length of the injury. There is a darker cluster between 500-1000 days injured that exists over the 40-82 games played, this could suggest that as more games are played there is likeliness for more severe injury.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_gamesplayed.png" alt="Injury Length in Days over Player Games Played that Season">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Injury Length in Days over Player Games Played that Season&lt;/p>
&lt;p>&lt;strong>Figures 6&lt;/strong>, &lt;strong>Figure 7&lt;/strong>, and &lt;strong>Figure 8&lt;/strong> attempt to demonstrate if any relationship exists visually between a player&amp;rsquo;s injury length and their age, weight, or height. For the most part &lt;strong>Figure 6&lt;/strong> shows most severe injuries occurring to younger players, which could make sense considering they can perform more difficult moves or have more stamina than older players. Some severe injuries still exist among the older players, this also makes sense considering their bodies have been under stress for many years and are more prone to injury. It should be noted that there are more players in the league that fall into the younger age bucket than the older ages. It is difficult to identify any pattern on &lt;strong>Figure 7&lt;/strong>. If anything the graph is somewhat normally shaped similar to the heights of players across the league. Suprisingly the injuries on &lt;strong>Figure 8&lt;/strong> are clustered a bit towards the left, being the lighter players. This could be explained through the fact that the lighter players are often more athletic and perform more strenuous moves than heavier players. It is also somewhat surprising since the argument that heavier players are putting more strain on their bodies could be used as a reason why heavier players would have worse injuries. One possible explanation could be the musculature adding more of the dense body mass could add protection to weakened joints. More investigation would be needed to identify an exact reason.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerage.png" alt="Injury Length in Days over Player Age that Season">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Injury Length in Days over Player Age that Season&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerHeight.png" alt="Injury Length in Days over Player Height in Inches">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Injury Length in Days over Player Height in Inches&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerWeight.png" alt="Injury Length in Days over Player Weight in Kilograms">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Injury Length in Days over Player Weight in Kilograms&lt;/p>
&lt;p>Finally, the team decided to use the z-score to normalize all of the data. By using the Z-score from the individual data in a column of df_Injury_stats, the team was able to limit variability of multiple metrics across the dataframe. A player&amp;rsquo;s blocks and steals should be a miniscule amount compared to minutes or points of some players. The same can be said of assists, technical fouls, or any other statistic in the course of an NBA game. The Z-score, by nature of the metric from the mean, allows for much less variability across the columns.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;p>The objective of this project was to develop performance indicators for injured players returning to basketball in the NBA. It is unreasonable to expect a player to return to the same level of play post injury immediately upon starting back up after recovery. It often takes a player months if not years to return to the same level of play as pre-injury, especially considering the severity of the injuries. In order to successfully analyze this information from the datasets, a predictive model will need to be created using a large set of the data to train.&lt;/p>
&lt;p>From this point, a test run was used to gauge the validity and accuracy of the model compared to some of the data set aside. The model created was able to provide feature importance to give a better understanding of which specific features are the most crucial when it comes to determining how bad the effects of an injury may or may not be on player performance. Feature engineering was performed prior to training the model in order to improve the chances of higher accuracy from the predictions. This model could be used to keep an eye out for how a player&amp;rsquo;s performance intensity and the engineered features could affect how long a player takes to recover from injury, if there are any warning signs prior to an injury, and even how well they perform when returning.&lt;/p>
&lt;h3 id="41-development-of-models">4.1 Development of Models&lt;/h3>
&lt;p>To help with review of the data, conditioned data was used to save resources on Google Colab. By conditioning the data and saving the files as a .CSV, the team was able to create a streamlined process. Additionally, the team found benefit by uploading these files to Google Drive to quickly import data near real time. After operating in this fashion for some time, the team was able to load the datasets into Github and utilize that feature. By loading the datasets up to Github, a url could be used to link the files directly to the files saved on Github without using a token like with Kaggle or Google Drive. The files saved were the following:&lt;/p>
&lt;p>&lt;strong>Table 1:&lt;/strong> Datasets Imported&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Dataframe&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Title&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">1.&lt;/td>
&lt;td style="text-align:center">df_Injury_stats&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">2.&lt;/td>
&lt;td style="text-align:center">df_Injury_length&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">3.&lt;/td>
&lt;td style="text-align:center">df_Season_stats&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">4.&lt;/td>
&lt;td style="text-align:center">games&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">5.&lt;/td>
&lt;td style="text-align:center">df_Games_gamesDetails&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">6.&lt;/td>
&lt;td style="text-align:center">injuries_2010-2018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">7.&lt;/td>
&lt;td style="text-align:center">players&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">8.&lt;/td>
&lt;td style="text-align:center">ranking&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">9.&lt;/td>
&lt;td style="text-align:center">teams&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Every time Google Colab loads data, it takes time and resources. The team was able to utilize the cross platform connectivity of the Google utilities. The team could then focus on building models as opposed to conditioning data every time the code was ran.&lt;/p>
&lt;h4 id="411-evaluation-metrics">4.1.1 Evaluation Metrics&lt;/h4>
&lt;p>The metrics chosen were designed to give results on Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and the Explained Variance (EV) Score. MAE is a measure of errors between paired observations experiencing the same expression. RMSE is the standard deviation of the prediction errors for our dataset. EV is the relationship between the train data and the test data. By using these metrics, the team is capable of reviewing the data in a statistical manner.&lt;/p>
&lt;h4 id="412-gradient-boost-regression">4.1.2 Gradient Boost Regression&lt;/h4>
&lt;p>The initial model that was used was a Gradient Boosting Regressor (GBR) model. This model produced the results shown in Table 2. The GBR model builds in a stage-wise fashion; similarly to other boosting methods. GBR also generalizes the data and attempts to optimize the results utilizing a loss function. An example of the algorithm can be seen in &lt;strong>Figure 5&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/gbr.png" alt="Gradient Boosting Regressor">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Gradient Boosting Regressor &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The team saw a relationship given the data. &lt;strong>Table 2&lt;/strong> shows the results of that model. The results were promising given the speed and utility of a GBR model. The team reviewed the data multiple times after multiple stages of conditioning the data.&lt;/p>
&lt;p>&lt;strong>Table 2:&lt;/strong> GBR Results&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Category&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Value&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MAE Mean&lt;/td>
&lt;td style="text-align:center">-10.787&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MAE STD&lt;/td>
&lt;td style="text-align:center">0.687&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE Mean&lt;/td>
&lt;td style="text-align:center">-115.929&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE STD&lt;/td>
&lt;td style="text-align:center">96.64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV Mean&lt;/td>
&lt;td style="text-align:center">1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV STD&lt;/td>
&lt;td style="text-align:center">0.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>After running a GBR model, the decision was made to try multiple models to see what gives the best results. The team settled on LightGBM and a Deep Learning model utilizing Keras built on the TensorFlow platform. These results will be seen in &lt;em>4.1.2&lt;/em> and &lt;em>4.1.3&lt;/em>.&lt;/p>
&lt;h4 id="412-lightgbm-regression">4.1.2 LightGBM Regression&lt;/h4>
&lt;p>Another algorithm chosen was a Light Gradient Boost Machine (LightGBM) model. LightGBM is known for its lightweight and resource sparse abilities. The model is built from decision tree algorithms and used for ranking, classification, and other machine learning tasks. By choosing LightGBM data scientists are able to analyze larger data a faster approach. LightGBM can often over fit a model if the data is too small, but fortunately for the purpose of this assignment the data available for NBA injuries and stats is extremely large. Availability of data allowed for smooth operation of the LightGBM model. Mandot explains the model really well in The Medium. Mandot said, &lt;em>&amp;ldquo;Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development&amp;rdquo;&lt;/em> &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. There are a lot of benefits available to this algorithm.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/lightGBM_regressor.png" alt="LightGBM Algorithm: Leafwise searching">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> LightGBM Algorithm: Leafwise searching &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>&lt;/p>
&lt;p>When running the model &lt;strong>Table 3&lt;/strong> was generated. This table uses the same metrics as the GBR Results Table (&lt;strong>Table 2&lt;/strong>). After reviewing the results, the GBR model still appeared to be a viable avenue. The Keras model will be evaluated next to see most optimal model to use for repeatable fresults.&lt;/p>
&lt;p>&lt;strong>Table 3:&lt;/strong> LightGBM Results&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Category&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Value&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MAE Mean&lt;/td>
&lt;td style="text-align:center">-0.011&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MAE STD&lt;/td>
&lt;td style="text-align:center">0.001&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE Mean&lt;/td>
&lt;td style="text-align:center">-0.128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE STD&lt;/td>
&lt;td style="text-align:center">0.046&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV Mean&lt;/td>
&lt;td style="text-align:center">0.982&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV STD&lt;/td>
&lt;td style="text-align:center">0.013&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="413-keras-deep-learning-models">4.1.3 Keras Deep Learning Models&lt;/h4>
&lt;p>The final model attempted was a Deep Learning model. A few runs of different layers and epochs were chosen. They can be seen in &lt;strong>Table 4&lt;/strong> (shown later). The model was sequentially ran through the test layers to refine the model. When this is done, each predecessor layer acts as an input to the next layer&amp;rsquo;s input for the model. The results can produce accurate results while using unsupervised learning. The visualization for this model can be seen in the following figure:&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/simple_neural_network_vs_deep_learning.jpg" alt="Neural Network">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Neural Network &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>&lt;/p>
&lt;p>When the team ran the Neural Networks, the data went through three layers. Each layer was built upon the previous similarly to the figure. This allowed for the team to capture information from the processing. &lt;strong>Table 4&lt;/strong> shows the results for the deep learning model.&lt;/p>
&lt;p>&lt;strong>Table 4:&lt;/strong> Epochs and Batch Sizes Chosen&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Number&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Regressor Epoch&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Regressor Batch Sizes&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>KFolds&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Model Epochs&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>R2&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">&lt;em>1.&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>25&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>25&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>10&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>10&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>0.985&lt;/em>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">2.&lt;/td>
&lt;td style="text-align:center">40&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.894&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">3.&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.966&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">4.&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.707&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">5.&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">0.611&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">6.&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">0.982&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The team has decided that the results for the Deep Learning are the most desirable. This model would be the one that the team would recommend based on the results from the metrics available. The parameters the team recommends are italicized in &lt;em>Line 1&lt;/em> of &lt;strong>Table 4&lt;/strong>.&lt;/p>
&lt;h2 id="5-inference">5. Inference&lt;/h2>
&lt;p>With the data available, some conclusions can be made. Not all injuries are of the same severity. By treating an ACL tear in the same manner as a bruise, the team doctors would take terrible approaches to rehab. The severity of the injury is a part of the approach to therapy. This detail is nearly impossible to capture in the model.&lt;/p>
&lt;p>Another aspect to come to a conclusion is that not every player recovers in the same timetable as another. Genetics, diet, effort, and mental health can all harm or reinforce the efforts from the medical staff. These areas are hard to capture in the data and cannot be appropriately reviewed with this model.&lt;/p>
&lt;p>It is also difficult to indicate where a previous injury may have contributed to a current injury. The kinetic chain is a structure of the musculoskeletal system that moves the body using the muscles and bones. If one portion of the chain is compromised, the entire chain will need to be modified to continue movement. This modification can result in more injuries. The data cannot provide this information. It is important to remember these possible confounding variables when interpreting the results of the model.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>After reviewing the results, the team created a robust model to predict the performance of a player after an injury. The coefficient of determination for the deep learning model shows a strong relationship between the training and test sets. After conditioning the data, the results can be seen in &lt;strong>Table 2&lt;/strong>, &lt;strong>Table 3&lt;/strong>, and &lt;strong>Table 5&lt;/strong>. The team had an objective to find this correlation and build it to the point where injury and performance can be modeled. The team was able to accomplish this goal.&lt;/p>
&lt;p>Additionally, these results are consistent with the current scientific literature &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The biological community has been able to record these results for decades. By leveraging this effort, the scientific community could move to a more proactive approach as opposed to reactive with respect to injury controls. This data will also allow for proper contract negotiations to take place in the NBA, considering potential decisions to avoid injury may include less playing time. The negotiations are pivotal to ensuring that expectations are met in the future seasons; especially when injury occurs in the final year of a player&amp;rsquo;s contract. Teams with an improved understanding of how players can or will return from injury have an opportunity to make the best of scenarios where other teams may be hesitant to sign an injured player. These different opportunities for a team&amp;rsquo;s front office could be the difference between a championship ring and missing the playoffs entirely.&lt;/p>
&lt;h2 id="61-limitations">6.1 Limitations&lt;/h2>
&lt;p>With respect to the current work, the models could be continued to be refined. Currently the results are to the original intentions of the team, but improvements can be made. Feature Engineering is always an area where the models can improve. Some valuable features to be created in the future are the calculations for the player&amp;rsquo;s efficiency overall, as well as offensinve and defensive efficiencies in each game. The team would also like to develop a model to use the stats of a player in pre-injury and apply that to the post-injury set of metrics. Also, the team would like to move to where the same could be applied given the length of the injury to the player while considering the severity of the injury. Longer and more severe injury will lead to different future results than say a long not severe injury, or a short injury that was somewhat severe. The number of varaibles that could provide more valuable information to the model are endless.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The authors would like to thank Dr. Gregor von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article. In addition to that the community of students from the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course also deserve a thanks from the author for the support, continued engagement, and valuable discussions through Piazza.&lt;/p>
&lt;h3 id="71-work-breakdown">7.1 Work Breakdown&lt;/h3>
&lt;p>For the effort developed, the team split tasks between each other to cover more ground. The requirements for the investigation required a more extensive effort for the teams in the ENGR-E 534 class. To accomplish the requirements, the task was expanded by addressing multiple datasets within the semester and building in multiple models to display the results. The team members were responsible for committing in Github multiple times throughout the semester. The tasks were divided as follows:&lt;/p>
&lt;ol>
&lt;li>Chelsea Gorius
&lt;ul>
&lt;li>Exploratory Data Analysis&lt;/li>
&lt;li>Feature Engineering&lt;/li>
&lt;li>Keras Deep Learning Model&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Gavin Hemmerlein
&lt;ul>
&lt;li>Organization of Items&lt;/li>
&lt;li>Model Development&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Both
&lt;ul>
&lt;li>Report&lt;/li>
&lt;li>All Outstanding Items&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>A. Mehra, &lt;em>Sports Medicine Market worth $7.2 billion by 2025&lt;/em>, [online] Markets and Markets.
&lt;a href="https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp">https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&lt;/a> [Accessed Oct. 15, 2020].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>J. Harris, B. Erickson, B. Bach Jr, G. Abrams, G. Cvetanovich, B. Forsythe, F. McCormick, A. Gupta, B. Cole,
&lt;em>Return-to-Sport and Performance After Anterior Cruciate Ligament Reconstruction in National Basketball Association Players&lt;/em>, Sports Health. 2013 Nov;5(6):562-8. doi: 10.1177/1941738113495788. [Online serial]. Available: &lt;a href="https://pubmed.ncbi.nlm.nih.gov/24427434">https://pubmed.ncbi.nlm.nih.gov/24427434&lt;/a> [Accessed Oct. 24, 2020].&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>W. Kraemer, C. Denegar, and S. Flanagan, &lt;em>Recovery From Injury in Sport: Considerations in the Transition From Medical Care to Performance Care&lt;/em>, Sports Health.
2009 Sep; 1(5): 392â€“395.[Online serial]. Available: &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445177">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445177&lt;/a> [Accessed Oct. 24, 2020].&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>R. Hopkins, &lt;em>NBA Injuries from 2010-2020&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/ghopkins/nba-injuries-2010-2018">https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&lt;/a> [Accessed Oct. 9, 2020].&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>N. Lauga, &lt;em>NBA games data&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv">https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&lt;/a> [Accessed Oct. 9, 2020].&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>J. Cirtautas, &lt;em>NBA Players&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/justinas/nba-players-data">https://www.kaggle.com/justinas/nba-players-data&lt;/a> [Accessed Oct. 9, 2020].&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>V. Aliyev, &lt;em>A hands-on explanation of Gradient Boosting Regression&lt;/em>, [online] Medium. &lt;a href="https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e">https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e&lt;/a> [Accessed Nov., 9 2020].&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>P. Mandon, &lt;em>What is LightGBM, How to implement it? How to fine tune the parameters?&lt;/em>, [online] Medium. &lt;a href="https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc">https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc&lt;/a> [Accessed Nov., 9 2020].&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>The Data Scientist, &lt;em>What deep learning is and isnâ€™t&lt;/em>, [online] The Data Scientist. &lt;a href="https://thedatascientist.com/what-deep-learning-is-and-isnt">https://thedatascientist.com/what-deep-learning-is-and-isnt&lt;/a> [Accessed Nov., 9 2020].&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: How Big Data Can Eliminate Racial Bias and Structural Discrimination</title><link>/report/fa20-523-304/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-304/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-304/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-304/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-304/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-304/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;ul>
&lt;li>Robert Neubauer, fa20-523-304&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-304/blob/main/report/report.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Healthcare is utilizing Big Data to to assist in creating systems that can be used to detect health risks, implement preventative care, and provide an overall better experience for patients. However, there are fundmental issues that exist in the creation and implementation of these systems. Medical algorithms and efforts in precision medicine often neglect the structural inequalities that already exist for minorities accessing healthcare and therefore perpetuate bias in the healthcare industry. The author examines current applications of these concepts, how they are affecting minority communities in the United States, and discusses improvements in order to achieve more equitable care in the industry.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-bias-in-medical-algorithms">2. Bias in Medical Algorithms&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-disparities-found-with-data-dashboards">3. Disparities Found with Data Dashboards&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-effect-of-precision-medicine-and-predictive-care">4. Effect of Precision Medicine and Predictive Care&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-precision-public-health">4.1 Precision Public Health&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-telehealth-and-telemedicine-applications">5. Telehealth and Telemedicine Applications&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-limitations-of-teleheath-and-telemedicine">5.1 Limitations of Teleheath and Telemedicine&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> healthcare, machine learning, data science, racial bias, precision medicine, coronavirus, big data, telehealth, telemedicine, public health.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big Data is helping to reshape healthcare through major advancements in telehealth and precision medicine. Due to the swift increase in telehealth services due to the COVID-19 pandemic, researchers at the University of California San Francisco have found that black and hispanic patients use these services less frequently than white patients. Prior to the pandemic, research showed that racial and ethnic minorities were disadvantaged by the digital divide &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. These differences were attributed to disparities in access to technology and digital literacy &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Studies like these highlight how racial bias in healthcare is getting detected more frequently; However, there are few attempts to eradicate it through the use of similar technology. This has implications in various areas of healthcare including major healthcare algorithms, telehealth, precision medicine, and overall care provision.&lt;/p>
&lt;p>From the 1985 &lt;em>Report of the Secretaryâ€™s Task Force on Black and Minority Health&lt;/em>, &amp;lsquo;Blacks, Hispanics, Native Americans and those of Asian/Pacific Islander heritage have not benefited fully or equitably from the fruits of science or from those systems responsible for translating and using health sciences technology&amp;rsquo; &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The utilization of big data in industries largely acts to automate a process that was carried out by a human. This makes the process quicker to accomplish and the outcomes more precise since human error can now be eliminated. However, whenever people create the algorithms that are implemented, it is common that these algorithms will align with the biases of the human, or system, that created it. An area where this is happening that is especially alarming is the healthcare industry. Structural discrimination has long caused discrepencies in healthcare between white patients and minority patients and, with the introduction of big data to determine who should receive certain kinds of care, the issue has not been resolved but automated. Studies have shown that minority groups that are often at higher risk than white patients receive less preventative care while spending almost equal amounts on healthcare &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. National data also indicates that racial and ethnic minorities also have poorer health outcomes from preventable and treatable diseases such as cardiovascular disease, cancer, asthma, and HIV/AIDS than those in the majority &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="2-bias-in-medical-algorithms">2. Bias in Medical Algorithms&lt;/h2>
&lt;p>In a research article published to &lt;em>Science&lt;/em> in October of 2019, the researchers uncovered that one of the most used algorithms in healthcare, widely adopted by non- and for-profit medical centers and government agencies, less frequently identified black patients for preventative care than white patients. This algorithm is estimated to be applied to around 200 million people in the United States every year in order to target patients for high-risk care management. These programs seek to improve the care of patients with complex health needs by providing additional resources. The dataset used in the study contained the algorithms predictions, the underlying ingredients that formed the algorithm, and rich data outcomes which allowed for the ability to quantify racial disparities and isolate the mechanisms by which they arise. The sample consisted of 6,079 self-identified black patients and 43,539 self-identified white patients where 71.2% of all patients were enrolled in commercial insurance and 28.8% were on Medicare. On average, the patient age was 50.9 years old and 63% of patients were female. The patients enrolled in the study were classified among risk percentiles, where patients with scores at or above the 97th percentile were auto-enrolled and patients with scores over the 55th percentile were encouraged to enroll &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In order to measure health outcomes, they linked predictions to a wide range of outcomes in electronic health records, which included all diagnoses, and key quantitative laboratory studies and vital signs that captured the severity of chronic illnesses. When focusing on a point in the very-high-risk group, which would be patients in the 97th percentile, they were able to quantify the differences between white and black patients, where black patients had 26.3% more chronic illnesses than white patients&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. To get a corrected health outcome measurement among white and black patients, the researchers set a specific risk threshold for health outcomes among all patients, and repeated the procedure to replace healthier white patients with sicker black patients. So, for a white patient with a health risk score above the threshold, their data was replaced with a black patient whose score fell below the threshold and this continued until the health risk scores for black and white patients were equal and the predictive gap between patients would be eliminated. The health scores were based on the number of chronic medical conditions. The researchers then compared the data from their corrected algorithm and the original and found that the fraction of black patients at all risk thresholds above the 50th percentile increased when using the corrected algorithm. At the 97th percentile, the fraction of black patients increased to 46.5% from the original 17.7% &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Black patients are likely to have more severe hypertension, diabetes, renal failure, and anemia, and higher cholesterol. Using data from clinical trials and longitudinal studies, the researchers found that for mortality rates with hypertension and diabetes black patients had a 7.6% and 30% increase, respectively&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In the original and corrected algorithms, black and white patients spent roughly the same amount on healthcare. However, black patients spent more on emergency care and dialysis while white patients spent more on inpatient surgery and outpatient specialist care&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. In a study that tracked black patients with a black versus a white primary care provider, it found the occurrence of a black primary care provider recommending preventative care was significantly higher than recommendations from a white primary care provider. This conclusion sheds additional light on the disparities black patients face in the healthcare system and further adds to the lack of trust black people have in the healthcare system that has been heavily documented since the Tuskegee study &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. The change recommended by the researchers that would correct the gap in the predictive care model was rather simple, shifting from predictions from purely future cost to an index that combined future cost prediction with health prediction. The researchers were able to work with the distributor of the original algorithm in order to make a more equitable algorithm. Since the original and corrected models from the study were both equal in cost but varied significantly in health predictions, they reworked the cost prediction based on health predictions, conditional on the risk factor percentiles. Both of the models excluded race from the predictions, but the algorithm created with the researchers saw an 84% reduction in bias among black patients, reducing the number of excess active chronic conditions in black patients to 7,758.&lt;/p>
&lt;h2 id="3-disparities-found-with-data-dashboards">3. Disparities Found with Data Dashboards&lt;/h2>
&lt;p>To relate this to a present health issue that is affecting everyone, more black patients are dying from the novel coronavirus than white patients. In the United States, in counties where more than 86% of residents are black, the COVID-19 death rates were 10 times higher than the national average &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Considering how medical algorithms allocate resources to black patients, similar trends are expected for minorities, people who speak languages other than english, low-income residents, and people without insurance. At Brigham Health, a member of the not-for-profit Mass General Brigham health system, Karthik Sivashanker, Tam Duong, Shauna Ford, Cheryl Clark, and Sunil Eappen created data dashboards in order to assist staff and those in positions of leadership. The dashboards included rates of those who tested positive for COVID-19 sorted into different subgroups based on race, ethnicity, language, sex, insurance status, geographic location, health-care worker status, inpatient and ICU census, deaths, and discharges &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Through the use of these dashboards, the COVID-19 equity committee were able to identify emerging risks to incident command leaders, including the discovery that non-English speaking Hispanic patients had higher mortality rates when compared to English speaking Hispanic patients. This led to quality-improvement efforts to increase patient access to language interpreters. While attempting to implement these changes, it was discovered that efforts to reduce clinicians entering patient rooms to maintain social distancing guidelines was impacting the ability for interpreters to join at a patient&amp;rsquo;s bedside during clinician rounding. The incident command leadership expanded their virtual translation services by purchasing additional iPads to allow interpreters and patients to communicate through online software. The use of the geographic filter, when combined with a visual map of infection-rates by neighborhood, showed that people who lived in historically segregated and red-lined neighborhoods were tested less frequently but tested positive more frequently than those from affluent white neighborhoods &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. In a study conducted with survey data from the Pew Research Center on U.S. adults with internet access, black people were significantly more likely to report using telehealth services. In the same study, black and latino respondents had higher odds of using telehealth to report symptoms &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>However, COVID-19 is not the only disease that
researchers have found to be higher in historically segregated communities. In 1999, Laumann and Youm found that disparities segregation in social and sexual networks explained racial disparities in STDs which, they suggested, could also explain the disparities black people face in the spread of other diseases &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Prior to 1999 researchers believed that some unexplained characteristic of black people described the spread of such diseases, which shows the pervasiveness of racism in healthcare and academia. Residential segregation may influence health by concentrating poverty, environmental pollutants, infectious agents, and other adverse conditions. In 2006, Morello-Frosch and Jesdale found that segregation increased the risk of cancer related to air pollution &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Big Data can assess national and local public health for disease prevention. An example is how the National Health Interview Survey is being used to estimate insurance coverage in different areas of the U.S. population and clinical data is being used to measure access and quality-related outcomes. Community-level data can be linked with health care system data using visualization and network analysis techniques which would enable public health officials and clinicians to effectively allocate resources and assess whether all patients are getting the medical services they need &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. This would drastically improve the health of historically segregated and red-lined communities who are already seeing disparities during the COVID-19 pandemic.&lt;/p>
&lt;h2 id="4-effect-of-precision-medicine-and-predictive-care">4. Effect of Precision Medicine and Predictive Care&lt;/h2>
&lt;p>Public health experts established that the most important determinant of health throughout a personâ€™s course of life is the environment where they live, learn, work, and play. There exists a discrepancy between electronic health record systems in well-resourced clinical practices and smaller clinical sites, leading to disparities in how they are able to support population health management. For Big Data technology, if patient, family, and community focus were implemented equally in both settings, it has shown that the social determinants of health information would both improve public health among minority communities and minimize the disparities that would arise. Geographic information systems are one way to locate social determinants of health. These help focus public health interventions on populations at greater risk of health disparities. Duke University used this type of system to visualize the distribution of individuals with diabetes across Durham County, NC in order to explore the gaps in access to care and self-management resources. This allowed them to identify areas of need and understand where to direct resources. A novel approach to identify place-based disparities in chronic diseases was used by Young, Rivers, and Lewis where they analyzed over 500 million tweets and found a significant association between the geographic location of HIV-related tweets and HIV prevalence, a disease which is known to predominantly affect the black community &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>One of the ways researchers call for strengthening the health of the nation is through community-level engagement. This is often ignored when it comes to precision medicine, which is one of the latest ways that big data is influencing healthcare. It has the potential to benefit racial and ethnic minority populations since there is a lack of clinical trial data with adequate numbers of minority populations. It is because of this lack of clinical data that predictions in precision medicine are often made off risks associated with the majority which give preferential treatment to those in the majority while ignoring the risks of minority groups, further widening the gap in the allocation of preventative health resources. These predictive algorithms are rooted in cost/benefit tradeoffs, which were proven to limit resources to black patients from the science magazine article on medical algorithms &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. For the 13th Annual Texas Conference on Health Disparities, the overall theme was &amp;ldquo;Diversity in the Era of Precision Medicine.&amp;rdquo; Researchers at the event said diversity should be kept at the forefront when designing and implementing the study in order to increase participation by minority groups &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Building a trusting relationship with the community is also necessary for increased participation, therefore the institution responsible for recruitment needs to be perceived as trustworthy by the community. Some barriers for participation shared among minority groups are hidden cost of participation, concern about misuse of research data, lack of understanding the consent form and research materials, language barrier, low perceived risk of disease, and fear of discrimination &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. As discussed previously, overall lack of distrust in the research process is rooted in the fact that research involving minority groups often overwhelmingly benefits the majority by comparison. Due to the lack of representation of minority communities, big clinical data can be generated for the means of conducting pragmatic trials with underserved populations and distribute the lack of benefits &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="41-precision-public-health">4.1 Precision Public Health&lt;/h3>
&lt;p>The benefit of the majority highlights the issue that one prevention strategy does not account for everyone. This is the motivation behind combining precision medicine and public health to create precision public health. The goal of this is to target populations that would benefit most from an intervention as well as identify which populations the intervention would not be suitable for. Machine learning applied to clinical data has been used to predict acute care use and cost of treatment for asthmatic patients and diagnose diabetes, both of which are known to affect black people at greater rates than white patients &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. This takes into account the aforementioned factors that contribute to a personâ€™s health and combines it with genomic data. Useful information about diseases at the population level are attributed to advancements in genetic epidemiology, through increased genetic and genomic testing. Integration of genomic technologies with public health initiatives have already shown success in preventing diabetes and cancers for certain groups, both of which affect black patients at greater rates than white patients. Specifically, black men have the highest incidence and mortality rates of prostate cancer. The presence of Kaiso, a transcriptional repressors present in human genes, is abundant in those with prostate cancer and, in black populations, it has been shown to increase cancer aggressive and reduce survival rates &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. The greatest challenge affecting advancements made to precision public health is the involvement of all subpopulations required to get effective results. This demonstrates another area where thereâ€™s a need for the healthcare industry to prioritize building a stronger relationship with minority communities in order to assist in advancing healthcare.&lt;/p>
&lt;p>Building a stronger relationship with patients begins with having an understanding of the patientâ€™s needs and their backgrounds, requiring multicultural understanding on the physicians side. This can be facilitated by the technological advances in healthcare. Researchers from Johns Hopkins University lay out three strategic approaches to improve multicultural communications. The first is providing direct services to minimize the gap in language barriers through the use of interpreters and increased linguistic competency in health education materials. The second is the incorporation of cultural homophily in care through staff who share a cultural background, inclusion of holistic medical suggestions, and the use of community health workers. Lastly, they highlight the need for more institutional accommodation such as increasing the ability of professionals to interact effectively within the culture of the patient population, more flexible hours of operation, and clinic locations &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. These strategic approaches are much easier to incorporate into practice when used in telehealth monitoring, providing more equitable care to minority patients who are able to use these services. There are three main sections of telehealth monitoring which include synchronous, asynchronous, and remote monitoring. Synchronous would be any real-time interaction, whether it be over the telephone or through audio/visual communication via a tablet or smartphone. This could occur when the patient is at their home or they are present with a healthcare professional while consulting with a medical provider virtually. Asynchronous communication occurs when patients communicate with their provider through a secure messaging platform in their patient portal. Remote patient monitoring is the direct transmission of a patientâ€™s clinical measurements to their healthcare provider. Remote access to healthcare would be the most beneficial to those who are medically and socially vulnerable or those without ready access to providers and could also help preserve the patient-provider relationship &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Connecting a patient to a provider that is from a similar cultural or ethnic background becomes easier through a virtual consultation, a form of synchronous telehealth monitoring. A virtual consultation would also help eliminate the need for transportation and open up the flexibility of meeting times for both the patient and the provider. From this, a way to increase minority patient satisfaction in regards to healthcare during the shift to telehealth services due to COVID-19 restrictions would be a push to increase technology access to these groups by providing them with low-cost technology with remote-monitoring capabilities.&lt;/p>
&lt;h2 id="5-telehealth-and-telemedicine-applications">5. Telehealth and Telemedicine Applications&lt;/h2>
&lt;p>Telehealth monitoring is evolving the patient-provider relationship by extending care beyond the in-person clinical visit. This provides an excellent opportunity to build a more trusting and personal relationship with the patient, which would be critical for minority patients as it would likely increase their trust in the healthcare system. Also, with an increase in transparency and involvement with their healthcare, the patient will be more engaged in the management of their healthcare which will likely have more satisfactory outcomes. Implementing these types of services will create large amounts of new data for patients, requiring big data applications in order to manage it. Similar to the issue of inequality in the common medical algorithm for determination of preventative care, if the data collected from minority groups using this method is not accounted for properly, then the issue of structural discrimination will continue. The data used in healthcare decision-making often comes from a patientâ€™s electronic health record. An issue that presents itself when considering the use of a patientâ€™s electronic health record in the process of using big data to assist with the patientâ€™s healthcare is missing data. In the scope of telehealth monitoring, since the visit and most of the patient monitoring would be done virtually, the electronic health record would need to be updated virtually as well &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>For telehealth to be viable, the tools that accommodate it need to work seamlessly and be supported by the data streams that are integrated into the electronic health record. Most electronic health record systems are unable to be populated with remote self-monitoring patient-generated data &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. However, the American Telemedicine Association is advocating for remotely-monitored patient-generated data to be incorporated into electronic health records. The SMART Health IT platform is an approach that would allow clinical apps to run across health systems and integrate with electronic health records through the use of a standards-based open-source application programming interface (API) Fast Healthcare Interoperability Resources (FHIR). There are also advancements being made in technology that is capable of integrating data from electronic health records with claims, laboratory, imaging, and pharmacy data &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. There is also a push to include social determinants of health disparities including genomics and socioeconomic status in order to further research underlying causes of health disparities &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="51-limitations-of-teleheath-and-telemedicine">5.1 Limitations of Teleheath and Telemedicine&lt;/h3>
&lt;p>The issue of lack of access to the internet and devices that would be necessary for virtual health visits would limit the participation of those from lower socioeconomic backgrounds. From this arises the issue of representativeness in remotely-monitored studies where the participant must have access to a smartphone or tablet. However, much like the Brigham Health group providing iPads in order to assist with language interpretation, there should be an incentive to provide access to these devices for patients in high risk groups in order to boost trust and representation in this type of care. From the article that discussed the survey results that found black and latino patients to be more responsive to using telehealth, the researchers contrasted the findings with another study where 52,000 Mount Sinai patients were monitored between March and May of 2020 that found black patients were less likely to use telehealth than white patients &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. One reason for the discrepancy the researchers introduce is that the Pew survey, while including data from across the country, only focused on adults that had internet access. This brings up the need for expanding broadband access, which is backed by many telehealth experts &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The process of providing internet access and devices with internet capabilities to those without them should be similar to that from the science magazine study where patients whose risk scores are above a certain threshold should automatically qualify for technological assistance. Programs such as the Telehealth Network Grant Program would be beneficial for researchers conducting studies with a similar focus, as the grant emphasizes advancements in tele-behavioral health and tele-emergency medical services and providing access to these services to those who live in rural areas. Patients from rural areas are less likely to have access to technology that would enable them to participate in a study requiring remote monitoring. The grant proposal defines tele-emergency as an electronic, two-way, audio/visual communication service between a central emergency healthcare center, the tele-emergency hub, and a remote hospital emergency department designed to provide real-time emergency care consultation &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. This is especially important when considering that major medical algorithms show that black patients often spend more on emergency medical care.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>Big Data is changing many areas of healthcare and all of the areas that itâ€™s affecting can benefit from making structural changes in order to allow minorities to get equitable healthcare. This includes how the applications are put into place, since Big Data has the ability to demonstrate bias and reinforce structural discrimination in care. It should be commonplace to consider race or ethnicity, socioeconomic status, and other relevant social determinants of health in order to account for this. Several studies have displayed the need for different allocations of resources based on race and ethnicity. From the findings that black patients were often given more equitable treatment when matched with a primary care provider that was black and that COVID-19 has limited in-person resources, such as a bedside interpreter for non-English speaking patients, there should be a development of a resource that allows people to be matched with a primary care provider that aligns with their identity and to connect with them virtually. When considering the lack of trust black people and other minority populations have in the healthcare system, there are a variety of services that would help boost trust in the process of getting proper care. Given the circumstances surrounding COVID-19 pandemic, there is already an emphasis on making improvements within telehealth monitoring as barriers to telehealth have been significantly reduced. Several machine-learning based studies have highlighted the importance of geographic locationâ€™s impact on aspects of the social determinants of health, including the effects in segregated communities. Recent work has shown that black and other ethnic minority patients report having less involvement in medical decisions and lower levels of satisfaction of care. This should motivate researchers who are focused on improving big data applications in the healthcare sector to focus on these communities in order to eliminate disparities in care and increase the amount of minority healthcare workers in order to have accurate representation. From the survey data showing that minority populations were more likely to use telehealth services, there needs to be an effort to highlight these communities in future work surrounding telehealth and telemedicine. Several studies have prepared a foundation for what needs to be improved and have already paved the way for additional research. With the progress that these studies have made and continued reports of inadequacies in care, it is only a matter of time before substantial change is implemented and equitable care is available.&lt;/p>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/kW1Y">E. Weber, S. J. Miller, V. Astha, T. Janevic, and E. Benn, &amp;ldquo;Characteristics of telehealth users in NYC for COVID-related care during the coronavirus pandemic,&amp;rdquo; J. Am. Med. Inform. Assoc., Nov. 2020, doi: 10.1093/jamia/ocaa216.&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/Wsa3">K. Senz, &amp;ldquo;Racial disparities in telemedicine: A research roundup,&amp;rdquo; Nov. 30, 2020. &lt;a href="https://journalistsresource.org/studies/government/health-care/racial-disparities-telemedicine/">https://journalistsresource.org/studies/government/health-care/racial-disparities-telemedicine/&lt;/a> (accessed Dec. 07, 2020).&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/VuXu">C. L. F. Gilbert C. Gee, &amp;ldquo;STRUCTURAL RACISM AND HEALTH INEQUITIES: Old Issues, New Directions1,&amp;rdquo; Du Bois Rev., vol. 8, no. 1, p. 115, Apr. 2011, Accessed: Dec. 07, 2020. [Online].&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/NRPs">Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan, &amp;ldquo;Dissecting racial bias in an algorithm used to manage the health of populations,&amp;rdquo; Science, vol. 366, no. 6464, pp. 447â€“453, Oct. 2019, Accessed: Dec. 07, 2020. [Online].&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/mDYj">J. N. G. Chazeman S. Jackson, &amp;ldquo;Addressing Health and Health-Care Disparities: The Role of a Diverse Workforce and the Social Determinants of Health,&amp;rdquo; Public Health Rep., vol. 129, no. Suppl 2, p. 57, 2014, Accessed: Dec. 07, 2020. [Online].&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/IZ1k">A. Mamun et al., &amp;ldquo;Diversity in the Era of Precision Medicine - From Bench to Bedside Implementation,&amp;rdquo; Ethn. Dis., vol. 29, no. 3, p. 517, 2019, Accessed: Dec. 07, 2020. [Online].&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/i6o0">&amp;ldquo;A Data-Driven Approach to Addressing Racial Disparities in Health Care Outcomes,&amp;rdquo; Jul. 21, 2020. &lt;a href="https://hbr.org/2020/07/a-data-driven-approach-to-addressing-racial-disparities-in-health-care-outcomes">https://hbr.org/2020/07/a-data-driven-approach-to-addressing-racial-disparities-in-health-care-outcomes&lt;/a> (accessed Dec. 07, 2020).&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/euqs">&amp;ldquo;Study: Black patients more likely than white patients to use telehealth because of pandemic,&amp;rdquo; Sep. 08, 2020. &lt;a href="https://www.healthcareitnews.com/news/study-black-patients-more-likely-white-patients-use-telehealth-because-pandemic">https://www.healthcareitnews.com/news/study-black-patients-more-likely-white-patients-use-telehealth-because-pandemic&lt;/a> (accessed Dec. 07, 2020).&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/E4t2">X. Zhang et al., &amp;ldquo;Big Data Science: Opportunities and Challenges to Address Minority Health and Health Disparities in the 21st Century,&amp;rdquo; Ethn. Dis., vol. 27, no. 2, p. 95, 2017, Accessed: Dec. 07, 2020. [Online].&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/0HLR">S. A. Ibrahim, M. E. Charlson, and D. B. Neill, &amp;ldquo;Big Data Analytics and the Struggle for Equity in Health Care: The Promise and Perils,&amp;rdquo; Health Equity, vol. 4, no. 1, p. 99, 2020, Accessed: Dec. 07, 2020. [Online].&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/A4tr">Institute of Medicine (US) Committee on Understanding and Eliminating Racial and Ethnic Disparities, B. D. Smedley, A. Y. Stith, and A. R. Nelson, &amp;ldquo;PATIENT-PROVIDER COMMUNICATION: THE EFFECT OF RACE AND ETHNICITY ON PROCESS AND OUTCOMES OF HEALTHCARE,&amp;rdquo; in Unequal Treatment: Confronting Racial and Ethnic Disparities in Health Care, National Academies Press (US), 2003.&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/0RYU">CDC, &amp;ldquo;Using Telehealth to Expand Access to Essential Health Services during the COVID-19 Pandemic,&amp;rdquo; Sep. 10, 2020. &lt;a href="https://www.cdc.gov/coronavirus/2019-ncov/hcp/telehealth.html">https://www.cdc.gov/coronavirus/2019-ncov/hcp/telehealth.html&lt;/a> (accessed Dec. 07, 2020).&lt;/a>&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/lyDn">&amp;quot;[No title].&amp;quot; &lt;a href="https://www.nejm.org/doi/full/10.1056/NEJMsr1503323">https://www.nejm.org/doi/full/10.1056/NEJMsr1503323&lt;/a> (accessed Dec. 07, 2020).&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/FjdO">&amp;ldquo;Telehealth Network Grant Program,&amp;rdquo; Feb. 12, 2020. &lt;a href="https://www.hrsa.gov/grants/find-funding/hrsa-20-036">https://www.hrsa.gov/grants/find-funding/hrsa-20-036&lt;/a> (accessed Dec. 07, 2020).&lt;/a>&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Review of the Use of Wearables in Personalized Medicine</title><link>/report/fa20-523-302/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-302/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-302/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-302/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-302/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-302/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Adam Martin, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-302">fa20-523-302&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-302/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Wearable devices offer an abundant source of data on wearer activity and health metrics. Smartphones and smartwatches have become increasingly
ubiquitous, and provide high-quality motion sensor data. This research attempts to classify movement types, including running, walking, sitting, standing,
and going up and down stairs, to establish the practicality of sharing this raw data with healthcare workers. It also addresses the existing research regarding
the use of wearable data in clinical settings and discusses shortcomings in making this data available.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-existing-devices">2.1 Existing Devices&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-need-for-wearable-data-in-healthcare">2.2 Need for Wearable Data in Healthcare&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-choice-of-dataset">3. Choice of Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology-and-code">4. Methodology and Code&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-discussion">5. Discussion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-descriptive-analysis">5.1 Descriptive Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-results">5.2 Results&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-results">6.1 Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-limitations">6.2 Limitations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#63-impact">6.3 Impact&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Wearables, Classification, Descriptive Analysis, Healthcare, Movement Tracking, Precision Health, LSTM&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Wearables have been on the market for years now, gradually improving and providing increasingly insightful data on user health metrics. Most wearables contain an array of sensors allowing the user to track aspects of their physical health.
This includes heart rate, motion, calories burned, and some devices now support ECG and BMI measurements. This vast trove of data is valuable to consumers, as it allows for the measurement and gamification of key
health metrics. But can this data also be useful for health professionals in determining a patientâ€™s activity levels and tracing important events in their health history?&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>Previous work exists on the use of sensors and wearables in assisted living environments. Consumer wearables are commonplace and have been used primarily for tracking individual activity metrics.
This research attempts to establish the efficacy of these devices in providing useful data for user activity, and how this information could be useful for healthcare workers.
This paper examines the roadblocks in making this information available to healthcare professionals and examines what wearable information is currently being used in healthcare.&lt;/p>
&lt;h3 id="21-existing-devices">2.1 Existing Devices&lt;/h3>
&lt;p>Existing research focuses on a wide variety of inputs &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Sensors including electrodes, chemical probes, microphones, optical detectors, and blood glucose sensors are referenced as devices used for gathering healthcare information. This research will focus on data that can be gathered with a modern smartphone or smartwatch. Most of the sensors described are not as ubiquitous as consumer items like FitBits or Apple Watches.
Furthermore, many users report diminished enthusiasm towards wearables due to complex sensors and pairing processes &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Focusing on devices that are already successful in the consumer market
ensures that the impact of this study will not be confined to specific users and use cases. Apple has released a suite of tools for interfacing with device sensors, and recently launched
ResearchKit and CareKit, providing a framework for researchers and healthcare workers to collect and analyze user data &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. There are several apps available that utilize these tools, including
Johns Hopkins' CorrieHealth app, which helps users manage their heart health care and shares data with their doctors. This is an encouraging step towards streamlining the sharing of
wearable data between patients and healthcare professionals, as Apple provides standards for privacy, consent, and data quality.&lt;/p>
&lt;h3 id="22-need-for-wearable-data-in-healthcare">2.2 Need for Wearable Data in Healthcare&lt;/h3>
&lt;p>Previous studies have indicated the significance of precision health and the need for patient-specific data from wearables to be integrated into a patient&amp;rsquo;s care strategy &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.
Wearable data outlining a patient&amp;rsquo;s sleep, motion habits, heart rate, and other metrics can be invaluable in diagnosing or predicting conditions. Increased sedentary activity could indicate
depression, and could predict future heart problems. A patient&amp;rsquo;s health could be graphed and historical trends could be useful in determining factors that contribute to the patient&amp;rsquo;s condition.
It is often asserted that a person&amp;rsquo;s environmental factors are better predictors for their health than their genetic makeup &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Linking behavioral and social determinants with biomedical data
would allow professionals to better target certain conditions.&lt;/p>
&lt;h2 id="3-choice-of-dataset">3. Choice of Dataset&lt;/h2>
&lt;p>The dataset used for this project contains labeled movement data from wearable devices. The goal is to establish the potential for wearable devices to provide high-quality data to users and healthcare professionals.&lt;/p>
&lt;p>A dataset gathered from 24 individuals with Apple devices measuring attitude, gravity, and acceleration was used to determine user states. The dataset is labeled with six states (walking downstairs,
walking upstairs, sitting, standing, walking and jogging) and each sensor has several attributes describing its motion. The attitude, gravity, and acceleration readings each have three components corresponding to each
axis of freedom. Many smartphones and wearables already offer comprehensive sleep tracking features, so sleep motion
data will not be considered for this study. The CrowdSense iOS application was used to record user movements. Each sensor was configured to sample at 50hz, and each user was instructed to start the recording,
and begin their assigned activity.&lt;/p>
&lt;h2 id="4-methodology-and-code">4. Methodology and Code&lt;/h2>
&lt;p>The IPython notebook used for this analysis is available on the &lt;a href="https://github.com/cybertraining-dsc/fa20-523-302/blob/main/project/code/Wearables.ipynb">GitHub repository&lt;/a>.&lt;/p>
&lt;p>The analysis of relevant wearable data is undertaken to determine the accuracy of activity information. This analysis will consist of a brief descriptive analysis of the motion tracking data,
and will proceed with attempts to classify the labeled data.&lt;/p>
&lt;p>First, the data has to be downloaded from the MotionSense project on GitHub. A basic descriptive analysis will be performed, visualizing the sensor values for each movement class over time.
During the data acquisition, the sensors are sampled at a 50hz rate. Since the dataset is a timeseries, classification methods that take advantage of historical datapoints will be the most effective.
The Keras Long Short Term Memory classifier implementation is used for this task. The dataset is first split into its various classes of motion using the one-hot-encoded matrix to filter out each
class. Each class is then subdivided into one-second &amp;lsquo;windows&amp;rsquo;, each with 50 entries. Each window is offset by 10 entries from the previous window. The use of windows allows the model to remain small
in size, while still gathering enough information to make accurate classifications. The hyperparameters that can be tuned include:&lt;/p>
&lt;ul>
&lt;li>Window size (50)&lt;/li>
&lt;li>Window offset (10)&lt;/li>
&lt;li>LSTM size (50)&lt;/li>
&lt;li>Dense layer size (50)&lt;/li>
&lt;li>Batch size (64)&lt;/li>
&lt;li>Epochs (15)&lt;/li>
&lt;li>Dropout ratio (0.5)&lt;/li>
&lt;/ul>
&lt;p>The resulting data structure is a 3-dimensional array of shape (107434, 12, 50) for the training set and (32439, 12, 50) for the testing set. The dimensions correspond to the number of windows, the number of movement
features, and the number of samples per window, respectively. These windows are then paired with their corresponding movement classifications and fed into a Keras LSTM workflow. This workflow is executed on a standard (non-gpu)
Google Colab instance and benchmarked. The workflow consists of the following:&lt;/p>
&lt;ul>
&lt;li>A Long Short Term Memory layer with the cell count matching the size of the input window (50)&lt;/li>
&lt;li>A Dropout layer to minimize overfitting&lt;/li>
&lt;li>A fully connected layer with relu activation to help learn the weights of the LSTM output&lt;/li>
&lt;li>A fully connected output layer with a softmax activation to return the final classifications&lt;/li>
&lt;/ul>
&lt;p>The model is trained in 15 epochs, and uses a batch size of 64 for each backpropagation.&lt;/p>
&lt;p>If a classification strategy of sufficient accuracy is possible, it will be determined that wearable data can potentially serve as a useful supplementary source of information to aid in establishing a patient&amp;rsquo;s
medical history.&lt;/p>
&lt;p>Reviewing relevant literature is important to determine the current state of wearables research regarding usefulness to healthcare workers and user well-being.
Much of this research will be focused on determining the state of wearables in the healthcare industry and determining if there is a need for streamlined data transfer to healthcare professionals.&lt;/p>
&lt;h2 id="5-discussion">5. Discussion&lt;/h2>
&lt;p>The dataset is comprised of six discrete classes of movement. There are 12 parameters describing the readouts of the sensors over time.&lt;/p>
&lt;h3 id="51-descriptive-analysis">5.1 Descriptive Analysis&lt;/h3>
&lt;p>There is an imbalance in the number of datapoints for each class, which could lead to classification errors.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/occurence.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Data distribution per movement class.&lt;/p>
&lt;p>Only roll, pitch, and yaw are shown for clarity and to illustrate the quality of the readings obtained by the sensors. Figures 2-7 illustrate sensor readouts over time for each class of movement.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_run.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> 10 second sensor readout of a jogging male.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_downstairs.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> 20 second sensor readout of a female going downstairs.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_upstairs.png" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> 20 second sensor readout of a male going upstairs.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_walk.png" alt="Figure 5">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> 10 second sensor readout of a female walking.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_sit.png" alt="Figure 6">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> 10 second sensor readout of a male sitting.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_stand.png" alt="Figure 7">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> 10 second sensor readout of a female standing.&lt;/p>
&lt;p>Interestingly, initial classification attempts involving random forests and knn methods performed fairly well despite their inherent lack of awareness of historical data.&lt;/p>
&lt;h3 id="52-results">5.2 Results&lt;/h3>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/LSTM_benchmark.png" alt="Figure 8">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Cloudmesh benchmark for LSTM train and test.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/lstm_curves.png" alt="Figure 9">&lt;/p>
&lt;p>&lt;strong>Figure 9&lt;/strong> LSTM training and loss curves.&lt;/p>
&lt;p>The final accuracy measurement for the LSTM was &lt;strong>%95.42&lt;/strong>. This proves that discrete movement classes can be determined through the analysis of basic sensor data regarding device movement.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;h3 id="61-results">6.1 Results&lt;/h3>
&lt;p>Using relatively basic machine learning methods, it is possible to determine with a high level of accuracy the type of movement being performed at a given moment. Viewing the benchmarks,
the inference time is rapid, taking only 3 seconds to validate results for the entire testing dataset. This model could be distilled for a production environment, and the rapid inference speed
would allow for faster analyses for end users.&lt;/p>
&lt;h3 id="62-limitations">6.2 Limitations&lt;/h3>
&lt;p>The classes of movement considered for this study were limited. For more precise movements, or movement combinations, more data and a more complex model would be required. For example;
classifying the type of activity being done while a user is seated, if they are typing or eating. Future research could involve a wider review of timeseries classifiers, including transformers
convolutional neural networks, and recurrent neural networks, in order to establish what classification strategy would be best suited for this data. Privacy is also important to consider; raw sensor data could provide malicious actors with
information regarding a users daily habits, their gender, their location, and other sensitive data.&lt;/p>
&lt;p>Existing research highlights some of the issues with the adoption of wearable devices in healthcare. Inconsistent reporting, usage, and data quality are the most common concerns &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. Addressing
these issues through an analysis of data quality and device usage could contribute towards the robustness of this study.&lt;/p>
&lt;h3 id="63-impact">6.3 Impact&lt;/h3>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/service_prop.png" alt="Figure 10">&lt;/p>
&lt;p>&lt;strong>Figure 10&lt;/strong> Proposal for integration of wearables data with other data sources and healthcare portals.&lt;/p>
&lt;p>Frameworks like Apple CareKit and Google Fit are emerging to address the increasing demand for health tracking applications. There is a need for a more effective pipeline for sharing this information
securely with doctors and researchers, and these frameworks are a step in the right direction. Furthermore, this research can be applied towards finding correlations between a patient&amp;rsquo;s condition and
their activity history, or helping a patient reach certain goals towards their overall well-being. Comprehensive movement history can be combined with device usage patterns, eating habit data, self-reported
well-being data, and other relevant sources to establish a more holistic perspective of a patient&amp;rsquo;s health.
Giving users and healthcare workers access to and insights on the data that they generate every day can promote healthier habits, increase physician efficacy, and promote overall well-being. The author proposes
the idea of a centralized system for user data tracking. This could support cross-platform devices, and tie into other fitness and well-being apps to provide a centralized and holistic view of a user&amp;rsquo;s health.
A system of this nature could also tie in information from patient portals, including test results, checkup info, and prescription information.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor von Laszewski for his invaluable feedback on this paper, and Dr. Geoffrey Fox for sharing his expertise in Big Data applications throughout this course.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Yetisen, Ali K. (2018, August 16). I Retrieved November 15, 2020 from &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6541866/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6541866/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Piwek L, Ellis DA, Andrews S, Joinson A. The Rise of Consumer Health Wearables: Promises and Barriers (2016, February 02). I Retrieved November 11, 2020 from &lt;a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001953">https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001953&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Loncar-Turukalo, Tatjana. Literature on Wearable Technology for Connected Health: Scoping Review of Research Trends, Advances, and Barriers (2019, September 21). I Retrieved December 1st from &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6818529/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6818529/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Glasgow, Russell E. Realizing the full potential of precision health: The need to include patient-reported health behavior, mental health, social determinants, and patient preferences data (2018, September 13). I Retrieved November 15, 2020 from &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6202010/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6202010/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Malekzadeh, Mohammad. Mobile Sensor Data Anonymization (2018). I Retrieved September 18, 2020 from &lt;a href="http://doi.acm.org/10.1145/3302505.3310068">http://doi.acm.org/10.1145/3302505.3310068&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Detect and classify pathologies in chest X-rays using PyTorch library</title><link>/report/fa20-523-319/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-319/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-319/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-319/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final&lt;/p>
&lt;p>Rama Asuri, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-319/">fa20-523-319&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-319/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;p>Code: &lt;a href="https://github.com/cybertraining-dsc/fa20-523-319/blob/main/project/project.ipynb">project.ipynb&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Chest X-rays reveal many diseases. Early detection of disease often improves the survival chance for Patients. It is one of the important tools for Radiologists to detect and identify underlying health conditions. However, they are two major drawbacks. First, it takes time to analyze a radiograph. Second, Radiologists make errors. Whether it is an error in diagnosis or delay in diagnosis, both outcomes result in a loss of life. With the technological advances in AI, Deep Learning models address these drawbacks. The Deep Learning models analyze the X-rays like a Radiologist and accurately predict much better than the Radiologists. In our project, first, we develop a Deep Learning model and train our model to use the labels for Atelectasis, Cardiomegaly, Consolidation, Edema, and Pleural Effusion that corresponds to 5 different diseases, respectively. Second, we test our model&amp;rsquo;s performance: how well our model predicts the diseases. Finally, we visualize our model&amp;rsquo;s performance using the AUC-ROC curve.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-overview-of-pytorch-library">2. Overview Of PyTorch Library&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-overview-of-densenet">3. Overview of DenseNet&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-dense-blocks">3.1 Dense blocks&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-transition-layers">3.2 Transition layers&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-overview-of-chexpert-dataset">4. Overview of CheXpert Dataset&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-data-collection">4.1 Data Collection&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-data-labelling">4.2 Data Labelling&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-label-extraction">4.3 Label Extraction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#44-label-classification">4.4 Label Classification&lt;/a>&lt;/li>
&lt;li>&lt;a href="#45-label-aggregation">4.5 Label Aggregation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-overview-of-auc-roc-curve">5. Overview Of AUC-ROC Curve&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-sensitivitytrue-positive-rate-tpr">5.1 Sensitivity/True Positive Rate (TPR)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-false-negative-rate-fnr">5.2 False Negative Rate (FNR)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-specificitytrue-negative-rate-tnr">5.3 Specificity/True Negative Rate (TNR)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#54-false-positive-rate-fpr">5.4 False Positive Rate (FPR)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#55-purpose-of-auc-roc-curve">5.5 Purpose of AUC-ROC curve&lt;/a>&lt;/li>
&lt;li>&lt;a href="#56-definition-of-auc-roc">5.6 Definition of AUC-ROC&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-chest-x-rays---multi-image-classification-using-deep-learning-model">6. Chest X-Rays - Multi-Image Classification Using Deep Learning Model&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-load-and-split-chest-x-rays-dataset">6.1 Load and split Chest X-rays Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-build-and-train-baseline-deep-learning-model">6.2 Build and train baseline Deep Learning model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#63-evaluate-the-model">6.3 Evaluate the model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#64-predict-the-pathologies">6.4 Predict the pathologies&lt;/a>&lt;/li>
&lt;li>&lt;a href="#65-calculate-the-auc-roc-score">6.5 Calculate the AUC-ROC score&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-results-and-analysis">7. Results and Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-conclusion">8. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-future-plans">9. Future Plans&lt;/a>&lt;/li>
&lt;li>&lt;a href="#10-acknowledgements">10. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#11-references">11. References&lt;/a>&lt;/li>
&lt;li>&lt;a href="#12-appendix">12. Appendix&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#121-project-plan">12.1 Project Plan&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> PyTorch, CheXpert&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Radiologists widely use chest X-Rays to identify and detect underlying conditions. However, analyzing Chest X-Rays takes too much time, and accurately diagnosing without errors requires considerable experience. On the one hand, if the analyzing process is expedited, it might result in misdiagnosis, but on the other hand, lack of experience means long analysis time and/or errors; even with the correct diagnosis, it might be too late to prescribe a treatment. Radiologists are up against time and experience. With the advancements in AI, Deep Learning can easily solve this problem quickly and efficiently.&lt;/p>
&lt;p>Deep Learning methods are becoming very reliable at achieving expert-level performance using large labeled datasets. Deep learning is a technique to extract and transform data using multiple layers of neural networks. Each layer takes inputs from previous layers and incrementally refines it. An algorithm is used to train these layers to minimize errors and improve these layers' overall accuracy &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. It enables the network to learn to perform a specified task and gain an expert level performance by training on large datasets. The scope of this project is to identify and detect the following 5 pathologies using an image classification algorithm: Atelectasis, Cardiomegaly, Consolidation, Edema, and Pleural Effusion. We use the CheXpert dataset, which consists of Chest X-rays. CheXpert dataset contains 224,316 chest Radiographs of 65,240 patients. The dataset has 14 observations in radiology reports and captures uncertainties inherent in radiograph interpretation using uncertainty labels. Our focus is on 5 observations (Atelectasis, Cardiomegaly, Consolidation, Edema, and Pleural Effusion). We impute uncertainty labels with randomly selected Boolean values. Our Deep Learning models are developed using the PyTorch library, enabling fast, flexible experimentation and efficient production through a user-friendly front-end, distributed training, and ecosystem of tools and libraries &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. It was primarily developed by Facebook&amp;rsquo;s AI Research lab (FAIR) and used for Computer Vision and NLP applications. PyTorch supports Python and C++ interfaces. There are popular Deep Learning applications built using PyTorch, including Tesla Autopilot, Uber&amp;rsquo;s Pyro &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In this analysis, first, we begin with an overview of the PyTorch library and DenseNet. We cover DenseNet architecture and advantages over ResNet for Multi-Image classification problems. Second, we explain the CheXpert dataset and how the classifiers are labeled, including uncertainties. Next, we cover the AUC-ROC curve&amp;rsquo;s basic definitions and how it measures a model&amp;rsquo;s performance. Finally, we explain how our Deep Learning model classifies pathologies and conclude with our model&amp;rsquo;s performance and results.&lt;/p>
&lt;h2 id="2-overview-of-pytorch-library">2. Overview Of PyTorch Library&lt;/h2>
&lt;p>The PyTorch library is based on Python and is used for developing Python deep learning models. Many of the early adopters of the PyTorch are from the research community. It grew into one of the most popular libraries for deep learning projects. PyTorch provides great insight into Deep Learning. PyTorch is widely used in real-world applications. PyTorch makes an excellent choice for introducing deep learning because of clear syntax, streamlined API, and easy debugging. PyTorch provides a core data structure, the tensor, a multidimensional array similar to NumPy arrays. It performs accelerated mathematical operations on dedicated hardware, making it convenient to design neural network architectures and train them on individual machines or parallel computing resources &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-overview-of-densenet">3. Overview of DenseNet&lt;/h2>
&lt;p>We use a pre-trained DenseNet model, which classifies the images. DenseNet is new Convolutional Neural Network architecture which is efficient on image classification benchmarks as compared to ResNet &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. RestNets, Highway networks, and deep and wide neural networks add more inter-layer connections than the direct connection in adjacent layers to boost information flow and layers. Similar to ResNet, DenseNet adds shortcuts among layers. Different from ResNet, a layer in dense receives all the outputs of previous layers and concatenate them in the depth dimension. In ResNet, a layer only receives outputs from the last two layers, and the outputs are added together on the individual same depth. Therefore it will not change the depth by adding shortcuts. In other words, in ResNet the output of layer of k is x[k] = f(w * x[k-1] + x[k-2]), while in DenseNet it is x[k] = f(w * H(x[k-1], x[k-2], &amp;hellip; x[1])) where H means stacking over the depth dimension. Besides, ResNet makes learn the identity function easy, while DenseNet directly adds an identity function &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Figure 1 shows the DenseNet architecture.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/densetnet.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> DenseNet Architecture &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/p>
&lt;p>As shown in Figure 1, DenseNet contains a feature layer (convolutional layer) capturing low-level features from images, several dense blocks, and transition layers between adjacent dense blocks &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="31-dense-blocks">3.1 Dense blocks&lt;/h3>
&lt;p>Dense block contains several dense layers. The depth of a dense layer output is called growth_rate. Every dense layer receives all the output of its previous layers. The input depth for the kth layer is (k-1)*growth_rate + input_depth_of_first_layer. By adding more layers in a dense block, the depth will grow linearly. For example, if the growth rate is 30 and after 100 layers, the depth will be over 3000. However, this could lead to a computational explosion. It is addressed by introducing a transition layer to reduce and abstract the features after a dense block with a limited number of dense layers to circumvent this problem &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. A 1x1 convolutional layer (bottleneck layer) is added to reduce the computation, which makes the second convolutional layer always has a fixed input depth. It is also easy to see the size (width and height) of the feature maps keeps the same through the dense layer, making it easy to stack any number of dense layers together to build a dense block. For example, densenet121 has four dense blocks with 6, 12, 24, and 16 dense layers. With repetition, it is not that difficult to make 112 layers &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="32-transition-layers">3.2 Transition layers&lt;/h3>
&lt;p>In general, the size of every layer&amp;rsquo;s output in Convolutional Neural Network decreases to abstract higher-level features. In DenseNet, the transition layers take this responsibility while the dense blocks keep the size and depth. Every transition layer contains a 1x1 convolutional layer and a 2x2 average pooling layer to reduce the size to half. However, transition layers also receive all the output from all the last dense block layers. So the 1*1 convolutional layer reduces the depth to a fixed number, while the average pooling reduces the size.&lt;/p>
&lt;h2 id="4-overview-of-chexpert-dataset">4. Overview of CheXpert Dataset&lt;/h2>
&lt;p>CheXpert is a large public dataset. It contains an interpreted chest radiograph consisting of 224,316 chest radiographs
of 65,240 patients labeled for the presence of 14 observations as positive, negative, or uncertain &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Figure 2 shows the CheXpert 14 labels and the Probability &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Our analysis is to predict the probability of 5 different observations (Atelectasis, Cardiomegaly, Consolidation, Edema, and Pleural Effusion) from multi-view chest radiographs shown in Figure 2&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/chest_disease.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Probability of different observations &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="41-data-collection">4.1 Data Collection&lt;/h3>
&lt;p>CheXpert dataset is a collection of chest radiographic studies from Stanford Hospital, performed between October 2002 and July 2017 in inpatient and outpatient centers, along with their associated radiology reports. Based on studies, a sampled set of 1000 reports were created for manual review by a board-certified radiologist to determine the feasibility for extraction of observations. The final set consists of 14 observations based on the prevalence in the reports and clinical relevance, conforming to the Fleischner Societyâ€™s recommended glossary. &lt;em>Pneumonia&lt;/em>, despite
being a clinical diagnosis, &lt;em>Pneumonia&lt;/em> was included as a label to represent the images that suggested primary infection as the diagnosis. The &lt;em>No Finding&lt;/em> observation was intended to capture the absence of all pathologies &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="42-data-labelling">4.2 Data Labelling&lt;/h3>
&lt;p>Labels developed using an automated, rule-based labeler to extract observations from the free text radiology reports to be used as structured labels for the images &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="43-label-extraction">4.3 Label Extraction&lt;/h3>
&lt;p>The labeler extracts the pathologies mentioned in the list of observations from the Impression section of radiology reports, summarizing the key findings in the radiographic study. Multiple board-certified radiologists manually curated a large list of phrases to match various observations mentioned in the reports &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="44-label-classification">4.4 Label Classification&lt;/h3>
&lt;p>Labeler extracts the mentions of observations and classify them as negative (&amp;ldquo;no evidence of pulmonary edema, pleural effusions or pneumothorax&amp;rdquo;), uncertain (&amp;ldquo;diffuse reticular pattern may represent mild interstitial pulmonary edema&amp;rdquo;), or positive (&amp;ldquo;moderate bilateral effusions and bibasilar opacities&amp;rdquo;). The &amp;lsquo;uncertain&amp;rsquo; label can capture both the uncertainty of a radiologist in the diagnosis as well as the ambiguity inherent in the report (&amp;ldquo;heart size is stable&amp;rdquo;). The mention classification stage is a 3-phase pipeline consisting of pre-negation uncertainty, negation, and post-negation uncertainty. Each phase consists of rules that are matched against the mention; if a match is found, the mention is classified accordingly (as uncertain in the first or third phase and as negative in the second phase). If a mention is not matched in any of the phases, it is classified as positive &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="45-label-aggregation">4.5 Label Aggregation&lt;/h3>
&lt;p>CheXpert dataset use the classification for each mention of observations to arrive at a final label for 14 observations that consist of 12 pathologies and the &amp;ldquo;Support Devices&amp;rdquo; and &amp;ldquo;No Finding&amp;rdquo; observations. Observations with at least one mention positively classified in the report are assigned a positive (1) label. An observation is assigned an uncertain (u) label if it has no positively classified mentions and at least one uncertain mention, and a negative label if there is at least one negatively classified mention. We assign (blank) if there is no mention of an observation. The &amp;ldquo;No Finding&amp;rdquo; observation is assigned a positive label (1) if there is no pathology classified as positive or uncertain &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="5-overview-of-auc-roc-curve">5. Overview Of AUC-ROC Curve&lt;/h2>
&lt;p>AUC-ROC stands for Area Under Curve - Receiver Operating Characteristics. It visualizes how well a machine learning classifier is performing. However, it works for only binary classification problems &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. In our project, we extend it to evaluate Multi-Image classification problem. AUC-ROC curve is a performance measurement for classification problems at various threshold settings. ROC is a probability curve, and AUC represents the degree or measure of separability. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By analogy, the Higher the AUC, the model distinguishes between patients with the disease and no disease &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Figure 3 shows Confusion Matrix. We use Confusion Matrix to explain Sensitivity and Specificity.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/confusion_matrix.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Confusion Matrix&lt;/p>
&lt;h3 id="51-sensitivitytrue-positive-rate-tpr">5.1 Sensitivity/True Positive Rate (TPR)&lt;/h3>
&lt;p>Sensitivity/True Positive Rate (TPR) explains what proportion of the positive class got correctly classified. A simple example would be determining what proportion of the actual sick people are correctly detected by the model &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/TPR.png" alt="Figure 4">&lt;/p>
&lt;h3 id="52-false-negative-rate-fnr">5.2 False Negative Rate (FNR)&lt;/h3>
&lt;p>False Negative Rate (FNR) explains what proportion of the positive class is incorrectly classified by the classifier. A higher TPR and a lower FNR means correctly classify the positive class &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/FNR.png" alt="Figure 5">&lt;/p>
&lt;h3 id="53-specificitytrue-negative-rate-tnr">5.3 Specificity/True Negative Rate (TNR)&lt;/h3>
&lt;p>Specificity/True Negative Rate (TNR) indicates what proportion of the negative class is classified correctly. For example, Specificity determines what proportion of actual healthy people are correctly classified as healthy by the model &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/TNR.png" alt="Figure 6">&lt;/p>
&lt;h3 id="54-false-positive-rate-fpr">5.4 False Positive Rate (FPR)&lt;/h3>
&lt;p>False Positive Rate (FPR) indicates what proportion of the negative class got incorrectly classified by the classifier. A higher TNR and a lower FPR means the model correctly classifies the negative class&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/FPR.png" alt="Figure 7">&lt;/p>
&lt;h3 id="55-purpose-of-auc-roc-curve">5.5 Purpose of AUC-ROC curve&lt;/h3>
&lt;p>A machine learning classification model can predict the actual class of the data point directly or predict its probability of belonging to different classes. The example for the former case is where a model can classify whether a patient is healthy or not healthy. In the latter case, a model can predict a patient&amp;rsquo;s probability of being healthy or not healthy and provide more control over the result by enabling a way to tune the model&amp;rsquo;s behavior by changing the threshold values. This is powerful because it eliminates the possibility of building a completely new model to achieve a different range of results &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. A threshold value helps to interpret the probability and map the probability to a class label. For example, a threshold value such as 0.5, where all values equal to or greater than the threshold, is mapped to one class and rests to another class &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Introducing different thresholds for classifying positive class for data points will inadvertently change the Sensitivity and Specificity of the model. Furthermore, one of these thresholds will probably give a better result than the others, depending on whether we aim to lower the number of False Negatives or False Positives &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.
As seen in Figure 8, the metrics change with the changing threshold values. We can generate different confusion matrices and compare the various metrics. However, it is very inefficient. Instead, we can generate a plot between some of these metrics so that we can easily visualize which threshold is giving us a better result. The AUC-ROC curve solves just that &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/aucroc.png" alt="Figure 8">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Probability of prediction and metrics &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="56-definition-of-auc-roc">5.6 Definition of AUC-ROC&lt;/h3>
&lt;p>The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the signal from the noise. The Area Under the Curve (AUC) is the measure of a classifier&amp;rsquo;s ability to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the model&amp;rsquo;s performance at distinguishing between the positive and negative classes. When AUC = 1, the classifier can perfectly distinguish between all the Positive and the Negative class points correctly. If, however, the AUC had been 0, then the classifier would be predicting all Negatives as Positives and all Positives as Negatives. When 0.5&amp;lt;AUC&amp;lt;1, there is a high chance that the classifier will be able to distinguish the positive class values from the negative class values. This is because the classifier can detect more True positives and True negatives than False negatives and False positives. When AUC=0.5, then the classifier is not able to distinguish between Positive and Negative class points. It means either the classifier is predicting random class or constant class for all the data points. Therefore, the higher the AUC value for a classifier, the better its ability to distinguish between positive and negative classes. In the AUC-ROC curve, a higher X-axis value indicates a higher number of False positives than True negatives. Simultaneously, a higher Y-axis value indicates a higher number of True positives than False negatives. So, the choice of the threshold depends on balancing between False positives and False negatives &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="6-chest-x-rays---multi-image-classification-using-deep-learning-model">6. Chest X-Rays - Multi-Image Classification Using Deep Learning Model&lt;/h2>
&lt;p>Our Deep Learning model loads and processes the raw data files and implement a Python class to represent data by
converting it into a format usable by PyTorch. We then, visualize the training and validation data.&lt;/p>
&lt;p>Our approach to predicting pathologies will have 5 steps.&lt;/p>
&lt;ul>
&lt;li>Load and split Chest X-rays Dataset&lt;/li>
&lt;li>Build and train baseline Deep Learning model&lt;/li>
&lt;li>Evaluate the model&lt;/li>
&lt;li>Predict the pathologies&lt;/li>
&lt;li>Calculate the AUC-ROC score&lt;/li>
&lt;/ul>
&lt;h3 id="61-load-and-split-chest-x-rays-dataset">6.1 Load and split Chest X-rays Dataset&lt;/h3>
&lt;p>We load and split the dataset to 90% for training and 10% for validation randomly.&lt;/p>
&lt;h3 id="62-build-and-train-baseline-deep-learning-model">6.2 Build and train baseline Deep Learning model&lt;/h3>
&lt;p>We use the PyTorch library to implement and train DenseNet CNN as a baseline model. With initial weights from ImageNet, we retrain all layers. In PyTorch, we implement a subclass for the PyTorch to transform CheXpert Dataset and create a custom data loading process. The Image Augmentation is executed within this subclass. Additionally, a DataLoader also needs to be created. We shuffle the dataset for the training dataloader. We also create a validation dataloader, which is different from the training dataloader and does not require shuffling. In the baseline model, we use DenseNet pre-trained on the ImageNet dataset. The model&amp;rsquo;s classifier is replaced with a new dense layer and use the CheXpert labels to train &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. The number of trainable parameters 6968206 (~7 million).&lt;/p>
&lt;h3 id="63-evaluate-the-model">6.3 Evaluate the model&lt;/h3>
&lt;p>To evaluate the model, we implement a function to validate the model on the validation dataset.&lt;/p>
&lt;h3 id="64-predict-the-pathologies">6.4 Predict the pathologies&lt;/h3>
&lt;p>We use our model to predict 1 of the 5 pathologies - Atelectasis, Cardiomegaly, Consolidation, Edema, and Pleural Effusion. Our model uses a test dataset.&lt;/p>
&lt;h3 id="65-calculate-the-auc-roc-score">6.5 Calculate the AUC-ROC score&lt;/h3>
&lt;p>We have multiple labels, and we need to calculate the AUCROC-score for each class against the rest of the classifiers.&lt;/p>
&lt;h2 id="7-results-and-analysis">7. Results and Analysis&lt;/h2>
&lt;p>Figure 9 shows training loss, Validation loss and validation AUC-ROC score after training our Deep Learning model for 7 hours.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/Train.png" alt="Figure 9">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> Training loss, Validation loss and validation AUC-ROC score&lt;/p>
&lt;p>Figure 10 shows model predicts False Positives. Below is AUCROC table.
&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/roc.png" alt="Figure 10">&lt;/p>
&lt;p>&lt;strong>Figure 10:&lt;/strong> AUC - ROC data&lt;/p>
&lt;p>This graph is taken from Stanford CheXpert dataset. Based on Figure 11 &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>, our AUCROC value is around 85%.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-319/raw/main/project/images/aucroc_stanford.png" alt="Figure 11">&lt;/p>
&lt;p>&lt;strong>Figure 11:&lt;/strong> AUC - ROC Curve &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="8-conclusion">8. Conclusion&lt;/h2>
&lt;p>Our model achieves the best AUC on Edema (0.89) and the worst on Plural(0.65). The AUC of all other observations is around 0.78. Our model achieves above 0.65 overall predictions.&lt;/p>
&lt;h2 id="9-future-plans">9. Future Plans&lt;/h2>
&lt;p>As the next steps, we will work to improve the model&amp;rsquo;s algorithm and leverage DenseNet architecture to train using smaller dataset.&lt;/p>
&lt;h2 id="10-acknowledgements">10. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors for providing continuous guidance and feedback for this final project.&lt;/p>
&lt;h2 id="11-references">11. References&lt;/h2>
&lt;h2 id="12-appendix">12. Appendix&lt;/h2>
&lt;h3 id="121-project-plan">12.1 Project Plan&lt;/h3>
&lt;ul>
&lt;li>October 26, 2020
&lt;ul>
&lt;li>Test train and validate functionality on PyTorch Dataset&lt;/li>
&lt;li>Update Project.md with project plan&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>November 02, 2020
&lt;ul>
&lt;li>Test train and validate functionality on manual uploaded CheXpert Dataset&lt;/li>
&lt;li>Update project.md with specific details about Deep learning models&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>November 09, 2020
&lt;ul>
&lt;li>Test train and validate functionality on downloaded CheXpert Dataset using &amp;ldquo;wget&amp;rdquo;&lt;/li>
&lt;li>Update project.md with details about train and validation data set&lt;/li>
&lt;li>Capture improvements to loss function&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>November 16, 2020
&lt;ul>
&lt;li>Self review - code and project.md&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>December 02, 2020
&lt;ul>
&lt;li>Review with TA/Professor - code and project.md&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>December 07, 2020
&lt;ul>
&lt;li>Final submission - code and project.md&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Howard, Jeremy; Gugger, Sylvain. Deep Learning for Coders with fastai and PyTorch . O&amp;rsquo;Reilly Media. Kindle Edition &lt;a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527/ref=sr_1_5?dchild=1&amp;amp;keywords=pytorch&amp;amp;qid=1606487426&amp;amp;sr=8-5">https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527/ref=sr_1_5?dchild=1&amp;amp;keywords=pytorch&amp;amp;qid=1606487426&amp;amp;sr=8-5&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>An open source machine learning framework that accelerates the path from research prototyping to production deployment &lt;a href="https://pytorch.org/">https://pytorch.org/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Overview of PyTorch Library &lt;a href="https://en.wikipedia.org/wiki/PyTorch">https://en.wikipedia.org/wiki/PyTorch&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Introduction to PyTorch and documentation &lt;a href="https://pytorch.org/deep-learning-with-pytorch">https://pytorch.org/deep-learning-with-pytorch&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>The efficiency of densenet121 &lt;a href="https://medium.com/@smallfishbigsea/densenet-2b0889854a92">https://medium.com/@smallfishbigsea/densenet-2b0889854a92&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Densetnet architecture &lt;a href="https://miro.medium.com/max/1050/1*znemMaROmOd1CzMJlcI0aA.png">https://miro.medium.com/max/1050/1*znemMaROmOd1CzMJlcI0aA.png&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Densely Connected Convolutional Networks &lt;a href="https://arxiv.org/pdf/1608.06993.pdf">https://arxiv.org/pdf/1608.06993.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Whitepaper - CheXpert Dataset and Labelling &lt;a href="https://arxiv.org/pdf/1901.07031.pdf">https://arxiv.org/pdf/1901.07031.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Chest X-ray Dataset &lt;a href="https://stanfordmlgroup.github.io/competitions/chexpert/">https://stanfordmlgroup.github.io/competitions/chexpert/&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Overview of AUC-ROC Curve in Machine Learning &lt;a href="https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/">https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>PyTorch Deep Learning Model for CheXpert Dataset &lt;a href="https://www.kaggle.com/hmchuong/chexpert-pytorch-densenet121">https://www.kaggle.com/hmchuong/chexpert-pytorch-densenet121&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Definition of Threshold &lt;a href="https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/#:~:text=The%20decision%20for%20converting%20a,in%20the%20range%20between%200">https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/#:~:text=The%20decision%20for%20converting%20a,in%20the%20range%20between%200&lt;/a>&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>AUCROC curves from CheXpert &amp;lt;&amp;gt;&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item></channel></rss>