<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cybertraining – reu</title><link>/tags/reu/</link><description>Recent content in reu on Cybertraining</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 21 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/reu/index.xml" rel="self" type="application/rss+xml"/><item><title>Report: Tutorial on Uploading Files to Google Colab</title><link>/report/su21-reu-361/tutorials/colab/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-361/tutorials/colab/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This tutorial teaches how to import csv&amp;rsquo;s into a Google Colab .ipynb.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#note">Note&lt;/a>&lt;/li>
&lt;li>&lt;a href="#read-file-from-drive">Read File from Drive&lt;/a>&lt;/li>
&lt;li>&lt;a href="#read-file-from-direct-upload">Read File from Direct Upload&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> colab&lt;/p>
&lt;h2 id="note">Note&lt;/h2>
&lt;p>There are two different methods on uploading files to Google Colab Jupyter notebooks. One way is to
have the user upload the file to the user&amp;rsquo;s Google Drive before running the notebook. Another way
is to have the notebook ask the user to upload a file from the user&amp;rsquo;s computer directly into the notebook.
This tutorial outlines both ways.&lt;/p>
&lt;p>The notebook code with both methods can be found &lt;a href="https://colab.research.google.com/drive/1nUMmLYpz_4fILf6xrJMDWs9_vFFUrZQ6?usp=sharing">here&lt;/a>&lt;/p>
&lt;h2 id="read-file-from-drive">Read File from Drive&lt;/h2>
&lt;p>The first cell contains import statements, some of which are not used because the code was taken from an
REU student&amp;rsquo;s code. Nonetheless, it should not be a problem to run the code on Google Colab which
automatically imports such modules.&lt;/p>
&lt;p>Cell 1:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">numpy&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">np&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">pandas&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">pd&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">matplotlib.pyplot&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">plt&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">seaborn&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">sns&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">tensorflow&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">tf&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">from&lt;/span> &lt;span style="color:#000">sklearn.model_selection&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">train_test_split&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>&lt;span style="color:#000">GridSearchCV&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">from&lt;/span> &lt;span style="color:#000">sklearn.neural_network&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">MLPClassifier&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">from&lt;/span> &lt;span style="color:#000">sklearn&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">metrics&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This code will read a csv file using pandas. Before running Cell 2 which immediately follows this paragraph, the user
should upload the csv file to the Google Drive of the same Google account which is running the notebook in Colab. The
csv in Cell 2 is titled &lt;code>kag_risk_factors_cervical_cancer&lt;/code> but please rename it accordingly to match the file
that you would like to upload.&lt;/p>
&lt;p>Cell 2:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#204a87;font-weight:bold">from&lt;/span> &lt;span style="color:#000">google.colab&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">drive&lt;/span>
&lt;span style="color:#000">drive&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">mount&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;/content/gdrive&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">force_remount&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#3465a4">True&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># The next line of code will tell Colab to read kag_risk_factors_cervical_cancer.csv in your Drive (not in any subfolders)&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># so you should alter the code to match whichever .csv you would like to upload.&lt;/span>
&lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">pd&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">read_csv&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;gdrive/My Drive/kag_risk_factors_cervical_cancer.csv&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># The next two lines of code convert question marks to NaN and converts values to numeric type, consider &lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># removing the next two lines if not necessary.&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;?&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">np&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">nan&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">apply&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">pd&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">to_numeric&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># If this cell successfully runs then it should output the first five rows, as requested in the next line of code&lt;/span>
&lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">head&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">5&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Colab will ask you to click on a blue link and to sign in with your account. Once done, the user must copy a code
and paste it into the box on Colab for authentication purposes. Press &lt;code>Enter&lt;/code> after pasting it into the box.&lt;/p>
&lt;p>If it outputs an error along the lines of &amp;ldquo;unknown directory&amp;rdquo; then try rerunning the two cells and ensuring that
your csv is not in any folders inside of Drive. You can also alter the code to point it to a subdirectory, if needed.&lt;/p>
&lt;h2 id="read-file-from-direct-upload">Read File from Direct Upload&lt;/h2>
&lt;p>Credit to Carlos Theran for creating this code and troubleshooting&lt;/p>
&lt;p>Cell 1:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">numpy&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">np&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">pandas&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">pd&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">matplotlib.pyplot&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">plt&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">seaborn&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">sns&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">tensorflow&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">tf&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">from&lt;/span> &lt;span style="color:#000">sklearn.model_selection&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">train_test_split&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>&lt;span style="color:#000">GridSearchCV&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">from&lt;/span> &lt;span style="color:#000">sklearn.neural_network&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">MLPClassifier&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">from&lt;/span> &lt;span style="color:#000">sklearn&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">metrics&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Cell 2:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#204a87;font-weight:bold">from&lt;/span> &lt;span style="color:#000">google.colab&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">files&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">files&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">upload&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>After running Cell 2, the user will be prompted to click &lt;code>Browse...&lt;/code> and to find the file on the user&amp;rsquo;s local
computer to upload. Sometimes trying to upload the file will give this error:&lt;/p>
&lt;p>&lt;code>MessageError: RangeError: Maximum call stack size exceeded.&lt;/code>&lt;/p>
&lt;p>&amp;hellip; in which case, the user should click the folder icon on the left side of Google Colab window, then the paper
with an arrow icon (to upload a file), then upload the csv you wish to use. Then rerunning Cell 2 is not
necessary, simply proceed to Cell 3. If this still does not work, see &lt;a href="https://stackoverflow.com/questions/53630073/google-colaboratory-import-data-stack-size-exceeded">this stackoverflow page&lt;/a> for further information.&lt;/p>
&lt;p>Cell 3:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">pd&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">read_csv&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;kag_risk_factors_cervical_cancer.csv&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># The next two lines of code convert question marks to NaN and converts values to numeric type, consider &lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># removing the next two lines if not necessary.&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;?&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">np&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">nan&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">apply&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">pd&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">to_numeric&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># If this cell successfully runs then it should output the first five rows, as requested in the next line of code&lt;/span>
&lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">head&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">5&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Remember to rename the instances of &lt;code>kag_risk_factors_cervical_cancer.csv&lt;/code> accordingly so that it matches your file name.&lt;/p></description></item><item><title>Docs: Adding a SSH Key for GitHub Repository</title><link>/docs/tutorial/git/git-ssh/</link><pubDate>Sat, 17 Jul 2021 00:00:00 +0000</pubDate><guid>/docs/tutorial/git/git-ssh/</guid><description>
&lt;p>Jacques Fleischer, Gregor von Laszewski&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>We present how to configure an SSH Key on GitHub so that you can clone, commit, pull, and push to repositories. SSH keys provide an easy way to authenticate to github. Together with ssh-agent and ssh-add it allows you to do multiple commtits without having to retype the password.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#documentation-for-linux-and-macos">Documentation for Linux and macOS&lt;/a>&lt;/li>
&lt;li>&lt;a href="#uploading-the-ssh-key">Uploading the SSH key&lt;/a>&lt;/li>
&lt;li>&lt;a href="#using-the-ssh-key">Using the ssh key&lt;/a>&lt;/li>
&lt;li>&lt;a href="#using-ssh-agent-and-ssh-add">Using ssh-agent and ssh-add&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ssh&lt;/p>
&lt;h2 id="documentation-for-linux-and-macos">Documentation for Linux and macOS&lt;/h2>
&lt;p>Please follow the Windows documentation, but instaed of using gitbash, pleas use the regular terminal. on macOS, make sure you have xcode installed.&lt;/p>
&lt;h2 id="uploading-the-ssh-key">Uploading the SSH key&lt;/h2>
&lt;p>Please ensure that you have Git (Git Bash) and a repository on GitHub. This tutorial assumes you already have a GitHub repository as well as a GitHub account.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Open Git Bash by pressing the Windows key, typing &lt;code>git bash&lt;/code>, and pressing Enter.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Then, go on GitHub, click on your profile icon in the top right, click &lt;code>Settings&lt;/code>, and click &lt;code>SSH and GPG keys&lt;/code> on the left hand side. Confirm that there are no SSH keys associated with your account. If there are keys, then perhaps you have made some already. This tutorial focuses on creating a new one.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Go back to Git Bash and type &lt;code>ssh-keygen&lt;/code>. Press &lt;code>Enter&lt;/code>. Press &lt;code>Enter&lt;/code> again when it asks you the file in which to save the key (it should say &lt;code>Enter file in which to save the key (/c/Users/USERNAME/.ssh/id_rsa):&lt;/code>.&lt;/p>
&lt;ol>
&lt;li>If you have already created a key here, it will ask you if you would like to overwrite the file. Type &lt;code>y&lt;/code> and press &lt;code>Enter&lt;/code>.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Enter a password that you will remember for your SSH key. It will not appear as you type it, so make sure you get it right the first time. Press &lt;code>Enter&lt;/code> after typing the password that you come up with.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>After seeing the randomart image associated with your SSH, you should be able to type a new command. Type &lt;code>cat ~/.ssh/id_rsa.pub&lt;/code> and press &lt;code>Enter&lt;/code>. Your key will appear— remember that this should not be shared with others. The key begins with &lt;code>ssh-rsa&lt;/code> and it may end with your username. Copy this entire key by clicking and dragging over it, right-clicking, and clicking &lt;code>Copy&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Return to your web browser which is on the GitHub SSH key settings page. Click the green button that reads &lt;code>New SSH Key&lt;/code> and type a Title for this key. You should name it something memorable and distinct; for example, if you just generated the key on your desktop computer, a suitable name is &lt;code>Desktop&lt;/code>. If generated on your laptop, name it &lt;code>Laptop&lt;/code>, or if you have numerous laptops, differentiate them with distinct names, and so on.&lt;/p>
&lt;ol>
&lt;li>If you only have one computer and you have preexisting keys on this page, maybe some which you do not remember the password to or have fallen out of use, consider deleting them (as long as you are sure this will not break anything).&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Paste the key into the key box. You should have copied it from Git Bash in Step #5. Then, click the green button that reads &lt;code>Add SSH key&lt;/code>. Congratulations— you have successfully configured your SSH key.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="using-the-ssh-key">Using the ssh key&lt;/h2>
&lt;p>Now we will try cloning a repository. We use as an example a repository that we created for a student from a REU. Your example may be different. please adjust the repository name. Your repository will have a format of &lt;code>xxxx-reu-xxx&lt;/code>&lt;/p>
&lt;ol start="8">
&lt;li>
&lt;p>Navigate to your repository and &lt;code>cd&lt;/code> into it. (In case of the REU we recommend to place it into a directory called &lt;code>cybertraining-dsc&lt;/code>.&lt;/p>
&lt;pre>&lt;code>$ mkdir ~/Descktop/cybertraining-dsc
$ cd cybertraining-dsc
$ git clone git@github.com:cybertraining-dsc/YOURREPONAME.git
&lt;/code>&lt;/pre>&lt;p>and replace YOURREPONAME with the name of your repository&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Alternatively you can download it via the GitHub Web GUI. Once you are on your repository page, click the green button that reads &lt;code>Code&lt;/code> with a download symbol. Click the &lt;code>SSH&lt;/code> option and click on the clipboard next to the link so that you copy it. It should say &lt;code>Copied!&lt;/code> after you click on it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Decide where you want your repository folder to be stored. This tutorial will clone the repo into the Documents folder. Go back to Git Bash and type
&lt;code>cd ~/Desktop/cybertraining-dsc&lt;/code> and press &lt;code>Enter&lt;/code>. It is a good idea to create a folder titled &lt;code>reu&lt;/code> for organization. Type &lt;code>mkdir reu&lt;/code> and press &lt;code>Enter&lt;/code>. Type &lt;code>cd reu&lt;/code> and press &lt;code>Enter&lt;/code>. Finally, type &lt;code>git clone&lt;/code>, and after you put a space after clone, paste the copied link from GitHub. For example, your command should look similar to this: &lt;code>git clone git@github.com:cybertraining-dsc/su21-reu-361.git&lt;/code> Then, press &lt;code>Enter&lt;/code>.&lt;/p>
&lt;ol>
&lt;li>The shortcut &lt;code>Ctrl + V&lt;/code> does not work in Git Bash for pasting. Instead, you can press &lt;code>Shift + Insert&lt;/code> to paste.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Type in your password for your SSH key and press &lt;code>Enter&lt;/code>. The repo should clone with no issue. You can now type &lt;code>code .&lt;/code> and press &lt;code>Enter&lt;/code> to open VSCode in this directory. Click &lt;code>Yes, I trust the authors&lt;/code> if prompted in VSCode. If you use PyCharm instead of VSCode, you can open it from Windows search; inside of PyCharm, click &lt;code>File&lt;/code>, &lt;code>Open...&lt;/code> and then navigate to &lt;code>C:&lt;/code>, &lt;code>Users&lt;/code>, your username, &lt;code>Documents&lt;/code>, and then click on &lt;code>reu&lt;/code> so it is highlighted in blue and then click &lt;code>OK&lt;/code>. If PyCharm asks, you can choose to open it in &lt;code>This Window&lt;/code> or a &lt;code>New Window&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="using-ssh-agent-and-ssh-add">Using ssh-agent and ssh-add&lt;/h2>
&lt;p>If you do not want to always type in your password you can prior to the first commit in the termnal in which you issue the commits say&lt;/p>
&lt;pre>&lt;code>$ eval `ssh-agent`
$ ssh-add
&lt;/code>&lt;/pre></description></item><item><title>Docs: Tutorial on Using venv in PyCharm</title><link>/docs/tutorial/pycharm/</link><pubDate>Fri, 16 Jul 2021 00:00:00 +0000</pubDate><guid>/docs/tutorial/pycharm/</guid><description>
&lt;p>Jacques Fleischer&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This tutorial teaches how to set PyCharm to use a venv.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#windows">Windows&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> venv&lt;/p>
&lt;h2 id="windows">Windows&lt;/h2>
&lt;p>Please ensure that you have Git (Git Bash), Python, and PyCharm. If you do not have those, look for the tutorials to install them.&lt;/p>
&lt;p>This tutorial was created with the REU program in mind, where the students are provided with a GitHub repository. If you are not in REU, then you can create a new repository on GitHub and clone that instead.&lt;/p>
&lt;p>Click the following image to be redirected to a YouTube video tutorial for setting venv in PyCharm. Please keep in mind that this video follows directions that are somewhat different from the written instructions below. REU students should follow the written instructions over the video. Otherwise, in the video, you should skip to timestamp 8:19 unless you do not have Git or a venv, in which case you should watch the entire video.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/HCotEx_xCfA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>If you have not already cloned your reu repository, you need to follow a separate tutorial which involves setting up your SSH key on GitHub, which can be found &lt;a href="https://github.com/cybertraining-dsc/cybertraining-dsc.github.io/blob/main/content/en/docs/tutorial/git/git-ssh/index.md">here&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Open PyCharm. If this is your first time opening PyCharm, then it will say &lt;code>Welcome to PyCharm&lt;/code>. You should have cloned your repo to a particular location on your computer; click &lt;code>Open&lt;/code> and then locate your reu folder. Once you have found it, click on it so it is highlighted in blue and then click &lt;code>OK&lt;/code>. Alternatively, if you have used PyCharm before, your previous project should open, in which case you should click &lt;code>File&lt;/code> and &lt;code>Open...&lt;/code> to open your repo (if it is not already open).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Please ensure that you have already configured a venv through Git Bash. If you have not, then read and follow &lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/blob/main/tutorials/python/venv.md">this tutorial&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In the top-right of PyCharm, click on the button that reads &lt;code>Add Configuration...&lt;/code>. Click &lt;code>Add new...&lt;/code> on the left underneath &lt;code>No run configurations added.&lt;/code> and then scroll and click &lt;code>Python&lt;/code>. Give this a name; you can just type &lt;code>Python venv&lt;/code>. Next to &lt;code>Python interpreter&lt;/code>, choose &lt;code>Python x.x (ENV3)&lt;/code>. The &lt;code>x.x&lt;/code> will depend on which version of Python you have. Then click &lt;code>OK&lt;/code>.&lt;/p>
&lt;ol>
&lt;li>The button might not read &lt;code>Add Configuration...&lt;/code>. If you have configured a run configuration previously, then you can create a new one. Click the button right next to the green play button in the top-right of PyCharm. Then, it should say &lt;code>Edit Configurations...&lt;/code> which you must click on. Change the Python interpreter to be the &lt;code>ENV3&lt;/code> one, as outlined in Step #4.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>You also have to click &lt;code>Python x.x&lt;/code> in the bottom-right of PyCharm, next to &lt;code>main&lt;/code>. From there, choose &lt;code>Python x.x (ENV3)&lt;/code>. To verify that your virtual environment is working, click on &lt;code>Terminal&lt;/code> in the bottom-left of PyCharm. Click the &lt;code>+&lt;/code> (plus) icon next to Local to start a new terminal. It should say &lt;code>(ENV3)&lt;/code> next to your current working directory. Congratulations!&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Report: Tutorial on Getting PyCharm Professional for Free</title><link>/report/su21-reu-361/tutorials/pycharm/</link><pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-361/tutorials/pycharm/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This tutorial teaches how to get PyCharm Professional for free on Windows 10 using a university email address.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#steps">Steps&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> pycharm&lt;/p>
&lt;h2 id="steps">Steps&lt;/h2>
&lt;p>Click the following image to be redirected to a YouTube video tutorial for installing PyCharm Professional.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/QPESX-VBnEU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Please ensure that you have a university or college email before proceeding.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Open up a web browser and search &lt;code>pycharm&lt;/code>. Look under the link from &lt;code>jetbrains.com&lt;/code> and click &lt;code>Download Pycharm&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click the blue button that reads &lt;code>Download&lt;/code> under Professional. Wait for the download to complete.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Open the completely downloaded file and click &lt;code>Yes&lt;/code> on the UAC prompt.&lt;/p>
&lt;ol>
&lt;li>If you have a school computer, please refer to the note under step 5 in the Python tutorial found here:
&lt;a href="https://cybertraining-dsc.github.io/report/su21-reu-361/tutorials/python/">https://cybertraining-dsc.github.io/report/su21-reu-361/tutorials/python/&lt;/a>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Next&lt;/code>, click &lt;code>Next&lt;/code> again, and check the box that reads &lt;code>Add launchers dir to the PATH&lt;/code>. You can also
create a Desktop Shortcut and create the &lt;code>.py&lt;/code> association, if you would like. The association changes which program,
by default, opens &lt;code>.py&lt;/code> files on your computer.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Next&lt;/code> and then click &lt;code>Install&lt;/code>. Wait for the green progress bar to complete. Then, you must restart your
computer after making sure all of your programs are saved and closed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Open PyCharm either by clicking on the Desktop shortcut you might have made, or hit the Windows key and type
&lt;code>PyCharm&lt;/code> and choose the program from the search results.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check the box that says &lt;code>I confirm that I have read and accept the terms...&lt;/code> after reading through each and every
word and fully committing every character on your screen to memory. Only if you want to!&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Continue&lt;/code>. You can choose to send anonymous statistics, if you want to; click the option you want.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click the hyperlink that says &lt;code>Buy license&lt;/code> in the top right of the window. Do not worry— you will not be spending
a cent.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click the person icon in the top right of the page (if you cannot find this person icon, then click this link
and hopefully it still works: &lt;a href="https://account.jetbrains.com/login">https://account.jetbrains.com/login&lt;/a> ).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create a JetBrains account by entering your university email address. Click &lt;code>Sign Up&lt;/code> after entering your email;
then, you have to go on your email and confirm your account in the automated email sent to you. Click &lt;code>Confirm your account&lt;/code> in the email.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Complete the registration form and click &lt;code>Submit&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Apply for a free student or teacher license&lt;/code>. Scroll down and click the blue button that reads &lt;code>Apply now&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Fill out the form, using your university email address and real name. Check the boxes if they apply to you. Then
click &lt;code>APPLY FOR FREE PRODUCTS&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>JetBrains should send you an automated email, ideally informing you that your information has been confirmed and
you have been granted a free license. If it does not immediately arrive, wait a few minutes. Go back to PyCharm and
sign in with your JetBrains account after receiving this email. Click &lt;code>Activate&lt;/code>. Congratulations!&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Report: Tutorial on Getting Raw Images on GitHub</title><link>/report/su21-reu-361/tutorials/github/</link><pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-361/tutorials/github/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This tutorial teaches how to get raw images on GitHub to post on your index.md repo (without any errors).&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#steps">Steps&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> github&lt;/p>
&lt;h2 id="steps">Steps&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Upload your image to GitHub in the images directory&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click on the image file and then right click on it and click &lt;code>Open image in new tab&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Use the URL shown in the address bar of the new tab to paste into index.md&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Report: Tutorial on Installing Python</title><link>/report/su21-reu-361/tutorials/python/</link><pubDate>Thu, 24 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-361/tutorials/python/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This tutorial teaches how to install Python on Windows 10.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#windows">Windows&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mac">Mac&lt;/a>&lt;/li>
&lt;li>&lt;a href="#linux">Linux&lt;/a>&lt;/li>
&lt;li>&lt;a href="#troubleshooting">Troubleshooting&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#incorrect-python-version-on-command-prompt">Incorrect Python Version on Command Prompt&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> python&lt;/p>
&lt;h2 id="windows">Windows&lt;/h2>
&lt;p>Click the following image to be redirected to a 2-minute YouTube walkthrough.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/T6UYyu5XVMc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>First, open up any web browser. This tutorial utilizes Google Chrome, but any other browser should work as long as it is not a 1990s version of Netscape. (Do not worry— you probably don&amp;rsquo;t have this.) The browser of choice can be Microsoft Edge, Firefox, Opera— as long as it can perform a search on a search engine, access a webpage, and download a file.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Open your browser by clicking the search box in the bottom left of your screen, where it says &amp;ldquo;Type here to search&amp;rdquo;. Then, type &amp;ldquo;google chrome&amp;rdquo; (or whatever is the name of the browser you use) and click it once it appears.&lt;/p>
&lt;ol>
&lt;li>The &amp;ldquo;Type here to search&amp;rdquo; box could be missing if you have customized your taskbar (the taskbar is the long box typically located on the bottom of your screen which has icons). In this case, just click the Windows logo in the bottom left and type your browser name.&lt;/li>
&lt;li>This is just one way to open your browser. You can even click a shortcut to your web browser on your taskbar, on your Desktop, or your Start Menu. In computing, there is typically many ways to accomplish the same end objective.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Once your browser has loaded, search for &amp;ldquo;python&amp;rdquo; on Google or any search engine. Click the result that reads &amp;ldquo;Downloads&amp;rdquo; from the website &amp;ldquo;python.org&amp;rdquo;.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>As of June 2021, the latest version of Python is &lt;code>3.9.5&lt;/code>. You may see a different number. As long as you click the button under &amp;ldquo;Download the latest version for Windows&amp;rdquo;, this will work. Try it now.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the download has completed, open the file by clicking on it in your Downloads pane.&lt;/p>
&lt;ol>
&lt;li>If you are utilizing a school-issued computer, you may be prevented from opening this .exe file because you are not the administrator. Please email or otherwise get in contact with your instructor, professor, or head of IT to discuss installing Python.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Be sure to check the box that reads &amp;ldquo;Add Python x.x to PATH&amp;rdquo;. This will allow you to run commands from the terminal/command prompt.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &amp;ldquo;Install Now&amp;rdquo;. The default options that entail this selection are appropriate for this experiment&amp;rsquo;s intents and purposes; choosing &amp;ldquo;Customize installation&amp;rdquo; may create reproducibility issues down the road, so please select &amp;ldquo;Install Now&amp;rdquo; instead.&lt;/p>
&lt;ol>
&lt;li>The UAC prompt will pop up. UAC stands for &amp;ldquo;User Account Control&amp;rdquo; and exists so that the computer will not have unauthorized changes performed on it. Click &amp;ldquo;Yes&amp;rdquo; because Python is safe. School-issued computers may ask for an administrator password, so refer to step 5&amp;rsquo;s sidenote.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Sit back and watch the green progress bar, whose speed will depend on the power of the computer.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>If the setup was successful, then it will say so. Click &amp;ldquo;Close&amp;rdquo;.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click the &amp;ldquo;Type here to search&amp;rdquo; box in the bottom-left of the screen, type &amp;ldquo;cmd&amp;rdquo;, and press Enter.&lt;/p>
&lt;ol>
&lt;li>An alternative method is to press the Windows key and the &amp;ldquo;R&amp;rdquo; key at the same time, type &amp;ldquo;cmd&amp;rdquo;, and press Enter. This is convenient for those who like to use the keyboard.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Type &lt;code>python --version&lt;/code> and the output should read &amp;ldquo;Python x.x.x&amp;rdquo;; as long as it is the latest version from the website, congratulations. Python is installed on the computer.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="mac">Mac&lt;/h2>
&lt;p>Click the following image to be redirected to a 5-minute YouTube walkthrough. (Yes, Mac&amp;rsquo;s video is a little longer, but do not fret!
You can skip to the 1:00 minute mark if you are in a hurry.)&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/TttmzM-EDmk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>Open a web browser that is able to search and download a file. This tutorial uses Google Chrome for Mac.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type in &lt;code>python&lt;/code> in the address bar and press enter. It should perform a search on your default search engine.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Look for the result that is from &lt;code>python.org&lt;/code>. Click on the subresult that says &lt;code>Downloads&lt;/code>.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Underneath &lt;code>Download the latest version for Mac OS X&lt;/code>, there should be a yellow button that reads &lt;code>Download Python x.x.x&lt;/code>. Click on it, and the download should commence.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the download finishes, open it by clicking on it. The installer will open. Click &lt;code>Continue&lt;/code>, click &lt;code>Continue&lt;/code> again, click &lt;code>Continue&lt;/code> again, oh my goodness!
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Agree&lt;/code>.&lt;/p>
&lt;ol>
&lt;li>If you want to check how much free storage you have on your computer, click the Apple icon in the top left of your computer. Click
&lt;code>About This Mac&lt;/code> and then click on &lt;code>Storage&lt;/code>. As of July 2021, Python takes ~120 MB of space. Remember that 1 GB = 1000 MB.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Install&lt;/code>. Enter your password and press Enter. Watch the blue progress bar crawl like a turtle&amp;hellip; or blast off at the speed of sound! This depends on your computer speed.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>A Finder window will open. You can close it as it is unnecessary. Click &lt;code>Close&lt;/code> in the bottom-right of the installer. Click &lt;code>Move to Trash&lt;/code> because you do not need the installer anymore.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Time to confirm that Python installed correctly. Click the magnifying glass in the top-right of your screen and then type &lt;code>terminal&lt;/code> into Spotlight Search. Double-click &lt;code>Terminal&lt;/code>.&lt;/p>
&lt;ol>
&lt;li>The terminal will be used frequently in this experiment. Consider keeping it in the dock for convenience. Click and hold the Terminal in the dock, go to &lt;code>Options&lt;/code>, and click &lt;code>Keep in Dock&lt;/code>.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Type &lt;code>python3 --version&lt;/code> into the terminal and press Enter. It should output the latest version of Python. Congratulations!
&lt;br>
 &lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="linux">Linux&lt;/h2>
&lt;p>Click the following image to be redirected to a 9-minute YouTube walkthrough. (Linux&amp;rsquo;s tutorial is the longest, but it is worth it.)
This tutorial uses Ubuntu, but it should work on other Linux distros, as well.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/cRp_ScANL1w" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>Open a web browser. It can be any browser as long as it can perform a search and navigate to a webpage.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Search for &lt;code>python&lt;/code> by typing it into the address bar and pressing enter. Click on &lt;code>Downloads&lt;/code> underneath the result from &lt;code>https://www.python.org&lt;/code>.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Look at the latest version. It is on the yellow button: &lt;code>Download Python x.x.x&lt;/code>. You do not need to click this button. Remember this version number.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Open a terminal by pressing the Windows key, or by clicking the grid on the bottom left of your screen. Type &lt;code>terminal&lt;/code>. Click on the &lt;code>Terminal&lt;/code> result that appears.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type &lt;code>sudo apt-get update&lt;/code> and press Enter. Wait for it to finish. It may already be up-to-date.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type &lt;code>sudo apt-get install libssl-dev openssl make gcc&lt;/code> and press Enter. This will install the libraries required to connect to an FTP to download Python. Type your password for your Linux user account, if prompted, and press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>You are then asked if you are okay with a certain amount of disk space being taken up. Type &lt;code>y&lt;/code>, which stands for Yes, and then press Enter.&lt;/p>
&lt;ol>
&lt;li>If you want to check how much disk space you have, press the Files icon on the left (on the taskbar) and click &lt;code>Other Locations&lt;/code>. You may have to scroll down on the sidebar in order to see it. It should say how much GB is available. Remember, 1 GB = 1000 MB and 1 MB = 1000 KB.
&lt;br>
 &lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>After this finishes, type &lt;code>cd /opt&lt;/code> and press Enter. Then, remember which version you read on the Python webpage (the latest version). Type &lt;code>sudo wget https://www.python.org/ftp/python/x.x.x/Python-x.x.x.tgz&lt;/code> after replacing the &lt;code>x.x.x&lt;/code> with the latest Python version number. As of July 2021, it is &lt;code>3.9.6&lt;/code>. Press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Wait for the download to complete. Then, type &lt;code>sudo tar xzvf Python-x.x.x.tgz&lt;/code> after you replace &lt;code>x.x.x&lt;/code> with the latest Python version number. Press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type &lt;code>cd Python-x.x.x&lt;/code> after replacing &lt;code>x.x.x&lt;/code> with the latest version number. Type &lt;code>./configure&lt;/code> and press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once it finishes, type &lt;code>make&lt;/code> and press Enter. Once &lt;em>that&lt;/em> finishes, type &lt;code>sudo make install&lt;/code> and press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the installation finishes, type &lt;code>sudo ln -fs /opt/Python-x.x.x/Python /usr/bin/pythonx.x&lt;/code>. Notice that &lt;code>x.x.x&lt;/code> should be replaced with the full version number and &lt;code>x.x&lt;/code> should have the first two numbers in the version number. Press Enter.
&lt;br>
 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Confirm Python&amp;rsquo;s successful installation by typing &lt;code>pythonx.x --version&lt;/code>; be sure to replace x.x with the first two numbers of the version number. It should output the latest version number. Congratulations!
&lt;br>
 &lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Credit to bobbyiliev for making the required commands publicly available. The commands are available here, as well: &lt;a href="https://www.digitalocean.com/community/questions/how-to-install-a-specific-python-version-on-ubuntu">https://www.digitalocean.com/community/questions/how-to-install-a-specific-python-version-on-ubuntu&lt;/a>
&lt;br>
 &lt;/p>
&lt;h2 id="troubleshooting">Troubleshooting&lt;/h2>
&lt;h3 id="incorrect-python-version-on-command-prompt">Incorrect Python Version on Command Prompt&lt;/h3>
&lt;p>If the Windows computer has installed an older version of Python, running &lt;code>python --version&lt;/code> on Command Prompt may output an older version. Typing &lt;code>python3 --version&lt;/code> may output the correct, latest version.&lt;/p></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-367/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-367/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-367/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-367/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-367/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-367/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Firstname Lastname, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-367">su21-reu-367&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-367/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-368/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-368/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-368/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-368/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-368/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-368/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Yohn Jairo Parra Bautista, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-368">su21-reu-368&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-368/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-373/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-373/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-373/actions">&lt;img src="https://github.com/su21-reu-373/hid-example/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-373/actions">&lt;img src="https://github.com/su21-reu-373/hid-example/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Firstname Lastname, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-373">su21-reu-373&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-373/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Pros and Cons of Artificial Intelligence for Breast Cancer Detection in Women</title><link>/report/su21-reu-377/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-377/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-377/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-377/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-377/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-377/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>RonDaisja Dunn, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-377">su21-reu-377&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-377/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Breast Cancer is one of the most dangerous type of disease that affects many women. For detecting Breast Cancer, machine learning techniques are applied to improve the accuracy of diagnosis. We collected data from African American women that have been diagnosed with Breast Cancer. Through the use of Artificial Intellingence, we will be able to detect the specific forms of Breast Cancer.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-methods-from-literature-review">2. Methods From Literature Review&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-results-from-literature-review">3. Results From Literature Review&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-datasets">4. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgments">6. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> project, reu, breast cancer, Artificial Intelligence, diagnosis detection, women, early detection, advantages, disadvantages&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The leading cause of cancer death in women worldwide is breast cancer. This deadly form of cancer has impacted many women across the globe. Specifically, African American women have been the most negatively impacted. Their death rates due to breast cancer have surpassed all other ethnicities. Serial screening is an essential part in detecting Breast cancer. Detecting the early stages of this disease and decreasing mortality rates is most effective by utilizing serial screening. Some women detect that they could have breast cancer by discovering a painless lump in their breast. Other women began to detect that there may be a problem due to annual and bi-annual breast screenings. Screening in younger women is not likely, because breast cancer is most likely to be detected in older women. Women from the age 55 to 69 are likely to be diagnosed with breast cancer. Women who frequently participate in receiving mammograms reduce the chance of breast cancer mortality.&lt;/p>
&lt;p>Artificial Intelligence is the branch of computer science dedicated to the development of computer algorithms to accomplish tasks traditionally associated with human intelligence, such as the ability to learn and solve problems. This branch of computer science coincides with diagnosing breast cancer in individuals because of the use of radiology. Radiological images can be quantitated and can inform and train some algorithms. There are many terms that relate to Artificial Intelligence such as artificial neural networks (ANNs), machine and deep learning (ML, DL). These techniques complete duties in healthcare, including radiology. Machine learning interprets pixel data and patterns from mammograms. Benign or malignant features for inputs are defined by microcalcifications. Deep learning is effective in breast imaging, where it can identify several features such as edges, textures, and lines. More intricate features such as organs, shapes, and lesions can also be detected. Neural networks algorithms are used for image feature extractions that cannot be detected beyond human recognition.&lt;/p>
&lt;p>A computer system that can perform complicated data analysis and picture recognition tasks is known as artificial intelligence (AI). Both massive processing power and the application of deep learning techniques made this possible, and are increasingly being used in the medical field. Mammograms are the x-rays used to detect breast cancer in women. Early detection is important to reduce deaths, because that is when the cancer is most treatable. Screenings have presented a 15%-35% false report in screened women. Errors and the ability to view the cancer from the human eye are the reasons for the false reports. Artificial Intelligence offers many advantages when detecting breast cancer. These advantages include less false reports, fewer cases missed because the AI program does not get tired and it reduces the effort of reading thousands of mammograms.&lt;/p>
&lt;h2 id="2-methods-from-literature-review">2. Methods From Literature Review&lt;/h2>
&lt;p>The goal was to emphasize the present data in terms of test accuracy and clinical utility results, as well as any gaps in the evidence. Women are screened by getting photos taken of each breast from different views. Two readers are assigned to interpret the photographs in a sequential order. Each reader decides whether the photograph is normal or whether a woman should be recalled for further examination. Arbitration is used when there is a disagreement. If a woman is recalled, she will be offered extra testing to see if she has cancer.&lt;/p>
&lt;p>Another goal is to detect cancer at an earlier stage during screening so that therapy can be more successful. Some malignancies found during screening, on the other hand, might never have given the woman symptoms. Overdiagnosis is a term used to describe a situation in which a person has caused harm to another person during their lifetime. As a result, overtreatment (unnecessary treatment) occurs. Since some malignancies are overlooked during screening, the women are misled.&lt;/p>
&lt;p>The methods in diagnostic procedures vary between radiologists and Artificial Intelligence networks. In a breast ultrasound exam, radiologists look for abnormal abnormalities in each image, while AI networks analyze each image in an exam that is processed separately using a ResNet-18 model, and a saliency map is generated, identifying the most essential sections. With radiologists, the focus is on photos with abnormal lesions and with AI networks the image is given an attention score based on its relative value. To make a final diagnosis, radiologists consider signals in all photos, and AI computes final predictions for benign and malignant results by combining information from all photos using an attention technique.&lt;/p>
&lt;h2 id="3-results-from-literature-review">3. Results From Literature Review&lt;/h2>
&lt;p>Using pathology data, each breast in an exam was given a label indicating the presence of cancer. Image-guided biopsy or surgical excision were used to collect tissues for pathological tests. The AI system was shown to perform comparably to board-certified breast radiologists in the reader study subgroup. In this reader research, the AI system detected tumors with the same sensitivity as radiologists, but with greater specificity, a higher PPV, and a lower biopsy rate. Furthermore, the AI system outperformed all ten radiologists in terms of AUROC and AUPRC. This pattern was replicated in the subgroup study, which revealed that the algorithm could correctly interpret Ultrasound Screening examinations that radiologists considered challenging.&lt;/p>
&lt;p>Figure -
Analysis of saliency maps on a qualitative level- This figure displays the sagittal and transverse views of the lesion (left) and the AI&amp;rsquo;s saliency maps indicating the anticipated sites of benign (center) and malignant (right) findings in each of the six instances (a-f) from the reader study.&lt;/p>
&lt;h2 id="4-datasets">4. Datasets&lt;/h2>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-377/main/project/images/Dataset%20Image.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> The probabilistic forecasts of each hybrid model were randomly divided to fit the reader&amp;rsquo;s sensitivity. The dichotomization of the AI&amp;rsquo;s predictions matches the sensitivity of the average radiologists. Readers' AUROC, AUPRC, specificity, and PPV improve as a result of the collaboration between AI and readers, whereas biopsy rates decrease.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>There are some benefits of AI help with mammogram screenings. The reduction in treatment expenses is one of the advantages of screening. Treatment for people who are diagnosed sooner is less invasive and expensive, which may lessen patient anxiety and improve their prognosis. One or all human readers could be replaced by AI. AI may be used to pre-screen photos, with only the most aggressive ones being reviewed by humans. AI could be employed as a reader aid, with the human reader relying on the AI system for guidance during the reading process.&lt;/p>
&lt;p>However, there is also fear that AI could discover changes that would never hurt women. Because the adoption of AI systems will alter the current screening program, it&amp;rsquo;s crucial to determine how accurate AI is in breast screening clinical practice before making any changes. It&amp;rsquo;s uncertain how effective AI is at detecting breast cancer in different sorts of women or in different groups of women (for example different ethnic groups). AI could significantly minimize staff workload, as well as the proportion of cancers overlooked during screening, and the amount of women who are asked to return for more tests despite the fact that they do not have cancer. According to the findings of the reader survey, such teamwork between AI systems and radiologists increases diagnosis accuracy and decreases false positive biopsies for all 10 radiologists. This research indicated that integrating the Artificial intelligence system&amp;rsquo;s predictions enhanced the performance of all readers.&lt;/p>
&lt;h2 id="6-acknowledgments">6. Acknowledgments&lt;/h2>
&lt;p>Thank you to the extremely intellectual, informative, patient and courteous instructors of the Research Experience for Undergraduates Program.&lt;/p>
&lt;ol>
&lt;li>Carlos Theran, REU Intructor&lt;/li>
&lt;li>Yohn Jairo Parra, REU Intructor&lt;/li>
&lt;li>Gregor von Laszewski, REU Instructor&lt;/li>
&lt;li>Victor Adankai, Graduate Student&lt;/li>
&lt;/ol>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p></description></item><item><title>Report: Project: This is the Descriptive Title of the Example</title><link>/report/su21-reu-379/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-379/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-379/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-379/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-379/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-379/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Carrington Kerr, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-379">su21-reu-379&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-379/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Automated Detection and Classification of Breast Cancer Subtypes using Machine Learning Algorithms</title><link>/report/su21-reu-362/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-362/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-362/actions">&lt;img src="https://github.com/cybertraining-dsc/hid-example/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-362/actions">&lt;img src="https://github.com/cybertraining-dsc/hid-example/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Kehinde Ezekiel, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-362">su21-reu-362&lt;/a>, [Edit](&lt;a href="https://github.com/cybertraining-dsc/su21-reu-">https://github.com/cybertraining-dsc/su21-reu-&lt;/a>
362/blob/main/project/index.md)&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Breast cancer is an heterogenous disease that forms in the cells of the breast and characterized by abnormal growth of the cells[^1]. Early diagnosis and detection of possible cancerous cells in the breast usually increase survival chances and provide a better approach for treatment and management. Artificial Intelligence involves the simulation of human intelligence in machines and can be used for learning or to solve problems. A major subset of AI is Machine Learning which involves training a piece of software (called model) to makwe useful predictions using dataset.&lt;/p>
&lt;p>In this project, a machine learning algotithm (kMeans) was used to classify a proteomic dataset into similar clusters using its proteins identifiers. The protein identifiers were associated with the PAM50genes. The project revealed that machine learning algorithm can be used to (WAIT FOR THE RESULT) and suggests the implementation of several algorithms that can be leveraged upon to address healthcare issues like breast cancer and other diseases that are charactized with subtypes. it also suggeests that studying the specific subtypes based on the algorithm has the potential in developing new therapies as well as effective management and treatment plan.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-dataset-in-the-project">3. Dataset in the Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-the-kmeans-approach">4. The KMeans Approach&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-using-images">5. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> AI, cancer, breast, algorithms, machine learning, healthcare, subtypes, classification.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Breast cancer is the most common cancer, and also the primary cause of mortality due to cancer in females around the World. It is an heterogenous disease that is characterized by the abnormal growth of cells in the breast region&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Early diagnosis and detection of possible cancerous cells in the breast usually increase survival chances and provide a better approach for treatment and management. Treatment and management often depend on the stage of cancr, the subtype, the tumor size, location and many other factors. Most times, breast cancer is diagnosed and detected through a combination of different approaches such as imaging (e.g. mammogram and ultrasound), physical examination by a radiologist and biopsy. Biopsy is used to confirm the breast cancer symptoms. However, research has shown that radiologists can miss up to 30% of breast cancer tissues during detection&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This gap has brought about the introduction of Computer aided Diagnosis (CAD) systems can help detect abnormalities in an efficient manner. CAD is a technology that includes utilizing the concept of artificial intelligence(AI) and medical image processing to find abnormal signs in the human body&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>During the last 20 years, four (4) major intrinsic molecular subtypes for breast cancer- luminal A, luminal B, HER2-enriched and Basal-like have been identified, classified and intensively studied. Each subtype has its distinct morphologies and clinical treatment. The classification is based on gene expression profiling, specificaly defined by mRNA expression of 50 genes (also known as, PAM50 Genes).The accurate grouping of breast cancer into its relevant subtypes can improve accurate treatment-decision making[^4.&lt;/p>
&lt;p>The PAM50 test is now known as he Prosigna Breast Cancer Prognostic Gene Signature Assay
50 (known ad Prosigna) and it analyzes the activity of certain genes in early-stage, hormone-receptor-positive breast cancer&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The classification of the breast cancer subtypes is based on the mRNA expression and the activity of 50 genes and it aims to estimate the rik of distanat reccurrence of breast cancer. Since the assay was based on mRNA expression, it was suggested that a classification based on the final product of mRNA, that is protien, can be implemented to investigate its role in the classifictaion of molecular breast cancer subtypes. As a result, the project was focused on the use of a proteomic dataset which contained published iTRAQ proteome profiling of 77 breast cancer samples and expression values ofr the proteins of each sample. Machine learning algorithm was implemented to design and analyze the data into clusters and determine the relationship of the clusters with each subtype.&lt;/p>
&lt;p>Recently, Artificial intelligence methods, Machine Learning methods, image classifcation have been largely used for breast cancer classification. The advancements in the field of Machine Learning (ML) have led to more intelligent and self-reliant computer-aided diagnosis (CAD) systems, as the learning ability of ML methods has been constantly improving. A conventional ML method includes enhancement, feature extraction, segmentation, and classification&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This project provides an automated classification of breast cancer subtypes using KMeans- a type of Machine Learning algorithms. The scope of the project is to identify the molecular subtypes of breast cancer using the protein unique identifiers of the breast cancer genes and to investigate its efficacy with the established PAM50 assay.&lt;/p>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;p>Datasets are eseential in drawing conclusion. In the diagnosis, detection and classification of breast cancecr, datasets have been essential to draw conclusion to identifiable patterns. They range from imaging datasets to clinical datasets, proteomic datasets etc. Due to new technological and computational advances like NCBI, EEG that record clinical information respectively, large amounts of data have been collected. Medical researchers leverage these datasets to make useful health care decisions that affect a region, gender or the world. The need for accuracy and reproducibilty has led to the use of machine learning as an important tool for drawing conclusions.&lt;/p>
&lt;p>Machine Learning involves training a piece of software, also known as model, to idnetify patterns from a dataset and make useful predictions. There are several factors to be considered when using datasets. One of such is data privacy. Recently, measures have been taken to ensure that the privacy of data. Some of these measures include, replacing codes for patients name, using documents and mobile applications that ask for permission from patients before using their data. Recently, the World Health Organization (WHO) made her report on AI and provided priniples that ensure that AI works for all. On of such is that the designer of AI technologies should satisfy regulatory requirements for safety, accuracy and efficacy for well-defined use cases or indications. Measures of quality control in practice and quality improvement in the use of AI must be available&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Building a model using machine learning involves selecting and preparing the appropriate dataset, identifying the accurate machine learnning algorithm to use, training the algorithm on the data to build a model, validating the resulting model&amp;rsquo;s performance on testing data and using the model on a new data&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-dataset-in-the-project">3. Dataset in the Project&lt;/h2>
&lt;p>KMeans clustering is an unsupervised machine learning algorithm that makes inferences from datasets without referring to a known outcome. It aims to identify underlying patterns in a dataset by looking for a fixed number of clusters , (known as k). The required number of clusters is chosen by the person building the model. KMens was used in this project to classify the protein IDs (or RefSeq_IDs) into clusters. Each cluster was designed to identify related protein IDs.&lt;/p>
&lt;p>Three datasets were used for the algorithm. The first and main dataset was a proteomic dataset. It contains published iTRAQ proteome profiling of 77 breast cancer samples generated by the Clinical Proteomic Tumor Analysis Consortium (NCI/NIH). It contains expression values for ~12.000 proteins for each sample, with missing values present when a given protein could not be quantified in a given sample. The variables include the RefSeq_accession_number(also known as RefSeq protein ID), “the gene_symbol” (which is unique to each gene), “the gene_name” (which is the full name of the gene). The remaining columns are the log2 iTRAQ ratios for each of the 77 samples while the last three columns are from healthy individuals.&lt;/p>
&lt;p>The second dataset was a PAM50 dataset. It contains the list of genes and proteins used in the PAM50 classification system. The variables include the RefSeqProteinID which matches the Protein IDs in the main proteome dataset.&lt;/p>
&lt;p>The third dataset was a clinical data of about 105 clinical of which the 77 breast cancer samples were extracted from after a . The variables are:
‘Complete TCGA ID', &amp;lsquo;Gender&amp;rsquo;, &amp;lsquo;Age at Initial Pathologic Diagnosis&amp;rsquo;, &amp;lsquo;ER Status&amp;rsquo;, &amp;lsquo;PR Status&amp;rsquo;, &amp;lsquo;HER2 Final Status&amp;rsquo;, &amp;lsquo;Tumor&amp;rsquo;, &amp;lsquo;Tumor&amp;ndash;T1 Coded&amp;rsquo;, &amp;lsquo;Node&amp;rsquo;, &amp;lsquo;Node-Coded&amp;rsquo;, &amp;lsquo;Metastasis&amp;rsquo;, &amp;lsquo;Metastasis-Coded&amp;rsquo;, &amp;lsquo;AJCC Stage&amp;rsquo;, &amp;lsquo;Converted Stage&amp;rsquo;, &amp;lsquo;Survival Data Form&amp;rsquo;, &amp;lsquo;Vital Status&amp;rsquo;, &amp;lsquo;Days to Date of Last Contact&amp;rsquo;, &amp;lsquo;Days to date of Death&amp;rsquo;, &amp;lsquo;OS event&amp;rsquo;, &amp;lsquo;OS Time&amp;rsquo;, &amp;lsquo;PAM50 mRNA&amp;rsquo;, &amp;lsquo;SigClust Unsupervised mRNA&amp;rsquo;, &amp;lsquo;SigClust Intrinsic mRNA&amp;rsquo;, &amp;lsquo;miRNA Clusters&amp;rsquo;, &amp;lsquo;methylation Clusters&amp;rsquo;, &amp;lsquo;RPPA Clusters&amp;rsquo;, &amp;lsquo;CN Clusters&amp;rsquo;, &amp;lsquo;Integrated Clusters (with PAM50)&amp;rsquo;, &amp;lsquo;Integrated Clusters (no exp)&amp;rsquo;, &amp;lsquo;Integrated Clusters (unsup exp).&amp;rsquo;&lt;/p>
&lt;h2 id="4-the-kmeans-approach">4. The KMeans Approach&lt;/h2>
&lt;p>The link to the original study where the samples were used &lt;a href="http://www.nature.com/nature/journal/v534/n7605/full/nature18003.html">http://www.nature.com/nature/journal/v534/n7605/full/nature18003.html&lt;/a>
The referred kernals for the code &lt;a href="https://www.kaggle.com/shashwatwork/proteomes-clustering-analysis">https://www.kaggle.com/shashwatwork/proteomes-clustering-analysis&lt;/a> and &lt;a href="https://pastebin.com/A0Wj41DP">https://pastebin.com/A0Wj41DP&lt;/a>&lt;/p>
&lt;p>During the preparation of the datasets for KMeans analysis, the first and third dataset were merged together, while . The variable &amp;lsquo;Complete TCGA ID&amp;rsquo; in the third dataset was found to be the same as the TCGAs in the first dataset. The Complete TCGA ID refers to a breast cancer patient, some patients can be found in both datasets. The TCGA ID in the first dataset was renamed to match with the TCGA of the third dataset, thereby giving the same syntax. The first dataset was also transposed as a row and its gene expression as the columns. These processes were done in order to merge both dataset.&lt;/p>
&lt;p>After merging, the &amp;ldquo;PAM5O RNA&amp;rdquo; variable from the second dataset was selected to join the merged dataset. This single dataset was named &amp;ldquo;pam50data&amp;rdquo;. It contained all the variables that were needed for KMeans Analysis which included the 12553 unique genes, the complete TCGA ID of each 80 patient, and their molecular tumor type.&lt;/p>
&lt;p>Missing values were imputed using SimpleImputer, the number of clusters that works best were determined. Figures 1 and 2 reveals the result for KMeans clustering using 3 and 5 clusters respectively.&lt;/p>
&lt;p>The link to the full code for the algorithm &lt;a href="https://colab.research.google.com/drive/1ETvGu_cMFlATT28LrHdWgRFahBv9gStN#scrollTo=Qu6nLUiudcYa">https://colab.research.google.com/drive/1ETvGu_cMFlATT28LrHdWgRFahBv9gStN#scrollTo=Qu6nLUiudcYa&lt;/a>&lt;/p>
&lt;h2 id="5-using-images">5. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-362/blob/main/project/images/new.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> The classification of Breast Cancer Moleecular Subtypes using KMeans Clustering. (k=3)&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-362/blob/main/project/images/k%3D4_image.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> The classification of Breast Cancer Moleecular Subtypes using KMeans Clustering. (k=4)&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Status&lt;/th>
&lt;th>Time&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>parallel 1&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.647&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 3&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.936&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 5&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.952&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 7&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.943&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 9&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>1.002&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 11&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.991&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 13&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.958&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>parallel 15&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>1.012&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Benchmark:&lt;/strong> The table shows the parallel process time take the for loop for n_components.&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>The results of the KMeans analysis indicated that a cluster of 3 is better than a cluster of 4. Future research would be to use other machine learning algorithms, possibly a supervised learning algotithm, to identify the correlation between the clusters and the four molecular subtypes.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Gregor guided me through this process&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> REU Instructors - Carlos Theran, Yohn Jairo, Victor Adankai&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Jacques Fleischer, David Umanzor&lt;/li>
&lt;/ul>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Akram, Muhammad et al. &amp;ldquo;Awareness and current knowledge of breast cancer.&amp;rdquo; Biological research vol. 50,1 33. 2 Oct. 2017, doi:10.1186/s40659-017-0140-9&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>L. Hussain, W. Aziz, S. Saeed, S. Rathore and M. Rafique, &amp;ldquo;Automated Breast Cancer Detection Using Machine Learning Techniques by Extracting Different Feature Extracting Strategies,&amp;rdquo; 2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE), 2018, pp. 327-331, doi: 10.1109/TrustCom/BigDataSE.2018.00057.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Halalli, Bhagirathi et al. &amp;ldquo;Computer Aided Diagnosis - Medical Image Analysis Techniques.&amp;rdquo; 20 Dec. 2017, doi: 10.5772/intechopen.69792&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Breast Cancer.org Prosigna Breast Cancer Prognostic Gene Signature Assay. &lt;a href="https://www.breastcancer.org/symptoms/testing/types/prosigna">https://www.breastcancer.org/symptoms/testing/types/prosigna&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Gardezi, Syed Jamal Safdar et al. &amp;ldquo;Breast Cancer Detection and Diagnosis Using Mammographic Data: Systematic Review.&amp;rdquo; Journal of medical Internet research vol. 21,7 e14464. 26 Jul. 2019, doi:10.2196/14464&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>WHO, WHO issues first global report on Artificial Intelligence (AI) in health and six guiding principles for its design and use. &lt;a href="https://www.who.int/news/item/28-06-2021-who-issues-first-global-report-on-ai-in-health-and-six-guiding-principles-for-its-design-and-use">https://www.who.int/news/item/28-06-2021-who-issues-first-global-report-on-ai-in-health-and-six-guiding-principles-for-its-design-and-use&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Salod, Zakia, and Yashik Singh. &amp;ldquo;Comparison of the performance of machine learning algorithms in breast cancer screening and detection: A protocol.&amp;rdquo; Journal of public health research vol. 8,3 1677. 4 Dec. 2019, doi:10.4081/jphr.2019.1677Articles&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Detection of Autism Spectrum Disorder with a Facial Image using Artificial Intelligence</title><link>/report/su21-reu-378/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-378/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-378/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-378/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-378/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Myra Saunders, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-378">su21-reu-378&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-378/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Autism affects 1 in every 160 children worlwide. Early detection and diagnosis of Autism, along with treatment, is needed to minimize some of the difficulties that people with Autism encounter. Autism is usually diagnosed by a specialist through various Autism screening methnods. This can be an expensive and complex process. Many children that display signs of Autism go undiagnosed because there families lack the expenses needed to pay for Autism screening and diagnosing. Therefore, the development of a potential inexpensive, but accurate way to detect Autism in children is necessary for low-income families. With all the technological advances in Artificial Intelligence today, deep learning can be used to develop an effective method to detect Autism in children. This project utilizes a Convolutional Neural Network classifier to explore the possibility of using a facial image analysis to detect Autism in children. The facial images of Autistic and non-autistic male and female children will be used in this project.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Add a bit more&lt;/li>
&lt;/ul>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-related-work">2. Related Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-proposed-methodology">4. Proposed Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-convolutional-neural-network">4.1. Convolutional Neural Network&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-architecture">4.2. Architecture&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-implementation">5. Implementation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-future-work">8. Future Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-acknowledgments">9. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#11-references">11. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Autism Spectrum Disorder, Detection, Artificial Intelligence, Deep Learning, Convolutional Neural Network, Computer Vision.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Autism Spectrum Disorder (ASD) is a broad range of lifelong developmental and neurological disorders that usually appear during early childhood. Autism affects the brain and can cause challenges with speech and nonverbal communication, repetitive behaviors, and social skills. Autism Spectrum Disorder can occur in all socioeconomic, ethnic, and racial groups, and can usually be detected and diagnosed from the age of three years old and up. As of June 2021, the World Health Organization has estimated that one in 160 children have an Autism Spectrum Disorder worldwide&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Early detection of Autism, along with treatment, is crucial to minimize some of the difficulties and symptoms that people with Autism face&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Symptoms of Autism Spectrum Disorder are normally identified based on psychological criteria&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Specialists use techniques such as behaivoral observation reports, questionaires, and a review of the child&amp;rsquo;s cognitive ability to detect and diagose Autism in children.&lt;/p>
&lt;p>Many researchers believe that there is a correlation between facial morphology and Autism Spectrum Disorder, and that people with Autism have distinct facial features that can be used to detect their Autism Spectrum Disorder&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Human faces encode important markers that can be used to detect Autism Spectrum Disorder by analyzing facial features, eye contact,facial movements, and more&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. Scientists found that children diagnosed with Autism share common facial feature distinctions from children who are not diagnosed with Autism&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Some of these facial features are wide-set eyes, short middle region of the face, and a broad upper face. Image one provides an example of the facial feature differences between a child with Autism and a child without.&lt;/p>
&lt;p>Due to the distinct features of Autistic individuals, we believe that it is necessary to explore the possiblities of using a facial analysis to detect Autism in children, using Artificial Intelligence (AI). Many researchers have attempted to explore the possibility of using various novel algorithms to detect and diagnose children, adolescents, and adults with Autism&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Previous research has been done to determine if Autism Spectrum Disorder can be detected in children by analyzing a facial image&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. The author of this research collected approximately 1500 facial images of children with Autism from websites and Facebook pages associated with Autism. The facial images of non-autistic children were randomly downloaded from online and cropped.The author aimed to provide a first level screening for autism diagnosis, whereby parents could submit an image of their child and in return recieve a probability of the potential of Autism, without cost.&lt;/p>
&lt;p>To contribute to this previous research&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>, this project will propose a model that can be used to detect the presence of Autism in children based on a facial image analysis.
A deep learning algorithm will be used to develop an inexpensive, accurate, and effective method to detect Autism in children. This project implements and utilizes a Convolutional Neural Network (CNN) classifier to explore the possibility of using a facial image analysis to detect Autism in children. Most of the coding used for this CNN model was obtained from the Kaggle dataset and was done by Fran Valuch&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. We made changes to some parts of this code, which will be discussed further in this project. The goal of this project is not to diagnose Autism, but to explore the possibility of detecting Autism at its early stage, using a facial image analysis.&lt;/p>
&lt;h2 id="2-related-work">2. Related Work&lt;/h2>
&lt;p>Study 1:&lt;/p>
&lt;p>Study 2:&lt;/p>
&lt;p>Study 3:&lt;/p>
&lt;p>Study 4:&lt;/p>
&lt;p>Study 5:&lt;/p>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>The dataset used for this project was obtained from Kaggle&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. This dataset contained approximately 1500 facial images of children with Autism that were obtained from websites and Facebook pages associated with Autism. The facial images of non-autistic children were randomly downloaded from online. The pictures obtained were not of the best quality or consistency with respect to the facial alignment. Therefore, the author developed a python program to automatically crop the images to include only the extent possible for a facial image. These images consist of male and female children that are of different races and range from around ages two to fourteen.&lt;/p>
&lt;p>This project uses version 12 of this dataset, which is the latest version. The dataset consists of three directories labled test, train, and valid, along with a CSV file. The training set is labeled as train, and consists of &amp;lsquo;Autistic&amp;rsquo; and &amp;lsquo;Non-Autistic&amp;rsquo; subdirectories. These subdirectories contain 1269 images of autistic and 1269 images of non-autistic children respectively. The validation set located in the valid directory are separated into &amp;lsquo;Autistic&amp;rsquo; and &amp;lsquo;Non-autistic&amp;rsquo; subdirectories. These subdirectories also contain 100 images of autistic and 100 images of non-autistic children respectively. The testing set located in the test directory is divided into 100 images of autistic children and 100 images of non-autistic children. All of the images provided in this dataset are in 224 X 224 X 3, jpg format.&lt;/p>
&lt;p>&lt;strong>Image 1:&lt;/strong> Image of Child with Autism (left) and Child with no Autism (right)&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-378/main/project/images/Autistic%20compared%20with%20Non-Autistic%20(4).png" alt="Autistic and Non-Autistic Child">&lt;/p>
&lt;h2 id="4-proposed-methodology">4. Proposed Methodology&lt;/h2>
&lt;h3 id="41-convolutional-neural-network">4.1. Convolutional Neural Network&lt;/h3>
&lt;p>-[ ] Add info about CNN and how it will be used.&lt;/p>
&lt;h3 id="42-architecture">4.2. Architecture&lt;/h3>
&lt;p>&lt;strong>Image 2:&lt;/strong> Architecture of utilized Convolutional Neural Network Model.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-378/main/project/images/CNN%20Architecture.png" alt="CNN Architecture">&lt;/p>
&lt;h2 id="5-implementation">5. Implementation&lt;/h2>
&lt;p>-[ ] Add info about each figure.&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Plot Histogram (1).&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-378/main/project/images/Plot%20Histogram%20(1).png" alt="Plot Histogram (1)">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Plot Histogram (2).&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-378/main/project/images/Plot%20Histogram%20(2).png" alt="Plot Histogram (2)">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Training and Validation (1).&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-378/main/project/images/Training%20and%20Validation%20(1).png" alt="Training and Validation (1)">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Training and Validation (2).&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-378/main/project/images/Training%20and%20Validation%20(2).png" alt="Training and Validation (2)">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Correct Labels.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-378/main/project/images/Correct%20Labels.png" alt="Correct Labels">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Incorrect Labels.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-378/main/project/images/Incorrect%20Labels.png" alt="Incorrect Labels">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Confusion Matrix CNN.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-378/main/project/images/Confusion%20Matrix%20CNN.png" alt="Confusion Matrix CNN">&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> State if goal was met, then explain.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Talk about problems experienced/limitations.&lt;/li>
&lt;/ul>
&lt;h2 id="8-future-work">8. Future Work&lt;/h2>
&lt;p>Talk about what work will be done in the future.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Work on a model that can be used for children outside of the dataset age range, i.e teenagers.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Work on model that trains data for children of all colors, i.e people of color.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Continue working on model to achieve accuracy above 95%.&lt;/li>
&lt;/ul>
&lt;h2 id="9-acknowledgments">9. Acknowledgments&lt;/h2>
&lt;p>The author of this project would like to express a vote of thanks to Yohn Jairo, Carlos Theran, and Dr. Gregor von Laszewski for their encouragement and guidance throughout this project. A special vote of thanks also goes to Florida A&amp;amp;M University for funding this wonderful research program. The completion of this project could not have been possible without their support.&lt;/p>
&lt;h2 id="11-references">11. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>World Health Organization. 2021. Autism spectrum disorders, [Online resource] &lt;a href="https://www.who.int/news-room/fact-sheets/detail/autism-spectrum-disorders">https://www.who.int/news-room/fact-sheets/detail/autism-spectrum-disorders&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Raj, S., and Masood, S., 2020. Analysis and Detection of Autism Spectrum Disorder Using Machine Learning Techniques, [Online resource &lt;a href="https://reader.elsevier.com/reader/sd/pii/S1877050920308656?token=D9747D2397E831563D1F58D80697D9016C30AAC6074638AA926D06E86426CE4CBF7932313AD5C3504440AFE0112F3868&amp;amp;originRegion=us-east-1&amp;amp;originCreation=20210704171932">https://reader.elsevier.com/reader/sd/pii/S1877050920308656?token=D9747D2397E831563D1F58D80697D9016C30AAC6074638AA926D06E86426CE4CBF7932313AD5C3504440AFE0112F3868&amp;amp;originRegion=us-east-1&amp;amp;originCreation=20210704171932&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Khodatars, M., Shoeibi, A., Ghassemi, N., Jafari, M., Khadem, A., Sadeghi, D., Moridian, P., Hussain, S., Alizadehsani, R., Zare, A., Khosravi, A., Nahavandi, S., Acharya, U. R., and Berk, M., 2020. Deep Learning for Neuroimaging-based Diagnosis and Rehabilitation of Autism Spectrum Disorder: A Review. [Online resource] &lt;a href="https://arxiv.org/pdf/2007.01285.pdf">https://arxiv.org/pdf/2007.01285.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Musser, M., 2020. Detecting Autism Spectrum Disorder in Children using Computer Vision, Adapting facial recognition models to detect Autism Spectrum Disorder. [Online resource] &lt;a href="https://towardsdatascience.com/detecting-autism-spectrum-disorder-in-children-with-computer-vision-8abd7fc9b40a">https://towardsdatascience.com/detecting-autism-spectrum-disorder-in-children-with-computer-vision-8abd7fc9b40a&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Akter, T., Ali, M. H., Khan, I., Satu, S., Uddin, Jamal., Alyami, S. A., Ali, S., Azad, A., and Moni, M. A., 2021. Improved Transfer-Learning-Based Facial Recognition Framework to Detect Autistic Children at an Early Stage. [Online resource] &lt;a href="https://www.mdpi.com/2076-3425/11/6/734">https://www.mdpi.com/2076-3425/11/6/734&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Beary, M., Hadsell, A., Messersmith, R., Hosseini, M., 2020. Diagnosis of Autism in Children using Facial Analysis and Deep Learning. [Online resource] &lt;a href="https://arxiv.org/ftp/arxiv/papers/2008/2008.02890.pdf">https://arxiv.org/ftp/arxiv/papers/2008/2008.02890.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Piosenka, G., 2020. Detect Autism from a facial image. [Online resource] &lt;a href="https://www.kaggle.com/gpiosenka/autistic-children-data-set-traintestvalidate?select=autism.csv">https://www.kaggle.com/gpiosenka/autistic-children-data-set-traintestvalidate?select=autism.csv&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Valuch, F., 2021. Easy Autism Detection with TF.[Online resource] &lt;a href="https://www.kaggle.com/franvaluch/easy-autism-detection-with-tf/comments">https://www.kaggle.com/franvaluch/easy-autism-detection-with-tf/comments&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Increasing Cervical Cancer Risk Analysis</title><link>/report/su21-reu-369/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-369/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-369/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-369/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Theresa Jean-Baptistee, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-369">su21-reu-369&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-369/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Cervical Cancer is an increasing matter that is affecting various women across the nation, in this project we will be analyzing risk factors that are producing
higher chances of this cancer. In order to analyize these risk factors a machine learning technique is implemented to help us understand the leading factors of
cervical cancer.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. DataSets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#iud-visulaization">IUD Visulaization&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tabacoo-visulization-affect-on-cervixs">Tabacoo Visulization Affect On Cervixs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correlation-of-age-and-start-of-sexual-activity">Correlation of Age and Start Of sexual activity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-other-people-works">3. Other People Works&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4--explantion-of-confusion-matrix">4. Explantion of Confusion Matrix&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-benchmark">5. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgments">7. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Cervical, Cancer, Diseases, Data, conditions&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Cervical cancer is a disease that is increasing in various women nationwide. It occurs within the cells of the cervix (can be seen in stage 1 of the image below).
This cancer is the fourth leading cancer, where there are about 52,800 cases found each year, predominantly being in lower developed countries. Cervical cancer
occurs most commonly in women who are within their 50&amp;rsquo;s and who has symptoms such as watery and bloody discharge, bleeding, and painful intercourse. Two other
common causes can be an early start on sexual activity and multiple partners. The most common way to determine if one may be affected by this disease is through a
pap smear. When witnessed early it, can allow a better chance of results and treatment.&lt;/p>
&lt;p>Cervical cancer is so important for the future of reproduction, being the cause of a successful or unsuccessful birth with completions like premature a child. The
cervix help keeps the fetus stable within the uterus during this cycle, towards the end of development, it softens and dilates for the birth of a child. If
diagnosed with this cancer, a miracle would be needed to conceive a child after having treatment. Most treatments begin with a biopsy removing affected areas of
cervical tissue. As it continues, to spread radiotherapy might be recommended to treat the cancer where may affect the womb. lastly, one may need to have a
hysterectomy which is the removal of the womb.&lt;/p>
&lt;p>In this paper, we will study the exact cause and risk factors that may place someone in this position. If spotted early it wouldn’t affect someone’s dream chance
of conceiving or affect their reproductive parts. Using various data sets we will study the way everything may alignes in causes and machine leaning would be the
primary technique to used interpretate the relation between variables and risk factor on cervical cancer.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-369/main/project/images/Cervical-Cancer-1024x624.jpg" alt="Figure 2">&lt;/p>
&lt;h2 id="2-datasets">2. DataSets&lt;/h2>
&lt;p>The Data sets obatained shows the primary risk factors that affect women ages 15 and above. The few factors that sticked out the most were age, start of sexual
activity, tabacoo intake, and IUD. The age and start of sexual activity maybe primary factor because a person is more liable to catch an STD and get this diease
from mutiple parnters never really knowing what the other person may be doing outside of the encounterment. Tabcoo intake causes an affect making a person by
weaking the immune system and making somone more septable to the disease. The IUD has the highest number on the data set being a primary factor that may put a
person at risk, this device aids the prevention of pregency by thickneing the mucos of the cervix that could later cause infection or make your more spetiable to
them.&lt;/p>
&lt;h2 id="iud-visulaization">IUD Visulaization&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/blob/main/project/images/cervical%20iud%20.jpg" alt="figure 2">&lt;/p>
&lt;h2 id="tabacoo-visulization-affect-on-cervixs">Tabacoo Visulization Affect On Cervixs&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/blob/main/project/images/tab.png" alt="Figure 3">&lt;/p>
&lt;h2 id="correlation-of-age-and-start-of-sexual-activity">Correlation of Age and Start Of sexual activity&lt;/h2>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-369/main/project/images/download-2021-06-29T15-34-01-628Z.png" alt="Figure 1">&lt;/p>
&lt;h2 id="3-other-people-works">3. Other People Works&lt;/h2>
&lt;p>The research of others work has made a huge imapact to this project starting from data to important knowledge needed to conduct the project. With the various
research sites, we were able to witness what the affects various day to day activtie affect women long term. The Cervical Cancer Diagnosis Using a Chicken Swarm
Optimization Based Machine Learning Method, was a big aid throught the project explaing the stages of cervical cancer, ways it can be treated, and the affects it
may cause. With the data that was used from UCI Machine Learning, we were able to find efficent correlation into the data, helping the implented machine learning algorithm for the classification task.&lt;/p>
&lt;h2 id="4--explantion-of-confusion-matrix">4. Explantion of Confusion Matrix&lt;/h2>
&lt;p>The confusion matrix generated by multilayer perceptron can be explained as the perdicted summary results from the data obtained. Zero is when no cervical cancer is witnessed, one is when cervical cancer is seen. A hundrend and sixty-two is the highest number of this disease seen on the chart and the lowest number being winessed is two and eight being quiet of a jump.&lt;/p>
&lt;h2 id="5-benchmark">5. Benchmark&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-369/blob/main/project/images/Screen%20Shot%202021-07-27%20at%201.11.05%20PM.png" alt="figure4">&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="7-acknowledgments">7. Acknowledgments&lt;/h2>
&lt;p>With the help of Gregor, Yohn and carlos this project was acheieved with the best with the best results possible. Thank you!&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2></description></item><item><title>Report: Cyber Attacks Detection Using AI Algorithms</title><link>/report/su21-reu-365/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-365/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-365/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-365/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-365/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-365/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Victor Adankai, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-365">su21-reu-365&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-374/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#types-of-cyber-attacks">Types of Cyber Attacks&lt;/a>&lt;/li>
&lt;li>&lt;a href="#examples-of-ai-algorithms-for-cyber-attacks-detection">Examples of AI Algorithms for Cyber Attacks Detection&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-benchmark">4. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgments">6. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> AI, ML, DL, Cybersecurity, Cyber Attacks.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;ul>
&lt;li>Find literature about AI and Cyber Attacks on IoT Devices Dectection.&lt;/li>
&lt;li>Analyze the literature and explain how AI for Cyber Attacks on IOT Devices Detection are beneficial.&lt;/li>
&lt;/ul>
&lt;h4 id="types-of-cyber-attacks">Types of Cyber Attacks&lt;/h4>
&lt;ul>
&lt;li>Denial of service (DoS) Attack:&lt;/li>
&lt;li>Remote to Local Attack:&lt;/li>
&lt;li>Probing:&lt;/li>
&lt;li>User to Root Attack:&lt;/li>
&lt;li>Adversarial Attacks:&lt;/li>
&lt;li>Poisoning Attack:&lt;/li>
&lt;li>Evasion Attack:&lt;/li>
&lt;li>Integrity Attack:&lt;/li>
&lt;li>Malware Attack:&lt;/li>
&lt;li>Phising Attack:&lt;/li>
&lt;li>Zero Day Attack:&lt;/li>
&lt;li>Sinkhole Attack:&lt;/li>
&lt;li>Causative Attack:&lt;/li>
&lt;/ul>
&lt;h4 id="examples-of-ai-algorithms-for-cyber-attacks-detection">Examples of AI Algorithms for Cyber Attacks Detection&lt;/h4>
&lt;ul>
&lt;li>Convolutional Neural Network (CNN)&lt;/li>
&lt;li>Autoencoder (AE)&lt;/li>
&lt;li>Deep Belief Network (DBN)&lt;/li>
&lt;li>Recurrent Neural Network (RNN)&lt;/li>
&lt;li>Generative Adversal Network (GAN)&lt;/li>
&lt;li>Deep Reinforcement Learning (DIL)&lt;/li>
&lt;/ul>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;ul>
&lt;li>Finding data sets in IoT Devices Cyber Attacks.&lt;/li>
&lt;li>Can any of the data sets be used in AI?&lt;/li>
&lt;li>What are the challenges with IoT Devices Cyber Attacks data set? Privacy, HIPPA, Size, Avalibility&lt;/li>
&lt;li>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/li>
&lt;/ul>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;ul>
&lt;li>Place a cool image into projects images in my directory&lt;/li>
&lt;li>Correct the following link, replace the fa number with my su number and thne chart of png.&lt;/li>
&lt;li>If the image has been copied, you must use a reference such as shown in the Figure 1 caption.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-benchmark">4. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common [^2]&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="6-acknowledgments">6. Acknowledgments&lt;/h2>
&lt;ul>
&lt;li>Gregor von Laszewski&lt;/li>
&lt;li>Yohn Jairo Bautista&lt;/li>
&lt;li>Carlos Theran&lt;/li>
&lt;/ul>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Dentronics: Artifical intelligence in Prosthodontics</title><link>/report/su21-reu-376/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-376/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-376/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-376/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-376/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-376/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Jamyla Young, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-376">su21-reu-376&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-376/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Artificial intelligence is a branch of computer science that focuses on building and programming machines to think like humans and mimic their actions. The proper concept definition of this term cannot be achieved simply by applying a mathematical, engineering, or logical approach but requires an approach that is linked to a deep cognitive scientific inquiry. THe use of machine-based learning is constantly evolving the dental and medical field to assist with medical decision making process.In addition to diagnosis of visually confirmed dental caries and impacted teeth, studies applying machine learning based on artificial neural networks to dental treatment through analysis of dental magnetic resonance imaging, computed tomography, and cephalometric radiography are actively underway, and some visible results are emerging at a rapid pace for commercialization.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-example-of-an-ai-algorithm-in-dentronics">5. Example of an AI algorithm in Dentronics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Dental implants, implant stability, prosthodotics.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>{https://www.drkarras.com/a-look-at-the-structure-of-dental-implants/}
&lt;a href="https://www.infodentis.com/dental-implants/abutment.php">https://www.infodentis.com/dental-implants/abutment.php&lt;/a>
Dental implants are ribbed oral protheses typically made up of biocompatible titanium to replace the missing root(s) of an absent tooth.
{1} These dental protheses are used to support the jaw bone to prevent deterioration due to an absent root. This is referred to as bone resorption which can result to facial malformation as well as reduced oral function such as biting and chewing. These devices are composed of three elements that imitate a natural tooth function and structure.
The implant which are typically ribbed and threaded to promote stability while integrating within the bone tissue. The osseointegration process usually takes 6-8 months to rebuild the bone to support the implant. An implant abutment is fixed on top of the implant to act as a base for prosthetic devices. (2) Prefabricated abutments are manufactured in many shapes, sizes and angles depending on the location of the implant and the types of prothesis that will be attached. Dental abutments support a range of prothetic devices such as dental crowns, bridges, and dentures.&lt;/p>
&lt;p>&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5268121/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5268121/&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7494467/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7494467/&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5480808/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5480808/&lt;/a>&lt;/p>
&lt;p>Osseointegrated dental implants depend on various factors that affect the anchorage of the implant to the bone tissue. Successful surgical anchoring techniques can contribute to long term success of implant stability. Primary stability plays a role 2 week postoperatively by achieving mechanical retention of the implant. It helps establish a mechanical microenvironment for gradual bone healing, or osseointegration-This is secondary implant stability. {2} Bone type, implant length, implant and diameter influences primary and secondary implant stability. Implant length can range from 6mm to 20mm; however, the most common lengths are between 8mm to 15mm. Many studies suggest that implant length contribute to decreasing bone stress and increasing implant stability. Bone stress can occur at both the cortical and cancellous part of the bone. Increasing implant length will decrease stress in the cancellous part of the bone while increasing the implant diameter can decrease stress in the cortical part of the bone. Bone type can promote positive bone stimulation around an implant improving the overall function. There are four different types: Type I, Type II, Type III, and Type IV. Type I is the most dense of them which provides more cortical anchorage but has limited vascularity. Type II is the best for osseointegration because it provides good cortical anchorage and has better vascularity than type I. Type III and IV have a thin layer of cortical bone which decrease the success rate of primary stability{3).&lt;/p>
&lt;p>&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7494467/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7494467/&lt;/a>
&lt;a href="https://www.sciencedirect.com/science/article/pii/S0901502720302496">https://www.sciencedirect.com/science/article/pii/S0901502720302496&lt;/a>&lt;/p>
&lt;p>Implant stability can be measured using the Implant Stability Quotient (ISQ) as an indirect indicator to determine the time frame for implant loading and prognostic indicator for implant failure(1). This can be measured by resonance frequency analysis (RFA) immediately after the implant has been placed. Resonance frequency analysis is the measurement in which a device vibrates in response to frequencies in the range of 5-15 kHz. The peak amplitude of the response is then encoded into the implant stability quotient (ISQ) .The clinical range of ISQ is from 55-80. High stability is &amp;gt;70 ISQ while medium stability is between 60-69 ISQ. Low stability is &amp;lt;60 ISQ.(2)&lt;/p>
&lt;h2 id="2-data-sets">2. Data sets&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Find data sets about effects about dentronics&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the purpose of data sets&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> What are the challenges with Dentronics data sets? HIPPA, size, availibility&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub. However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/li>
&lt;/ul>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Place a cool image into projects images in my directory&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Correct the following, replace the fa number with my su number and the chart of png&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> if image is copied , you must use a reference such as shown in the Figure 1 caption&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> list&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="5-example-of-an-ai-algorithm-in-dentronics">5. Example of an AI algorithm in Dentronics&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete data set that will be used&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete algorithm that is used to anaylze the data set&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Write the program&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Verify that it works&lt;/li>
&lt;/ul>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Gregor helped me&lt;/li>
&lt;/ul>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>-[ ] Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at one point automatically change the references from superscript to square brackets it is best to introduce a space before the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/hid-example/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/hid-example/project/</guid><description>
&lt;p>Fix the links: and than remove this line&lt;/p>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/hid-example/actions">&lt;img src="https://github.com/cybertraining-dsc/hid-example/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/hid-example/actions">&lt;img src="https://github.com/cybertraining-dsc/hid-example/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Fix the links: and than remove this line&lt;/p>
&lt;p>Gregor von Laszewski, &lt;a href="https://github.com/cybertraining-dsc/hid-example/">hid-example&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Aquatic Animals Classification Using AI</title><link>/report/su21-reu-370/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-370/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-370/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-370/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-370/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-370/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Timia Williams, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-370">su21-reu-370&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-370/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Marine animals play an important role in the ecosystem. Aquatic animals play an important role in nutrient cycles because they store a large proportion of ecosystem nutrients in their tissues, transport nutrients farther than other aquatic animals and excrete nutrients in dissolved forms that are readily available to primary producers. Fish images are captured by scuba divers, tourist, or underwater submarines. different angles of fishes image can be very difficult to get because of the constant movement of the fish. In addition to getting the right angles, the images of marine animals are usually low-quality because of the water. Underwater cameras that is required for a good quality image can be expensive. Using AI could potentially increase the marine population by the help of classification by testing the usage of machine learning using the images obtained from the aquarium combined with advanced technology. We collect 164 fish images data from Georgia acquarium to look at the different movements.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-machine-learning-in-fish-species">2. Machine learning in fish species.&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-datasets">3. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#31-sample-of-images-of-personal-dataset">3.1. Sample of Images of Personal Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-sample-of-images-from-large-scale-fish-dataset">3.2. Sample of Images from Large Scale Fish Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-conclusion">4. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-acknowledgments">5. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-references">6. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>It can be challenging to obtain a large number of different complex species in a single aquatic environment. Traditionally, it would take marine biologists years to collect the data and successfully classify the type of species obtained [1]. Scientist says that more than 90 percent of the ocean&amp;rsquo;s species are still undiscovered, with some estimating that there are anywhere between a few hundred thousand and a few million more to be discovered&amp;quot; (National Geographic Society). Currently, scientists know of around 226,000 ocean species. Now and days, Artificial intelligence and machine learning have been well used for detection and classification in images. In this project, I will propose to use machine learning techniques to analyze the images obtained from the Georgia Aquarium to identify legal and illegal fishing.&lt;/p>
&lt;h2 id="2-machine-learning-in-fish-species">2. Machine learning in fish species.&lt;/h2>
&lt;p>Aquatic ecologists often count animals to keep up the population count of providing critical conservation and management. Since the creation of underwater cameras and other recording equipment, underwater devices have allowed scientists to safely and efficiently classify fishes images without the disadvantages of manually entering data, ultimately saving lots of time, labor, and money. The use of machine learning to automate image processing has its benefits but has rarely been adopted in aquatic studies. With using efforts to use deep learning methods, the classification of specific species could potentially increase. In fact, there is a study done in Australia&amp;rsquo;s ocean waters that classification of fish through deep learning was more efficient that manual human classification. In the study to test the abundance of different species, &amp;ldquo;The computer’s performance in determining abundance was 7.1% better than human marine experts and 13.4% better than citizen scientists in single image test datasets, and 1.5 and 7.8% higher in video datasets, respectively&amp;rdquo; (Campbell, M. D.). This remarkably explain that using machiene learning in marine animals is a better method than a manually classifying Aquatic animals Not only is it good for classification, it will be used to answer broader questions such as population count, the location of species, its abundance, and how it appears to be thriving. Since Machine learning and deep learning are often defined as one, both learning methods will be used to analyze the images and find patterns on my data.&lt;/p>
&lt;h2 id="3-datasets">3. Datasets&lt;/h2>
&lt;p>I used two datasets in my project. The first dataset includes the pictures that I took at the Georgia Acquarium. That dataset was used for testing. The second dataset used was a fish dataset from kaggle which contains 9 different seafood types (Black Sea Sprat, Gilt-Head Bream, Hourse Mackerel, Red Mullet, Red Sea Bream, Sea Bass, Shrimp, Striped Red Mullet, Trout). For each type, there are 1000 augmented images and their pair-wise augmented ground truths.&lt;/p>
&lt;p>The link to access the dataset I used from kaggle is &lt;a href="https://www.kaggle.com/crowww/a-large-scale-fish-dataset">https://www.kaggle.com/crowww/a-large-scale-fish-dataset&lt;/a>&lt;/p>
&lt;h2 id="31-sample-of-images-of-personal-dataset">3.1. Sample of Images of Personal Dataset&lt;/h2>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-370/main/project/images/IMG_1566.jpg" width="30%"> &lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-370/main/project/images/IMG_1583.jpg" width="30%"> &lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-370/main/project/images/IMG_1574.jpg" width="30%">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> These images are samples of my personal data which is made up of images of fishes taken at the Georgia Acquarium.&lt;/p>
&lt;h2 id="32-sample-of-images-from-large-scale-fish-dataset">3.2. Sample of Images from Large Scale Fish Dataset&lt;/h2>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-370/main/project/images/IMG_1565.jpg" alt="Figure 1">&lt;/p>
&lt;h2 id="4-conclusion">4. Conclusion&lt;/h2>
&lt;p>Deep learning methods provide a faster, cheaper, and more accurate alternative to manual data analysis methods currently used to monitor and assess animal abundance and have much to offer the field of aquatic ecology.&lt;/p>
&lt;h2 id="5-acknowledgments">5. Acknowledgments&lt;/h2>
&lt;h2 id="6-references">6. References&lt;/h2></description></item><item><title>Report: Project: Hand Tracking with AI</title><link>/report/su21-reu-364/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-364/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-364/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-364/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-364/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-364/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>David Umanzor, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-364">su21-reu-364&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-364/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>In this project, we study the ability of an AI to recognize letters from the American Sign Language (ASL) alphabet. We use a Convolutional Neural Network and apply it to a dataset of hands in different positionings showing the letters &amp;lsquo;a&amp;rsquo;, &amp;lsquo;b&amp;rsquo;, and &amp;lsquo;c&amp;rsquo; in ASL. The proposed CNN model receives an ASL image and recognizes the feature of the image, generating the predicted letter.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data Sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-documentation">3. Documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-benchmark">5. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgments">7. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ai, object recognition, image processing, computer vision, american sign language.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Object detection and feature selection are essential tasks in computer vision and have been approached from various perspectives over the past few decades [1]. The brain uses object recognition to solve an inverse problem: one where (surface properties, shapes, and arrangements of objects) need to be inferred from the perceived outcome of the image formation process [3]. Visual object recognition as a neural substrate in humans was revealed by neuropsychological studies. There are specific brain regions that cause object recognition, yet we still do not understand how the brain achieves this remarkable behavior [2]. Human beings rely and rapidly recognize objects despite considerable retinal image transformations arising from changes in lighting, image size, position, and viewing angle [2].&lt;/p>
&lt;p>A gesture is a form of nonverbal communication done with positions and movements of the hand, arms, body parts, hand shapes, movements of the lips or face [4]. One of the key differences of hand gestures is that they allow communication over a long distance [5]. American Sign Language (ASL) is a formal language that has the same lingual properties as oral languages commonly used by deaf people to communicate [6]. ASL typically is formed by the finger, hand, and arm positioning and can contain static and dynamic movement or a combination of both to communicate words and meanings to another [7]. Communication with other people can be challenging because people are not typically willing to learn sign language [7].&lt;/p>
&lt;p>In this paper, we consider the problem of detecting and understanding American Sign Language. We test CNN&amp;rsquo;s ability to recognize the ASL alphabet. As advancements in technology increase, there are more improvements to 2D methods of hand detection. Commonly these methods are visual-based, using color, shape, and edge to detect and recognize the hand [8]. There are issues to these technologies like inconsistent lighting conditions, non-hand color similarity, and varying viewpoints that can decrease the model&amp;rsquo;s ability to recognize the hand and its positioning. We use a Convolutional Neural Network to create the model and detect different letters of American Sign Language.&lt;/p>
&lt;h2 id="2-data-sets">2. Data Sets&lt;/h2>
&lt;p>In this research we use two sources of datasets, the first is from kaggle which it was already prepared but we needed more. The second is self made dataset by take images in good lighting against a white wall, it was then cropped to 400x400 pixels focused on the hand. The program then sets the images to grayscale as the color is not needed for this research. Finally, the images are reduced to 50x50 resolution for the AI to use for training.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-364/raw/main/project/images/Hand%20B%20Dataset%20Demo.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Dataset of hands doing different alphabet letters in ASL &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-documentation">3. Documentation&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-364/raw/main/project/images/CNNDiagram.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> The Convolutional Neural Network (CNN) model.&lt;/p>
&lt;p>This model shows the CNN model that we used to train the AI. The CNN takes pictures and breaks them down into smaller segments called features. It is trained to find patterns and features over the images allowing the CNN to predict an &amp;lsquo;a&amp;rsquo;, &amp;lsquo;b&amp;rsquo;, or &amp;lsquo;c&amp;rsquo; upon the given ASL image with high accuracy. A CNN uses a convolution operation that filters every possible position the feature it collected can be matched to and attempts to find where it fits in [10]. This process is repeated and becomes the convolution layer or in the image depicted as Conv2d + Relu. The ReLU stands for the rectified linear unit and is used as an activation function for the CNN [11].&lt;/p>
&lt;ul>
&lt;li>
&lt;p>[] What is a Relu operation?
ReLU operation is a rectified linear unit and is used as an activation function for the CNN, we use a Leaky ReLU in our model because it is easy to use to train the model quickly and it has a small tolerance for negatives values unlike the normal ReLU fuction.
paperswithcode.com/method/leaky-relu
add figure of a leaky ReLU&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[] What is a Conv2d?
Conv2d is a 2D Convolution layer meant for a images as it uses height and width. They build a filter across the image by recognizing the similarities of the image&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[] What is a BashNormalization operation?
Batch Normalization is a process that standardizes the updates as the Convolutional process sets weights and as the neural network goes through each layer the procedure keeps adjusting to a target that never stays the same, requiring more epochs and and reduces the time it takes to train a deep learning neural network. :Reference 12:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[] what is Maxpooling operation?
Maximum pooling is an operation the gathers the biggest number in each collection of each feature map. This provides a way to avoid over-fitting&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[] what is Fully Connected?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[] what is Softmax operation?&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;p>In this research, we built the model using a convolution neural network (CNN) to create an AI that can recognize ASL letters (&amp;lsquo;a&amp;rsquo;, &amp;lsquo;b&amp;rsquo;, and &amp;lsquo;c&amp;rsquo;), using a collection of 282 images. The Dataset contains 94 images for each letter to train the AI&amp;rsquo;s CNN. This can be expanded to allow an AI to recognize letters, words, and any expression that can be made using a still image of the hands. A CNN fits this perfectly as we can use its ability to assign importance to segments of an image and tell the difference from one another using weights and biases. With the proper training, it is able to learn and identify these characteristics [9].&lt;/p>
&lt;h2 id="5-benchmark">5. Benchmark&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-364/raw/main/project/images/ConfusionMatrixCNN.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> The Confusion Matrix of the finished CNN model.&lt;/p>
&lt;p>The Confusion Matrix shows the results of the model after being tested on its ability to recognize each letter, in the image it shows that the AI had a difficult time recognizing the difference between an &amp;lsquo;a&amp;rsquo; and &amp;lsquo;c&amp;rsquo; only getting 6% of the images labelled as &amp;lsquo;c&amp;rsquo; correct.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>We build a model to recognize an ASL given an image and predict the corresponding letter using a convolutional neural network. The model provides a means of 66% accuracy in classifying the ASL among the three classes &amp;lsquo;a&amp;rsquo;, &amp;lsquo;b&amp;rsquo;, and &amp;lsquo;c&amp;rsquo;. From the given results, the letters &amp;lsquo;a&amp;rsquo; and &amp;lsquo;c&amp;rsquo; became the most difficult for the CNN to differentiate from each other, as shown in the confusion matrix in figure 3. We suggest that the low accuracy rate is based on similar appearing grayscale of the letters &amp;lsquo;a&amp;rsquo; and &amp;lsquo;c&amp;rsquo; and the lack of a larger dataset for the AI to learn from. We determine that using a larger dataset of the entire alphabet and increasing the number of examples of each letter to train the AI could improve the results.&lt;/p>
&lt;p>We found that the low accuracy can be increased by improving the resolution of the image giving the program more features to go off of in its computing to recognize the image, going from model 1 at 50 x 50 pixels to model 2 at 80 x 80 pixels there was an increase from 66% to 76% in accuracy, this in theory should improve as the resolution of the image increases from 100x 100 to 200x 200 and at the best the image&amp;rsquo;s resolution would be left at the original size off 400 x 400. This is accuracy increase would be because as the resolution drops the program has less information and some of the important landmarks of the hand are lost due to the resolution of the image.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Correct formatting and grammar&lt;/li>
&lt;/ul>
&lt;p>Future studies using a larger dataset can be applied to more complex methods than just singular letters but words from the ASL language to recreate a text to speech software based around ASL hand positioning.&lt;/p>
&lt;h2 id="7-acknowledgments">7. Acknowledgments&lt;/h2>
&lt;p>We thank Carlos Theran (Florida A &amp;amp; M University) for advising, guidance, and resources used in the research; We thank Yohn Jairo (Florida A &amp;amp; M University) for guidance and aid on the research report; We thank Gregor von Laszewki (Florida A &amp;amp; M University) for advice and commenting on the code and report; We thank the Polk State LSAMP Program for aid in obtaining this opportunity. We thank Florida A &amp;amp; M University for funding this research.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>tecperson, Sign Language MNIST Drop-In Replacement for MNIST for Hand Gesture Recognition Tasks, [Kaggle]
&lt;a href="https://www.kaggle.com/datamunge/sign-language-mnist">https://www.kaggle.com/datamunge/sign-language-mnist&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Handwriting Recognition Using AI</title><link>/report/su21-reu-366/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-366/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-366/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-366/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-366/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-366/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Report&lt;/p>
&lt;p>Mikahla Reeves, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-366">su21-reu-366&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-366/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="to-do">To-Do&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;input checked="" disabled="" type="checkbox"> Add in introduction&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;input disabled="" type="checkbox"> Add in references&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;input disabled="" type="checkbox"> Add in more images&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;input disabled="" type="checkbox"> Explain what persons have done so far, approach to the prob&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The first thing that comes to numerous minds when they hear &lt;em>Handwriting Recognition&lt;/em> is simply computers identifying handwriting,
and that is correct. Handwriting Recognition is the ability of a computer to interpret handwritten input received from different sources.
In the artificial intelligence world, handwriting recognition has become a very established area. Over the years, there have been many
developments and applications made in this field. This study investigates some of the approaches taken by researchers/developers in the
last few years to convert handwritten information from images to digital forms. Also, it discusses the importance of handwriting recognition in modern society.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-importance-of-handwriting-recognition">2. Importance of Handwriting Recognition&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-images">3. Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-what-has-been-done-so-far-in-the-field">4. What has been done so far in the field?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-machine-learning-tools">5. Machine Learning Tools&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-results">6. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> handwriting recognition, optical character recognition, deep learning.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Perhaps one of the most monumental things in this modern-day is how our devices can behave like brains. Our various devices can call mom, play our favorite song,
and answer our questions by just a simple utterance of Siri or Alexa. These things are all possible because of what we call artificial intelligence. Artificial
intelligence is a part of computer science that involves learning, problem-solving, and replication of human intelligence. When we hear of artificial intelligence,
we often hear of machine learning as well. The reason for this is because machine learning also involves the use of human intelligence. Machine learning is the
process of a program or system getting more capable over time &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. One example of machine learning at work is Netflix. Netflix is a streaming service that allows
users to watch a variety of tv shows and movies, and it also falls under the category of a recommendation engine. Recommendation engines/applications like Netflix
do not need to be explicitly programmed. However, their algorithms mine the data, identify patterns, and then the applications can make recommendations.&lt;/p>
&lt;p>Now, what is handwriting recognition? Handwriting Recognition is a branch of (OCR) Optical Character Recognition. It is a technology that receives handwritten
information from paper, images, and other items and interprets them into digital text in real-time &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Handwriting recognition is a well-established area in the
field of image processing. Over the last few years, developers have created handwriting recognition technology to convert written postal codes, addresses, math questions,
essays, and many more types of written information into digital forms, thus making life easier for businesses and individuals. However, the development of handwriting
recognition technology has been quite challenging.&lt;/p>
&lt;p>One of the main challenges of handwriting recognition is accuracy. There is a wide variety of handwriting styles, both good and bad, thus making it harder for developers to
provide enough samples of what a specific character/integer looks like &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. In handwriting recognition, the computer has to translate the handwriting into a format that it
understands, and this is where Optical Character Recognition becomes useful. In OCR, the computer focuses on a character, compares it to characters in its database, then
identifies what the letters are and fundamentally what the words are.&lt;/p>
&lt;h2 id="2-importance-of-handwriting-recognition">2. Importance of Handwriting Recognition&lt;/h2>
&lt;p>Handwriting recognition plays a vital role in the field of image processing. Any machine or technology with the capability to identify and convert handwritten
information into digital text is very valuable in almost every field work.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Expound&lt;/li>
&lt;/ul>
&lt;h2 id="3-images">3. Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-366/raw/main/project/images/ocr_work.jpg" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Overview of Optical Character Recognition at work.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> MORE IMAGES&lt;/li>
&lt;/ul>
&lt;h2 id="4-what-has-been-done-so-far-in-the-field">4. What has been done so far in the field?&lt;/h2>
&lt;h2 id="5-machine-learning-tools">5. Machine Learning Tools&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Add images of the models , and the input &amp;amp; output etc&lt;/li>
&lt;/ul>
&lt;h2 id="6-results">6. Results&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Results from the papers.&lt;/li>
&lt;/ul>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>This paper would not have been possible without the exceptional support of Gregor von Laszewski, Carlos Theran, Yohn Jairo.
Their constant guidance, enthusiasm, knowledge and encouragement have been a huge motivation to keep going and to complete this work.
Thank you to Jacques Fleicher, for always making himself available to answer questions. Finally, thank you to Byron Greene
and the Florida A&amp;amp;M University for providing this great opportunity for undergraduate students to do research.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Brown, S., 2021. Machine learning, explained | MIT Sloan. [online] MIT Sloan. Available at: &lt;a href="https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained">https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained&lt;/a>.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Handwriting Recognition in 2021: In-depth Guide. (n.d.). &lt;a href="https://research.aimultiple.com/handwriting-recognition">https://research.aimultiple.com/handwriting-recognition&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>ThinkAutomation. 2021. Why is handwriting recognition so difficult for AI? - ThinkAutomation. [online] Available at: &lt;a href="https://www.thinkautomation.com/bots-and-ai/why-is-handwriting-recognition-so-difficult-for-ai/">https://www.thinkautomation.com/bots-and-ai/why-is-handwriting-recognition-so-difficult-for-ai/&lt;/a>.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Analyzing Hashimoto disease causes, symptoms and cases improvements using Topic Modeling</title><link>/report/su21-reu-372/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-372/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-372/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-372/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Sheimy Paz, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-372">su21-reu-372&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-372/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This project proposes a new view of Hashimoto’s disorder, its association with other pathologies, possible causes, symptoms, diets, and recommendations. The intention is to explore the association of Hashimoto disorder with disease like h pylori bacteria, inappropriate diet, environmental factors, and genetic factors. To achieve this, we are going to utilize AI in particular
topic modeling which is a technic used to process large collection of data to identifying topics.
Topic modeling is a text-mining tool that help to correlate words with topics making the research process easy and organized with the purpose to get a better understanding of the disorder and the relationship that this has with other health issues hoping to find clear information about the causes and effect that can have on the human body.
The dataset was collected from silo breaker software, which contains information about news, reports, tweets, and blogs. The program will organize our findings highlighting key words related to symptoms, causes, cures, anything that can apport clarification to the disorder.&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-summary-tables">2. Summary Tables&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-datasets">3. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-results">4. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-hashimoto-findings">5. Hashimoto Findings&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Thyroid disease, Hashimoto, H Pylori, Implants, Food Sensitivity, Diary sensitivity, Healthy Diets, Exercise, topic modeling, text mining, BERT model.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Hashimoto thyroiditis is an organ-specific autoimmune disorder. its symptoms were first described 1912 but the disease was not recognized until 1957. Hashimoto is an autoimmune disorder that destroys thyroid cells and is antibody-mediated &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. In a female-to-men radio at least 10:4 women are more often affected than men. The diagnostic is often called between the ages of 30 to 50 years &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Pathologically speaking, Hashimoto stimulates the formation of antithyroid antibodies that attack the thyroid tissue, causing progressive fibrosis. Hashimoto is believe to be the concequence of a combination of mutated genes and eviromental factors&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The disorder is difficult to diagnose since in the early course of the disease the patients may or may not exhibit symptoms or laboratory findings of hyperthyroidism, it may show normal values because the destruction of the gland cells may be intermittent &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Clinical and epidemiological studies suggest worldwide that the most common cause of hypothyroidism is an inadequate dietary intake of iodine.&lt;/p>
&lt;p>Due to the arduous labor to identify this disorder a Machine Learning algorithm based on prediction would help to identify Hashimoto in early stages as well as any other health issues related to it &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This will be helpful for patients that would be able to get the correct treatment in an early stage of the illness avoiding future complications. This research algorithm was mainly intended to find patient testimonies of improvements, completed healed cases, early symptoms, trigger factors or any useful information about the disorder.&lt;/p>
&lt;p>Hashimoto autoimmune diseases have been linked to the infection caused by H pylori bacteria. H pylori is until the date the most common chronic bacterial infection, affecting half of the world&amp;rsquo;s population and is known for the presence of Caga antigens which are virulent strains that have been found in organ and non-organ specific autoimmune diseases &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Another important trigger of Hashimoto disorder is the inadequate modern diet patterns and the environmental factors that are closely related to it &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. For instance, western diet consumption is an essential factor that trigs the disorder since this food is highly preserved and predominate the consumption of artificial flavors and sugars which have dramatically increase in the past years, adding to it the use of chemicals and insecticides in the fruits and vegetables and the massive introduction of hormones for meat production, all this can be the cause of the rise of autoimmune diseases &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We utilize deep learning BERT model to train our dataset. BERT is a superior performer Bidirectional Encoder, which superimposes 12 or 24 layers of multiheaded attention in a Transformer &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Bert stands for Bidirectional(read from left to right and vice versa with the purpose of an accurate understanding of the meaning of each word in a sentence or document) Encoder Representations from Transformers(the used of transformers and bidirectional models allows the learning of contextual relations between words). Notice that BERT uses two training strategies MLM and NSP.&lt;/p>
&lt;p>Masked Lenguage Model (MLM) process is made by masking around 15% of token making the model predict the meaning or value of each of the masked words. In technical word it requires 3 steps Adding a classification layer on top of the encoder output, Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension. And lastly calculating the probability of each word in the vocabulary with SoftMax. Here we can see an image of the process &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/MLMBertexPic1.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>MLM Bert Figure:&lt;/strong> &amp;ldquo;Masked Language Model Figure Example&amp;rdquo; &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Next Sentence Prediction (NSP) process is based in sentence prediction. The model obtains pair of sentences as inputs, and it is train to predict which is the second sentence in the pair. In The training process 50% of the input sentences are in fact first and second sentence and in the other 50% the second sentences are random sentences used for training purposes.
The model is able to distinguish if the second sentence is connected to the first sentence by a 3-step process. An CLS (the reserved token to represent the start of sequence) is inserted at the beginning of the first sentence while the SEP (separate segments or sentence) is inserted at the end of each sentence. And embedding indicating sentence A or B is added to each token, and lastly a positional embedding is added to each token to indicate its position in the sequence like is shown on the image &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/NSPBertexpic.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>NSP Bert Figure:&lt;/strong> &amp;ldquo;Next Sentence Prediction Figure Example&amp;rdquo; &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>By the trained model Parameter learning we obtains the word embeddings of the input sentence or input sentence pair in the unsupervised learning framework proceeds by solving the following two tasks: Masked Language Model and Next Sentence Prediction.&lt;/p>
&lt;p>We try to use Bert model in the small dataset Hashimoto without any success because the BERT model was overfitting the data points. We use LDA model to train the Hashimoto dataset which allow us to find topic probabilities that we compare with the thyroiditis dataset that was trained with the BERT model-framework.&lt;/p>
&lt;p>We used Natural Language Toolkit (NLTK) which is a module that uses the process of splitting sentences from paragraph, split words, recognizing the meaning of those words, to highlighting the main subjects, with the purpose to help to understand the meaning of the document &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. For instance, in our NLTK model we used two data sets &lt;a href="https://drive.google.com/drive/u/0/folders/1Omtnn5e-yH3bbhW0-5fIbLgi8SEyfYBP">Hashimoto and thyroiditis&lt;/a> and we were able to identify the top 30 topics connected to these disorders. From the information collected we were able to identify general information like association of the disorder with other health issues. The impact of Hashimoto patient with covid19, long term consequences of untreated Hashimoto, recommendation for advance cases, and diet suggestion for improvement. The used of Natural Language tool kit made a precise and less time consuming research process.&lt;/p>
&lt;h2 id="2-summary-tables">2. Summary Tables&lt;/h2>
&lt;p>We can observe in this table the differences between this two similar disorders that are frequently misunderstood.&lt;/p>
&lt;p>&lt;strong>Summary Table 1:&lt;/strong> &amp;ldquo;Differences Between Hashimoto&amp;rsquo;s Thyroiditis and Grave&amp;rsquo;s Disease&amp;rdquo; &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/hertoghe-table-2.jpg" alt="Summary Table 1">&lt;/p>
&lt;p>&lt;strong>Summary Table 2:&lt;/strong> &amp;ldquo;Hashimoto’s thyroiditis is associated with other important disorders&amp;rdquo; &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/Thyroiditis%20Associated%20Pathologies.jpg" alt="Summary Table 2">&lt;/p>
&lt;p>&lt;strong>Summary Table 3:&lt;/strong> &amp;ldquo;Overview of the main dietary recommendations for patients with Hashimoto&amp;rdquo; &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/Dietary%20changes%20that%20reduce%20Antithyroid%20Antibody%20levels.jpg" alt="Summary Table 3">&lt;/p>
&lt;h2 id="3-datasets">3. Datasets&lt;/h2>
&lt;p>Silobreaker software was used to obtain scientific information related to the Hashimoto disease coming from different sources such as journals, proceedings, tweets, and news. Our date consists in the fallowing feature: ID, cluster Id, Description, publication date, Source URL, publisher. And the purpose is to analyze the preform of the proposed approach to discover the hiding semantic structures related with Hashimoto and thyroiditis the description from the gather data is used to study the frequency of Hashimoto and thyroiditis appears in the documents and detecting words and phrases patterns within them to automatically clustering work groups.&lt;/p>
&lt;p>The dataset was obtained from Silobreaker database which is a commercial database. We got access through Florida A&amp;amp;M University who provided me the right to query the data. the link for the silobreaker information is [Here] (&lt;a href="https://www.silobreaker.com/">https://www.silobreaker.com/&lt;/a>) &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This data was preprocessed dropping the columns &amp;lsquo;Id&amp;rsquo;, &amp;lsquo;ClusterId&amp;rsquo;, &amp;lsquo;Language&amp;rsquo;, &amp;lsquo;LastUpdated&amp;rsquo;,&amp;lsquo;CreatedDate&amp;rsquo;,&amp;lsquo;FirstReported&amp;rsquo;. Also, stop words and punctuation were removed, we convert to lower case all the titles.&lt;/p>
&lt;p>The dataset already query can be download in my personal drive [Here](&lt;a href="https://drive.google.com/drive/u/0/folders/1Omtnn5e-yH3bbhW0-5fIbLgi8SEyfYBP">https://drive.google.com/drive/u/0/folders/1Omtnn5e-yH3bbhW0-5fIbLgi8SEyfYBP&lt;/a>.&lt;/p>
&lt;h2 id="4-results">4. Results&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> introduction paragraph missing&lt;/li>
&lt;/ul>
&lt;p>The following figures observed were create with different techniques used in this project with the help of libraries like gemsim. Each figure is described and explains the method we used to created it along with the relationship of the key word or major topic to the Hashimoto disorder.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/wordCloudObject.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> &amp;ldquo;Example of a Word Cloud Object&amp;rdquo;&lt;/p>
&lt;p>On Figure 1 we observed an example of a word cloud object and represent the difference words found in our dataset and the size of the words means the frequency of the given words in the document. Meaning that the size of the words is proportional to the frequency of its used.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/IntertopicDistanceMap.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> &amp;ldquo;Example of a Intertopic Distance Map&amp;rdquo;&lt;/p>
&lt;p>Figure 2 shows an Intertopic Distance Map which is a two-dimensional space filled with circles representing the proportional number of words that belongs to each topic making the distance to each other represent the relation between the topics, meaning that topics that are closer together have more words in common. For instance, in topic 1 we observed word like hypothyroidism, Morgan, symptoms after a small search we were able to find that Morgan is a well-known writer that presented thyroiditis symptoms after giving birth which is something that happen to some women’s and then recover after a couple of months, however this increments the risk of developing the syndrome later in their lives &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. On topic 4 we see words like food, levothyroxine, liothyronine, selenium and dietary. the relationship between these words is symptom control, symptoms relive, some natural remedies and supplements &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>&lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/topic%20modeling%20picture.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> &amp;ldquo;Top 30 major Topics&amp;rdquo;&lt;/p>
&lt;p>On figure 3 we observed a bar chart that shows 30 major terms. The bars indicate the total frequency of the term across the entire corpus. The size of the bubble measures the importance of the topics, relative to the data. for example, for visualization purposes we used the first topic that include Hashimoto, thyroiditis, and selenium. Saliency is a measure of how much the term talks about the topic. And in terms of findings is important to mentions the relationship between Hashimoto thyroiditis and selenium. Selenium is a suplement recomended for patients with this disorder that have shown a reduction on antibody levels &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/HierarchicalClustering.png" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> &amp;ldquo;Example of Hierarchical Clustering chart&amp;rdquo;&lt;/p>
&lt;p>On figure 4 we can see that the dendrograms have been created joining points 4 with 9, 0 with 2, 1 with 6, and 12 with 13. The vertical height of the dendrogram shows the Euclidean distances between points. It is easy to see that Euclidean distance between points 12 and 13 is greater than the distance between point 4 and 9. This is because the algorithm is clustering by similarity, differences, and frequency of words. We observed in the dark green dendrogram topic 7,3,4,9 which are all related to an advance stage of the disorder. we can find the information about certain treatments, causes of the disorder, level of damage at certain stages. On the reds dendrograms we observe topics 0,2,1,6 which are closely related to diagnosis, early symptoms and procedures used for the diagnosis of the disorder.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/SimilarityMatrix.png" alt="Figure 5">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> &amp;ldquo;Exaple of Similarity Matrix Chart&amp;rdquo;&lt;/p>
&lt;p>On figure 5 we can see a similarity matrix chart, the graph is build based on similarity reached from the volume of topic and association by document, therefore the graph show groups of documents that are cluster together based on similarities. in this case the blue square is an indication of a strong similarity, and the green and light green is an indication of different topics. for instance, we are able to derive as a conclusion that carcinoma cancer, carcinoma therapy, lymph papillary metastasis and hypothyroidism are closely related. in facts they are advance stages of the disorder. E.g. Carcinoma therapy is a type of treatment that can be used for this disorder &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/TermScoreDeclinePerTopic.png" alt="Figure 6">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> &amp;ldquo;Example of Term Score Decline Per Topic Chart&amp;rdquo;&lt;/p>
&lt;p>On figure 6 we observed TF-IDF which is an interesting technic used on machine learning that have the ability to give weight to those words that are not frequent in the document but can carry important information. In this example we can see how topic 12, covid19 pandemic patients is the at the top of the chart and then start declining when the rank term increase. The science behind this behave is explain by the TF-IDF which is term frequency - Inverse document frequency. Therefore, covid 19 was a relative new disease, and we do not expect to have a high frequency used in the document. In this case we were able to find information about Hashimoto patients and covid19 which it seems not to causes any extreme symptoms for patient with this disorder others than the ones expected from a healthy person in other words Hashimoto patients have the same risk of a healthy person &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/TopicProbability.png" alt="Figure 7">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> &amp;ldquo;Example of Topic Probability chart&amp;rdquo;&lt;/p>
&lt;p>On figure 7 we see a probability distribution chart based on each topic frequency and its relationship with the main topic: Hashimoto thyroiditis causes or cure. We can see that topic 12 is the least frequent or least related since most of its content is about covid19. Then we have topic 11 zebrafish which is related to the investigation of the disorder but most of its content is about the research made on zebrafish and how had help researchers to understand thyroid diseases in other no mammals’ animals, but is not closely related to the major point of this project, however, is an interesting research which have provide useful information about thyroiditis &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-372/main/project/images/topicwordscore.png" alt="Figure 8">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> &amp;ldquo;Example of a Topic Word Score Chart&amp;rdquo;&lt;/p>
&lt;p>On figure 8 we have Topic Word Scores chart that provides a deep understanding of large corpus of texts trough topic extraction. for instance, the data used in this project provide 5 fundamental topics from 0 to 4. Essentially each topic provided closely related words with deep information about the disorder itself, treatments, diagnosis, and symptoms. E.g. in topic number 4 we find a specific word &amp;ldquo;eye&amp;rdquo; which it does not seem to have a close relationship with Hashimoto thyroiditis but in facts is related to one of the early symptoms that the human body experiment most likely when is still undiagnosed [16]. In the same topic we also find the word teprotumumab which is an eye relieve medication recommended from doctors to relive the symptoms, in other word is not the cure but it helps &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="5-hashimoto-findings">5. Hashimoto Findings&lt;/h2>
&lt;p>As we can see our findings are wide in aspects of causes which is one of the main keys, because if we know the cause of something most likely we will be able to avoid it. However, this disorder is considered relative knew and have been around for some decades only, but it is necessary to point out the relation of diseases with the environment. Environmental changes are a fact and are affecting us every day even when we don’t notice it. We have seen an exponential increase of Hashimoto cases in the last five decades, and at the same time the last five decades have been potentially related to climate change, high levels of pollution, less fertile soils, increased use of pesticides on food, etc. It would be a good idea to think about our environment and how to help it heal since it will bring benefits for all of us&lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Table Summary:&lt;/strong> &amp;ldquo;Finding summary on causes, descriptions and recommendations.&amp;rdquo;&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Possible Causes&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Recomendations&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Genetic predispositionsc&lt;/td>
&lt;td>Genetically linked&lt;/td>
&lt;td>stress levels&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dietary errors&lt;/td>
&lt;td>Imbalance of iodine intake&lt;/td>
&lt;td>Balance is key&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Nutritional deficiencies&lt;/td>
&lt;td>not enough veggies, vitamins and minerals&lt;/td>
&lt;td>yoga, meditation reiki&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hormone deficiencies&lt;/td>
&lt;td>lover levels of vit D&lt;/td>
&lt;td>Enough sleep&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Viral, bacterial, yeast, and parasitic infections.&lt;/td>
&lt;td>H pylori, etc.&lt;/td>
&lt;td>food hygiene&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Possible causes&lt;/strong>&lt;/p>
&lt;p>Genetic predispositions, Dietary errors, Nutritional deficiencies, Hormone deficiencies, Viral, bacterial, yeast, and parasitic infections &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Hashimoto Trigger Food&lt;/strong>&lt;/p>
&lt;p>Some Food that can trigger Hashimoto are gluten, dairy, some type of grains, eggs, nuts or nightshades, sugar, sweeteners, sweet fruits, including honey, agave, maple syrup, and coconut sugar and high-glycemic fruits like watermelon, mango, pineapple, grapes, canned and dried fruits. Vegetable oil, specially hydrogenated oils, ad trans-fat. Patient with this disorder may experience symptoms of fatigue, rashes, joint pain, digestive issues, headaches, anxiety, and depression after eating some of these foods &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Hashimoto recommended Diets&lt;/strong>&lt;/p>
&lt;p>The recommended foods are healthy fats like coconut, avocado, and olive oil, ghee, grass-fed and organic meat, wild fish, healthy fats, fermented foods like coconut yogurt, kombucha, fermented cucumbers and pickle ginger, and plenty of vegetable like Asparagus, spinach, lettuce, broccoli, beets, cauliflower, carrots, celery, artichokes, garlic, onions &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Environmental causes of Hashimoto&lt;/strong>&lt;/p>
&lt;p>There have been an increase in the number of Hashimoto cases in the United States since 1950s. These is one of the reason research explain that Hashimoto disorder can be closely related to environmental causes since the rapid increase of cases can not only be related to family gens as it takes at least two generations to acquire and transfer gen mutation. Adding to this that for generation thru history human have been fitting microorganisms than enter our body but for the past centuries our environment has become very hygienic consequently our immune system suddeling was left without aggressors therefore humane start developing more allergies and autoimmune diseases.
Another important factor is the balance of iodine intake because too much is as dangerous for people with genetic Hashimoto predisposition but too littler can be also dangerous for patients with the disorder to reduce goiter which is the enlargement of the thyroid glands &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>It is still not enough research to state that low vitamin D levels are a cause or a consequence of the Hashimoto disorder, but it is a fact that most patients with this disorder have low levels of vitamin D this insufficient this is closely related to insufficient sun exposure &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The exposure to certain synthetic pesticide. An important fact is that 9 out of 12 pesticides are dangerous and persistent pollutants &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Symptoms of Hashimoto’s&lt;/strong>&lt;/p>
&lt;p>Some of the symptoms are fatigue and sluggishness, sensitivity to cold, constipation, pale and dry skin, dry eyes, puffy face, brittle nails, hair loss, enlargement of the tongue, unexplained weight gain, muscle aches, tenderness and stiffness, joint pain and stiffness, muscle weakness, excessive or prolonged menstrual bleeding, depression, memory lapses, Another symptom reported by some patients was ablation, some patient described as an acceleration of the heart rhythm &lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Complications&lt;/strong>&lt;/p>
&lt;p>Tissue damage, Abnormal look of the thyroid gland (figure 2), goiter, Heart problems, mental health issues, myxedema, birth defects &lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>, Nodule (figure 4 Similarity Matrix topic 3), and High antibody level. It is important to mention an association between high levels of thyroid autoantibodies and the increased of mood disorders, thyroid autoimmunity disease, celiac disease, panic disorder and major depressive disorder &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Recomendations&lt;/strong>&lt;/p>
&lt;p>Healthy diets, exercising, selenium supplementation [8], healthy sun exposure at an adequate time, getting enough sleep is primordial for the human body, in special for the metabolism regulation and the creation of normal hormones that the human body needs, &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> lowering stress levels by physical exercise is a good idea, exercise like yoga and reiki are valuable because it also exercise you brain with meditation which is a great stress reliever.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>We used benchmark to perform the process time to get topics frequency in parallel using google colab with run type: GPU and TPU. We can observe that TPU machines take less time to classify topic 1. Tensor Processor Unit (TPU) is designed to run cutting-edge machine learning models with AI services on Google Cloud &lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup>&lt;/p>
&lt;p>&lt;strong>Benchmark Topics Frequency:&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>parallel Topic&lt;/th>
&lt;th>Status&lt;/th>
&lt;th>Time&lt;/th>
&lt;th>processor&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>164 1_cancer_follicular_carcinoma_autoimmune&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.53&lt;/td>
&lt;td>GPU&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>190 1_cancer_follicular_carcinoma_autoimmune&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.002&lt;/td>
&lt;td>TPU&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>As expected, we were able to derive helpful information of the Hashimoto thyroiditis disorder. we attempted to summarize our findings concerning Hashimoto thyroiditis in aspects of causes, symptoms, recommended diets and supplements and used medication.&lt;/p>
&lt;p>Our findings highlight the great potential of the model we used. certainly, topic modeling method was a precise idea for the optimization of the research process. We also used various features of genism, which allows to manipulate data texts on NPL projects. The use of clustering technics was very useful to label our findings on the large datasets. Each used graph provided useful details and key words that later help us to review each important topic in a faster manner and develop the research project with accurate results.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Gregor von Laszewski&lt;/p>
&lt;p>Yohn J Parra&lt;/p>
&lt;p>Carlos Theran&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, [Online resource]
&lt;a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Helicobacter pylori infection in women with Hashimoto thyroiditis, [Online resource]
&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5265752/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5265752/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>I. Voloshyna, V.I Krivenko, V.G Deynega, M.A Voloshyn, Autoimmune thyrod disease related to helicobacterer pylori contamination, [Online resource]
&lt;a href="https://www.endocrine-abstracts.org/ea/0041/eposters/ea0041gp213_eposter.pdf">https://www.endocrine-abstracts.org/ea/0041/eposters/ea0041gp213_eposter.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>How your diet can trigger Hashimoto&amp;rsquo;s, [Online resource]
&lt;a href="https://www.boostthyroid.com/blog/2019/4/5/how-your-diet-can-trigger-hashimotos">https://www.boostthyroid.com/blog/2019/4/5/how-your-diet-can-trigger-hashimotos&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Hypothyroidism in Context: Where We’ve Been and Where We’re Going, [Online resource]
&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822815/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822815/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>BERT Explained: State of the art language model for NLP
&lt;a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Pavan Sanagapati, Knowledge Graph &amp;amp; NLP Tutorial-(BERT,spaCy,NLTK), [Online resource]
&lt;a href="https://www.kaggle.com/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk">https://www.kaggle.com/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Hashimoto’s Thyroiditis, A Common Disorder in Women: How to Treat It, [Online resource]
&lt;a href="https://www.townsendletter.com/article/441-hashimotos-thyroiditis-common-disorder-in-women/">https://www.townsendletter.com/article/441-hashimotos-thyroiditis-common-disorder-in-women/&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Silobreaker: Intelligent platform for the data era
&lt;a href="https://www.silobreaker.com">https://www.silobreaker.com&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Julia Haskins, Thyroid Conditions Raise the Risk of Pregnancy Complications, [Online resource]
&lt;a href="https://www.healthline.com/health-news/children-thyroid-conditions-raise-pregnancy-risks-052913">https://www.healthline.com/health-news/children-thyroid-conditions-raise-pregnancy-risks-052913&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>How your diet can trigger Hashimoto&amp;rsquo;s, [Online resource]
&lt;a href="https://www.boostthyroid.com/blog/2019/4/5/how-your-diet-can-trigger-hashimotos">https://www.boostthyroid.com/blog/2019/4/5/how-your-diet-can-trigger-hashimotos&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Selenium Supplementation for Hashimoto&amp;rsquo;s Thyroiditis, [Online resource]
&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4005265/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4005265/&lt;/a>&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Thyroid Cancer Treatment, [Online resource]
&lt;a href="https://www.cancer.gov/types/thyroid/patient/thyroid-treatment-pdq">https://www.cancer.gov/types/thyroid/patient/thyroid-treatment-pdq&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Hashimoto&amp;rsquo;s Disease And Coronavirus (COVID-19), [Online resource]
&lt;a href="https://www.palomahealth.com/learn/coronavirus-and-hashimotos-disease">https://www.palomahealth.com/learn/coronavirus-and-hashimotos-disease&lt;/a>&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>How zebrafish research has helped in understanding thyroid diseases, [Online resource]
&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5730863/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5730863/&lt;/a>&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16" role="doc-endnote">
&lt;p>Teprotumumab for the Treatment of Active Thyroid Eye Disease, [Online resource]
&lt;a href="https://www.nejm.org/doi/full/10.1056/nejmoa1910434">https://www.nejm.org/doi/full/10.1056/nejmoa1910434&lt;/a>&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17" role="doc-endnote">
&lt;p>11 environmental triggers of Hashimoto’s, [Online research]
&lt;a href="https://www.boostthyroid.com/blog/11-environmental-triggers-of-hashimotos">https://www.boostthyroid.com/blog/11-environmental-triggers-of-hashimotos&lt;/a>&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18" role="doc-endnote">
&lt;p>Hashimoto’s low thyroid autoimmune, [Online research]
&lt;a href="https://www.redriverhealthandwellness.com/diet-hashimotos-hypothyroidism/">https://www.redriverhealthandwellness.com/diet-hashimotos-hypothyroidism/&lt;/a>&amp;#160;&lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19" role="doc-endnote">
&lt;p>Hashimoto&amp;rsquo;s disease, [Online research]
&lt;a href="https://www.mayoclinic.org/diseases-conditions/hashimotos-disease/symptoms-causes/syc-20351855">https://www.mayoclinic.org/diseases-conditions/hashimotos-disease/symptoms-causes/syc-20351855&lt;/a>&amp;#160;&lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20" role="doc-endnote">
&lt;p>TPU: Tensor Processor Unit
&lt;a href="https://cloud.google.com/tpu">https://cloud.google.com/tpu&lt;/a>&amp;#160;&lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: Classification of Hyperspectral Images</title><link>/report/su21-reu-360/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-360/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-360/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-360/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-360/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-360/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Carlos Theran, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-360">su21-reu-360&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-360/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>??
Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-report-format">2. Report Format&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-github-actions">2.1. GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-report-or-project">2.3. Report or Project&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-datasets">5. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Do not include this tip in your document:&lt;/p>
&lt;blockquote>
&lt;p>Tip: Please note that an up to date version of these instructions is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md">https://github.com/cybertraining-dsc/hid-example/blob/main/project/index.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Here comes a convincing introduction to the problem&lt;/p>
&lt;h2 id="2-report-format">2. Report Format&lt;/h2>
&lt;p>The report is written in (hugo) markdown and not commonmark. As such some features are not visible in GitHub. You can
set up hugo on your local computer if you want to see how it renders or commit and wait 10 minutes once your report is
bound into cybertraining.&lt;/p>
&lt;p>To set up the report, you must first &lt;code>replace&lt;/code> the word &lt;code>hid-example in this example report with your hid. the hid will look something like &lt;/code>sp21-599-111`&lt;/p>
&lt;p>It is to be noted that markdown works best if you include an empty line before and after each context change.
Thus the following is wrong:&lt;/p>
&lt;pre>&lt;code># This is My Headline
This author does ignore proper markdown while not using empty lines between context changes
1. This is because this author ignors all best practices
&lt;/code>&lt;/pre>&lt;p>Instead, this should be&lt;/p>
&lt;pre>&lt;code># This is My Headline
We do not ignore proper markdown while using empty lines between context changes
1. This is because we encourage best practices to cause issues.
&lt;/code>&lt;/pre>&lt;h2 id="21-github-actions">2.1. GitHub Actions&lt;/h2>
&lt;p>When going to GitHub Actions you will see a report is autmatically generated with some help on improving your markdown.
We will not review any document that does not pass this check.&lt;/p>
&lt;h2 id="22-past-copy-from-word-or-other-editors-is-a-disaster">2.2. PAst Copy from Word or other Editors is a Disaster!&lt;/h2>
&lt;p>We recommend that you sue a proper that is integrated with GitHub or you use the commandline tools. We may include
comments into your document that you will have to fix, If you juys past copy you will&lt;/p>
&lt;ol>
&lt;li>Not learn how to use GitHub properly and we deduct points&lt;/li>
&lt;li>Overwrite our coments that you than may miss and may result in point deductions as you have not addressed them.&lt;/li>
&lt;/ol>
&lt;h2 id="23-report-or-project">2.3. Report or Project&lt;/h2>
&lt;p>You have two choices for the final project.&lt;/p>
&lt;ol>
&lt;li>Project, That is a final report that includes code.&lt;/li>
&lt;li>Report, that is a final project without code.&lt;/li>
&lt;/ol>
&lt;p>YOu will be including the type of the project as a prefix to your title, as well as in the Type tag
at the beginning of your project.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-using-itemized-lists-only-where-needed">4. Using itemized lists only where needed&lt;/h2>
&lt;p>Remember this is not a powerpoint presentation, but a report so we recommend&lt;/p>
&lt;ol>
&lt;li>Use itemized or enumeration lists sparingly&lt;/li>
&lt;li>When using bulleted lists use * and not -&lt;/li>
&lt;/ol>
&lt;h2 id="5-datasets">5. Datasets&lt;/h2>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Tutorial on Installing Visual Studio Code</title><link>/report/su21-reu-361/tutorials/visual-studio-code/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-361/tutorials/visual-studio-code/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This tutorial teaches how to install Visual Studio Code on Windows 10.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#steps">Steps&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> visual-studio-code&lt;/p>
&lt;h2 id="steps">Steps&lt;/h2>
&lt;p>Click the following image to be redirected to a YouTube video tutorial for installing Visual Studio Code (also called VSCode).&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/DG_wQslWWAc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Sidenote: An exasperated reader may wonder, &amp;ldquo;why go through steps 1-3 when it can be as easy as clicking a link to the VSCode download page?&amp;rdquo; This &lt;em>would&lt;/em> be easier, but hyperlinks (or URLs) are bound to change through the years of website maintenance and alterations. (One could also argue that steps 1-3 could become incorrect, as well, but hopefully they will not.) If you, time-traveler, would like to try your luck, go here: &lt;a href="https://code.visualstudio.com/download">https://code.visualstudio.com/download&lt;/a>&lt;/p>
&lt;p>If the link works, skip to step 4.&lt;/p>
&lt;p>P.S. It should be second-nature to a user to quickly search, find, download, and install a program. It is vital to ensure that the correct program is downloaded and installed, however. Over time, guides like this one can become deprecated, but one must be resilient in problem-solving. Use search engines like Google to find what you are looking for. If one path does not work, take another that will lead to the same destination or a better one.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Open up your favorite web browser. This can be done by pressing the Windows key and typing in the name of the browser, like &lt;code>google chrome&lt;/code> (as long as this browser is already installed on your computer). Then press Enter.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the browser loads, search for &lt;code>visual studio code&lt;/code> through the address bar. Press Enter and you will see a list of results through the default search engine (Google, Bing, or whatever is configured on your browser).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Identify the result that reads &lt;code>code.visualstudio.com&lt;/code>. If using Google, a subresult should read &lt;code>Download&lt;/code>. Click that link.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>This tutorial assumes that the reader is using Windows. Click the blue link that reads &lt;code>Windows&lt;/code>. The download will commence; wait for it to finish.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click and open the file once it finishes; the license agreement will appear. If you are proficient in legalese, you can read the wall of text. Then, click &lt;code>I accept the agreement&lt;/code> and click &lt;code>Next&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Next&lt;/code> again; it is best to leave the default install path alone for reproducibility in this experiment.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Next&lt;/code> again to create a Start Menu folder. Ensure that &lt;code>Add to PATH&lt;/code> is checked. &lt;code>Create a desktop icon&lt;/code> can be checked for convenience; it is up to the reader&amp;rsquo;s choice. Then click &lt;code>Next&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click Install and watch the green progress bar go at the speed of light. Once completed, click Finish. VSCode will open as long as everything went smoothly.&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Report: Project: Detecting Multiple Sclerosis Symptoms using AI</title><link>/report/su21-reu-371/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-371/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-371/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-371/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-371/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-371/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Raeven Hatcher, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-371">su21-reu-371&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-371/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Multiple sclerosis (M.S.) is a chronic central nervous system disease that potentially affects the brain, spinal cord, and optic nerves in the eyes. People that suffer from M.S had their immune system attacks the myelin (protective sheath) that covers nerve fibers, resulting in communication problems between the brain and the body. The cause of M.S. is unknown; however, researchers believe that genetic and environmental factors play a role in those affected. Symptoms differ significantly from person to person due to varying nerves involved. The most common symptoms include tremors, numbness or weakness in limbs, vision loss, blurry vision, double vision, slurred speech, fatigue, dizziness, involuntary movement, and muscle paralysis. There is currently no cure for Multiple sclerosis and treatment focuses on slowing the progression of the disease and managing symptoms.&lt;/p>
&lt;p>There is no proven way to predict how an individual with M.S will progress certainly. However, researchers established four phenotypes that will assist in identifying those who are more inclined to have disease progression and help aid in more effective treatment targeting. In this experiment, Artificial Intelligence (AI) will be applied by ascertaining what causes these different phenotypes and which phenotype is at most risk for disease progression using a Magnetic Resonance Scan.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-datasets">4. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-benchmark">5. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgments">7. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>MS or Multiple sclerosis is a potentially disabling autoimmune disease that can damage the brain, spinal cord, and optic nerves located in the eyes. It is the most common progressive neurological disability that affects adolescents. The immune system attacks the central nervous system in this disease, specifically myelin (sheth that covers and protects nerve fibers), oligodendrocytes (myelin-producing cells), and the nerve fibers located under myelin. Myelin enables nerves to send and receive electrical signals swiftly and effectively. The myelin sheath becomes scarred from being attacked. These attacks make the myelin sheath inflamed in little patches, observable on an MRI scan. These little inflamed patches potentially disrupt messages moving along the nerves, which ultimately lead to the symptoms of Multiple sclerosis. If these attacks happen frequently, permanent damage can occur to the involved nerve.&lt;br>
Because Multiple sclerosis affects the central nervous system, which control all of the actions carried out in the body, symptoms can affect any part of the body and vary. The most common symptoms of this progressive disease include muscle weakness, pins and needle sensation, electrical shock sensation, loss of bladder control, muscle spasms, tremors, double or blurred vision, partial or total vision loss, to name a few. Researchers are not sure what causes Multiple sclerosis but believe those between the ages of 20 and 40, women, smoke, are exposed to certain infections, have a vitamin D and B12 deficiency, and related to someone affected by this disease are more susceptible.&lt;/p>
&lt;p>It can be difficult to diagnose MS due to the symptoms usually being vague or very similar to other conditions. There is no single test to diagnose it positively. However, doctors can choose a neurological examination, MRI scan, evoked potential test, lumbar puncture, or a blood test to diagnose a patient properly. Currently, clinical practices divide MS into four phenotypes: clinically isolated syndrome (CIS), relapsing-remitting MS (RRMS), primary-progressive MS (PPMS), and secondary progressive MS (SPMS). Two factors define these phenotypes; disease activity (evidenced by relapses or new activity on MRI scan) and progression of disability. Phenotypes are routinely used in clinical trials to choose patients and conduct treatment plans.&lt;/p>
&lt;p>New technologies, such as artificial intelligence and machine learning, help assess multidimensional data to recognize groups with similar features. When implemented in apparent abnormalities on MRI scans, these new technologies have assured promising results in classifying patients who share similar pathobiological mechanisms rather than the typical clinical features.&lt;/p>
&lt;p>Researchers at UCL work with the Artificial intelligence (AI) tool SuStain (Subtype and Stage Inference) to ask whether AI can find Multiple sclerosis subtypes that follow a particular pattern on brain images? The results uncovered three data-driven MS subtypes defined by pathological abnormalities seen on brain images (Skylar). The three data-driven MS subtypes are cortex-led, normal-appearing WM-led, Lesion-led. Cortex-led MS is characterized by early tissue shrinkage (atrophy) in the outer layer of the brain. Normal-appearing WM-led is identified by irregular diffused tissue located in the middle of the brain. Lastly, a lesion-led subtype is characterized by early extension accumulation of brain damage areas that lead to severe atrophy in numerous brain regions. All three of these subtypes correlate to the earliest abnormalities observed on an MRI scan within each pattern.&lt;/p>
&lt;p>In this experiment, researchers utilized the SuStain tool to capture MRI scans of 6,332 patients. The unsupervised SuStain taught itself and identified those three patterns that were previously undiscovered.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://user-images.githubusercontent.com/85815818/126948339-7723b810-83e4-463b-a2b0-bf417fac4458.jpg" alt="projectpic2">
The above image shows the MRI-based subtypes. The color shades range from blue to pink, representing the probability of abnormality mild to severe, respectively. (Eshaghi)
&lt;img src="https://user-images.githubusercontent.com/85815818/127168907-4d7e444f-14bd-4ee3-8027-a7b0e5b7d213.jpg" alt="INSERTPIC">&lt;/p>
&lt;h2 id="4-datasets">4. Datasets&lt;/h2>
&lt;p>MRI brain scans of 6,322 MS patients. look if you can find figures descrbing the data.&lt;/p>
&lt;h2 id="5-benchmark">5. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> define method that will be used to study the performance of the proposed method&lt;/li>
&lt;/ul>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A vital barrier in distinguishing subtypes in Multiple sclerosis is to stitch observations together from cross-sectional or longitudinal studies. Grouping individuals based wholly on their MRI scan is ineffective because patients belonging to the same subgroup could show ranging abnormalities as the disease progresses and would appear different. SuStaIn, Subtype and Staging Inference, a newly developed unsupervised machine learning algorithm aids in uncovering data-driven disease subtypes that have distinct temporal progression patterns. &amp;ldquo;The ability to disentangle temporal and phenotypic heterogeneity makes SuStain different from other unsupervised learning or clustering algorithms&amp;rdquo; (Eshaghi). SuStaIn identifies subtypes given the data, defined by a particular pattern of variation in a set of features, such as MRI abnormalities. Once the SuStain subtypes and their MRI trajectories are adequately identified, the disease model can conclude how approximately a patient, whose MRI is unseen, belongs to each of the three subtypes and stages.&lt;/p>
&lt;p>A total of 9,390 patients participated in this research study. Six thousand three hundred twenty-two patients were utilized in training, and 3,068 patients were used for the validation dataset. Patient characteristics such as sex, age, disease duration, and expanded disability status scale (EDSS) were similar between the training and validation dataset. There were 18 MRI features measured, 13 of those differed dramatically from those between the MS training dataset and control group and were maintained in the SustaIn model. Three subtypes, with very distinct patterns, were identified in the training dataset and validated in the validation dataset. The early abnormalities noticed by SuStain helped define the three subtypes: cortex-led, normal-appearing white matter-led, and lesion-led.&lt;/p>
&lt;p>There was a statistically significant difference in the rate of the disease progression between the subtypes in the training dataset and validation datasets. The lesion-led subtype held a 30% higher risk of developing 24-week confirmed disability progression (CDP) than the cortex-led subtype in the training dataset. The lesion-led validation dataset had a 32% higher risk of confirmed disability progression than the cortex-led subtype. No other differences in the advancement of disability between subtypes were noted. When SuStaIn was applied to the training and validation dataset, it was pointed out that there were differences in the risk of disability progression between SuStaIn stages.&lt;/p>
&lt;p>Each MRI-based subtype had a different response to treatment, comparing those on treatment and those on placebo. The lesion-led subtype showed a remarkable response to the treatment. Patients on the lesion-led active treatment subtype showed a significantly slower worsening of EDSS than those on the placebo. No differences in the rate of EDSS were observed in those on the placebo compared to active treatment in the NAWM-led and cortex-led subtypes.&lt;/p>
&lt;p>When SuStain was applied to a large set of Multiple sclerosis scans, it identified three subtypes. Researchers found out the patient&amp;rsquo;s baseline subtype and stage were associated with an increased risk of disease progression. Combining clinical information with the MRI-based three subtypes increased the predictive accuracy of just using the MRI scan information alone. The patterns of MRI abnormality in these subtypes provide perspicacity into disease mechanisms, and, alongside clinical phenotypes, they may aid the stratification of patients for future studies.&lt;/p>
&lt;h2 id="7-acknowledgments">7. Acknowledgments&lt;/h2>
&lt;p>-Gregor von Laszewski
-Yohn Jairo
-Carlos Theran&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Mayo Foundation for Medical Education and Research. (2020, June 12). Multiple sclerosis. Mayo Clinic. &lt;a href="https://www.mayoclinic.org/diseases-conditions/multiple-sclerosis/symptoms-causes/syc-20350269">https://www.mayoclinic.org/diseases-conditions/multiple-sclerosis/symptoms-causes/syc-20350269&lt;/a>.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Project: AI in Orthodontics</title><link>/report/su21-reu-363/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-363/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-363/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-363/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-363/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-363/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Whitney McNair, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-363">su21-reu-363&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-363/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>In this effort we are analzying X-ray images in AI and identify cavitites&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-data-sets">2. Data Sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-example-of-a-ai-algorighm-in-orthodontics">5. Example of a AI algorighm in Orthodontics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ai, orthodontics, x-rays.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Dental field technology capability has increased over the past 25 years, and has helped reduce time, cost, medical errors, and dependence on human expertise[1]. Intelligence in orthodontics can learn, build, remember, understand and recognize designs from techniques used in correcting the teeth like retainers. Dental field can create alternatives, adapt to change and explore experiences with sub-groups of patients[reference]. AI has taken part of the dental field by accurately and efficiently processes the best data from treatments. For smart use of Health Data, machine learning and artificial intelligence (AI) are expected to promote further development of the digital revolution in (dental) medicine using algorithms to simulate human cognition in the analysis of complex data. The performance is better, the higher the degree of repetitive pattern and the larger the amount of accessible data[5].&lt;/p>
&lt;h2 id="2-data-sets">2. Data Sets&lt;/h2>
&lt;p>We found a dataset in kaggle website that is about dental images. The data was collected by Mr. Parth Chokhra. The name of the dataset is Dental Images of kjbjl. The dataset did not have metadata and an explanation of how they collected the data. The data set supports how x-rays of teeth in dentistry becomes artificial intelligence. The Dental Images of kjbjl dataset was used in AI already using autoencoders. Autoencoders are an freely artificial neural network (located in the nervous system) that learns how to accurately encode data and reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.&lt;/p>
&lt;p>Describe the data that was in the paper.&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> What are the challenges with Orthodontics data set? Privacy, HIPAA, Size, Avalibility
Some challenges with Orthodontics data sets with privacy&lt;/li>
&lt;/ul>
&lt;p>explain how its hard to find&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/li>
&lt;/ul>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Place a cool image into projects images in my directory&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Correct the following link, replace the fa number with my su number and thne chart of png.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> If the image has been copied, you must use a reference such as shown in the Figure 1 caption.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Place image of x ray and url of the kaggle images. Talk about what and how etc. the images are about and look like and what you can do with them.&lt;/p>
&lt;h2 id="5-example-of-a-ai-algorighm-in-orthodontics">5. Example of a AI algorighm in Orthodontics&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete data sets that will be used.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Identify the concrete algorighm to analyze the data sets.&lt;/li>
&lt;/ul>
&lt;p>Read over google scholar review page on orthondontics about their research and algorighm (if used).&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>Talk about the matrics in the google scholar review says. The matrics that the researchers are using to study the performance of their algorighns or code.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;p>What the researchers have done in conclusion. &amp;ldquo;&amp;hellip; used these algorigns to&amp;hellip;.&amp;rdquo;&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Gregor guided me throughout this process.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Carlos guided me throughout this process.&lt;/li>
&lt;/ul>
&lt;p>Please add acknowledgments to all that contributed or helped on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/li>
&lt;/ul>
&lt;p>Change 6 reference and replace with google scholar research paper.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Use of energy explained - Energy use in homes, [Online resource]
&lt;a href="https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php">https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>&amp;lt;Bichu, Y.M., Hansa, I., Bichu, A.Y. et al. Applications of artificial intelligence and machine learning in orthodontics: a scoping review. Prog Orthod. 22, 18 (2021).&amp;gt;&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: AI Time Series Analysis</title><link>/report/su21-reu-374/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-374/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-374/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-374/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;ul>
&lt;li>[*] syntax for refences wron, see original example&lt;/li>
&lt;li>[*] never ever use the word I or my in your report&lt;/li>
&lt;/ul>
&lt;p>Zyion Morris, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-374">su21-reu-374&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-374/edit/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Here comes a short abstract of the project that summarizes what it is about&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-benchmark">4. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgments">6. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> AI, ML, DL, Cybersecurity, Cyber Attacks, IoT.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Concurrently, research on AI Time analysis includes query on stock market data stucture.
The mean (level), maximum, minimum values; Time Series Data Components - trend, seasonality, noise or randomness, a curve, and the level.
Blockchain levies to be the coin flip of how Artificial Intellignece predicts future prices through machine learning.
numpy, matplotlib, pandas, cloudmesh-common, cloudmesh-cmd5s, are codes existing that demonstrate the use of blockchain with AI using Python.
What is the code in this project that will be developed?
What is the performance of this code?
What is one conclusion utilizing the source code?
*Never, ever use the words ‘I’ or ‘my’ in a report&lt;/p>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;p>This report pertains a .CSV file of a dataset predicting the consumption of electricity in the coming future. &lt;a href="https://github.com/cybertraining-dsc/su21-reu-374/files/6743411/archive.zip">archive.zip&lt;/a>&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Place a cool image into projects images in my directory&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Correct the following link, replace the fa number with my su number and thne chart of png.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> If the image has been copied, you must use a reference such as shown in the Figure 1 caption.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-374/raw/main/project/images/image.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Images can be included in the report, but if they are copied you must cite them [^1].&lt;/p>
&lt;h2 id="4-benchmark">4. Benchmark&lt;/h2>
&lt;p>*Your project must include a benchmark. The easiest is to use cloudmesh-common [^2]&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>*A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="6-acknowledgments">6. Acknowledgments&lt;/h2>
&lt;p>*Needs full sentence&lt;/p>
&lt;ul>
&lt;li>Gregor von Laszewski&lt;/li>
&lt;li>Yohn Jairo Bautista&lt;/li>
&lt;li>Carlos Theran&lt;/li>
&lt;/ul>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Marco Iansiti and Karim R. Lakhani, The Truth About Blockchain, [Online resource] &lt;a href="https://hbr.org/2017/01/the-truth-about-blockchain">https://hbr.org/2017/01/the-truth-about-blockchain&lt;/a> ↩︎&lt;/p>
&lt;ol start="2">
&lt;li>
&lt;p>Jeremy Swinfen Green, Understanding cryptocurrency market fluctuations, [Online resource] &lt;a href="https://www.telegraph.co.uk/business/business-reporter/cryptocurrency-market-fluctuations/">https://www.telegraph.co.uk/business/business-reporter/cryptocurrency-market-fluctuations/&lt;/a> ↩︎&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Mehmet Tarik Akcay, Historical Data for Top 20 Coins by Market Cap, [Online resource] &lt;a href="https://www.kaggle.com/mtakcy/historical-data-for-top-20-coins-by-market-cap?select=eos.csv">https://www.kaggle.com/mtakcy/historical-data-for-top-20-coins-by-market-cap?select=eos.csv&lt;/a> ↩︎&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol></description></item><item><title>Report: Time Series Analysis of Blockchain-Based Cryptocurrency Price Changes</title><link>/report/su21-reu-361/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-361/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-361/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-361/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Jacques Fleischer, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-361">su21-reu-361&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Code:
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/blob/main/project/code/README.md">Install documentation README.md&lt;/a>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/blob/main/project/code/yfinance-lstm.ipynb">yfinance-lstm.ipynb&lt;/a>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This project applies neural networks and Artificial Intelligence (AI) to historical records of high-risk cryptocurrency coins to train a prediction model that guesses their price. The code in this project contains Jupyter notebooks, one of which outputs a timeseries graph of any cryptocurrency price once a csv file of the historical data is inputted into the program. Another Jupyter notebook trains an LSTM, or a long short-term memory model, to predict a cryptocurrency&amp;rsquo;s closing price. The LSTM is fed the close price, which is the price that the currency has at the end of the day, so it can learn from those values. The notebook creates two sets: a training set and a test set to assess the accuracy of the results.&lt;/p>
&lt;p>The data is then normalized using manual min-max scaling so that the model does not experience any bias; this also enhances the performance of the model. Then, the model is trained using three layers— an LSTM, dropout, and dense layer—minimizing the loss through 50 epochs of training; from this training, a recurrent neural network (RNN) is produced and fitted to the training set. Additionally, a graph of the loss over each epoch is produced, with the loss minimizing over time. Finally, the notebook plots a line graph of the actual currency price in red and the predicted price in blue. The process is then repeated for several more cryptocurrencies to compare prediction models. The parameters for the LSTM, such as number of epochs and batch size, are tweaked to try and minimize the root mean square error.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-architecture">3. Architecture&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-implementation">4. Implementation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-benchmark">5. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgments">7. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> cryptocurrency, investing, business, blockchain.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Blockchain is &lt;em>an open, distributed ledger&lt;/em> which records transactions of cryptocurrency. Systems in blockchain are decentralized, which means that these transactions are shared and distributed among all participants on the blockchain for maximum accountability. Furthermore, this new blockchain technology is becoming an increasingly popular alternative to mainstream transactions through traditional banks&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. These transactions utilize blockchain-based &lt;em>cryptocurrency&lt;/em>, which is a popular investment of today&amp;rsquo;s age, particularly in Bitcoin. However, the U.S. Securities and Exchange Commission warns that high-risk accompanies these investments&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Artificial Intelligence (AI) can be used to predict the prices' behavior to avoid cryptocurrency coins' severe volatility that can scare away possible investors&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. AI and blockchain technology make an ideal partnership in data science; the insights generated from the former and the secure environment ensured by the latter create a goldmine for valuable information. For example, an up-and-coming innovation is the automatic trading of &lt;em>digital investment assets&lt;/em> by AI, which will hugely outperform trading conducted by humans&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. This innovation would not be possible without the construction of a program which can pinpoint the most ideal time to buy and sell. Similarly, AI is applied in this experiment to predict the future price of cryptocurrencies on a number of different blockchains, including the Electro-Optical System and Ethereum.&lt;/p>
&lt;p>Long short-term memory (LSTM) is a neural network (form of AI) which ingests information and processes data using a &lt;em>gradient-based learning algorithm&lt;/em>&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. This creates an algorithm that improves with additional parameters; the algorithm &lt;em>learns&lt;/em> as it ingests. LSTM neural networks will be employed to analyze pre-existing price data so that the model can attempt to generate the future price in varying timetables, such as ten days, several months, or a year from the last date. This project will provide as a boon for insights into investments with potentially great returns. These findings can contribute to a positive cycle of attracting investors to a coin, which results in a price increase, which repeats. The main objective is to provide insights for investors on an up-and-coming product: cryptocurrency.&lt;/p>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;p>This project utilizes yfinance, a Python module which downloads the historical prices of a cryptocurrency from the first day of its inception to whichever day the program is executed. For example, the Yahoo Finance page for EOS-USD is the source for Figure 1&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. Figure 1 shows the historical data on a line graph when the program receives &amp;ldquo;EOS-USD&amp;rdquo; as an input.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-361/main/project/images/eos-price.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Line graph of EOS price from 1 July 2017 to 22 July 2021. Generated using yfinance-lstm.ipynb&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> located in project/code, utilizing price data from Yahoo Finance&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-architecture">3. Architecture&lt;/h2>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-361/main/project/images/architecture-process.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> The process of producing LSTM timeseries based on cryptocurrency price.&lt;/p>
&lt;h2 id="4-implementation">4. Implementation&lt;/h2>
&lt;p>Initially, this project was meant to scrape prices using the BeautifulSoup Python module; however, slight changes in a financial page&amp;rsquo;s website caused the code to break. Alternatively, Kaggle offered historical datasets of cryptocurrency, but they were not up to date. Thus, the final method of retrieving data is from Yahoo Finance through the yfinance Python module, which returns the coins' price from the day to its inception to the present day.&lt;/p>
&lt;p>The code is inspired from Towards Data Science articles by Serafeim Loukas&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> and Viraf&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>, who explore using LSTM to predict stock timeseries. This program contains adjustments and changes to their code so that cryptocurrency is analyzed instead. This project opts to use LSTM (long short-term memory) to predict the price because it has a memory capacity, which is ideal for a timeseries data set analysis such as cryptocurrency price over time. LSTM can remember historical patterns and use them to inform further predictions; it can also selectively choose which datapoints to use and which to disregard for the model&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. For example, this experiment&amp;rsquo;s code isolates only the close values to predict them and nothing else.&lt;/p>
&lt;p>Firstly, the code asks the user for the ticker of the cryptocurrency that is to be predicted, such as EOS-USD or BTC-USD. A complete list of acceptable inputs is under the Symbol column at &lt;a href="https://finance.yahoo.com/cryptocurrencies">https://finance.yahoo.com/cryptocurrencies&lt;/a> but theoretically, the program should be able to analyze traditional stocks as well as cryptocurrency.&lt;/p>
&lt;p>Then, the historical data for the corresponding coin is downloaded through the yfinance Python module. The data must go through normalization for simplicity and optimization of the model. Next, the Close data (the price that the currency has at the end of the day, everyday since the coin&amp;rsquo;s inception) is split into two sets: a training set and a test set, which are further split into their own respective x and y sets to guide the model through training.&lt;/p>
&lt;p>The training model is run through a layer of long short-term memory, as well as a dropout layer to prevent overfitting and a dense layer to give the model a memory capacity. Figure 3 showcases the setup of the LSTM layer.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-361/main/project/images/lstm.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Visual depiction of one layer of long short-term memory&lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>&lt;/p>
&lt;p>After training through 50 epochs, the program generated Figure 4, a line graph of the prediction model. Unless otherwise specified, the following figures use the EOS-USD data set from July 1st, 2017 to July 26th, 2021. Note that only the last 200 days are predicted so that the model can analyze the preexisting data prior to the 200 days for training purposes.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-361/main/project/images/prediction-model.png" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> EOS-USD price overlayed with the latest 200 days predicted by LSTM&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-361/main/project/images/prediction-model-zoomed.png" alt="Figure 5">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Zoomed-in graph (same as Figure 4 but scaled x and y-axis for readability)&lt;/p>
&lt;p>During training, the number of epochs can affect the model loss. According to the following figures 6 and 7, the loss starts to minimize around the 30th epoch of training. The greater the number of epochs, the sharper and more accurate the prediction becomes, but it does not vastly improve after around the 30th epoch.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-361/main/project/images/EOS-USD-training-loss.png" alt="Figure 6">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Line graph of model loss over the number of epochs the prediction model completed using EOS-USD data set&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-361/main/project/images/adjusting-epochs.png" alt="Figure 7">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Effect of EOS-USD prediction model based on number of epochs completed&lt;/p>
&lt;p>The epochs can also affect the Mean Squared Error, which details how close the prediction line is to the true Close values in United States Dollars (USD).&lt;/p>
&lt;p>&lt;strong>Table 1:&lt;/strong>: Number of epochs compared with mean squared error; all tests were run with EOS-USD as input. The Mean Squared Error is rounded to the nearest thousandth.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Epochs&lt;/th>
&lt;th>Mean Squared Error&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>0.924 USD&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>0.558 USD&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>0.478 USD&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>50&lt;/td>
&lt;td>0.485 USD&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>100&lt;/td>
&lt;td>0.490 USD&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Lastly, cryptocurrencies other than EOS such as Dogecoin, Ethereum, and Bitcoin can be analyzed as well. Figure 8 demonstrates the prediction models generated for these cryptocurrencies.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/su21-reu-361/main/project/images/other-cryptocurrencies.png" alt="Figure 8">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> EOS, Dogecoin, Ethereum, and Bitcoin prediction models&lt;/p>
&lt;p>Dogecoin presents a model that does not account well for the sharp rises, likely because the training period encompasses a period of relative inactivity (no high changes in price).&lt;/p>
&lt;h2 id="5-benchmark">5. Benchmark&lt;/h2>
&lt;p>The benchmark is run within yfinance-lstm.ipynb located in project/code&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The program ran on a 64-bit Windows 10 Home Edition (21H1) computer with a Ryzen 5 3600 processor (3.6 GHz). It also has dual-channel 16 GB RAM clocked at 3200 MHz and a GTX 1660 Ventus XS OC graphics card. The amount of time it takes to train the 50 epochs for the LSTM is around 16 seconds. A StopWatch module was used from the package cloudmesh-common&lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup> to precisely measure the training time.&lt;/p>
&lt;p>&lt;strong>Table 2:&lt;/strong>: First half of cloudmesh benchmark output, which details the specifications and status of the computer at the time of program execution&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attribute&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>cpu_cores&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu_count&lt;/td>
&lt;td>12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu_threads&lt;/td>
&lt;td>12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>frequency&lt;/td>
&lt;td>scpufreq(current=3600.0, min=0.0, max=3600.0)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.available&lt;/td>
&lt;td>7.1 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.free&lt;/td>
&lt;td>7.1 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.percent&lt;/td>
&lt;td>55.3 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.total&lt;/td>
&lt;td>16.0 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.used&lt;/td>
&lt;td>8.8 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>platform.version&lt;/td>
&lt;td>(&amp;lsquo;10&amp;rsquo;, &amp;lsquo;10.0.19043&amp;rsquo;, &amp;lsquo;SP0&amp;rsquo;, &amp;lsquo;Multiprocessor Free&amp;rsquo;)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python&lt;/td>
&lt;td>3.9.5 (tags/v3.9.5:0a7dcbd, May 3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.pip&lt;/td>
&lt;td>21.1.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.version&lt;/td>
&lt;td>3.9.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sys.platform&lt;/td>
&lt;td>win32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.machine&lt;/td>
&lt;td>AMD64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.processor&lt;/td>
&lt;td>AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.release&lt;/td>
&lt;td>10&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.system&lt;/td>
&lt;td>Windows&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.version&lt;/td>
&lt;td>10.0.19043&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 3:&lt;/strong>: Second half of cloudmesh benchmark output, which reports the execution time of training, overall program, and prediction&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Time&lt;/th>
&lt;th>Sum&lt;/th>
&lt;th>Start&lt;/th>
&lt;th>OS&lt;/th>
&lt;th>Version&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Overall time&lt;/td>
&lt;td>16.589 s&lt;/td>
&lt;td>35.273 s&lt;/td>
&lt;td>2021-07-26 18:39:57&lt;/td>
&lt;td>Windows&lt;/td>
&lt;td>(&amp;lsquo;10&amp;rsquo;, &amp;lsquo;10.0.19043&amp;rsquo;, &amp;lsquo;SP0&amp;rsquo;, &amp;lsquo;Multiprocessor Free&amp;rsquo;)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training time&lt;/td>
&lt;td>15.186 s&lt;/td>
&lt;td>30.986 s&lt;/td>
&lt;td>2021-07-26 18:39:58&lt;/td>
&lt;td>Windows&lt;/td>
&lt;td>(&amp;lsquo;10&amp;rsquo;, &amp;lsquo;10.0.19043&amp;rsquo;, &amp;lsquo;SP0&amp;rsquo;, &amp;lsquo;Multiprocessor Free&amp;rsquo;)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Prediction time&lt;/td>
&lt;td>0.227 s&lt;/td>
&lt;td>0.474 s&lt;/td>
&lt;td>2021-07-26 18:40:13&lt;/td>
&lt;td>Windows&lt;/td>
&lt;td>(&amp;lsquo;10&amp;rsquo;, &amp;lsquo;10.0.19043&amp;rsquo;, &amp;lsquo;SP0&amp;rsquo;, &amp;lsquo;Multiprocessor Free&amp;rsquo;)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>At first glance, the results look promising as the predictions have minimal deviation from the true values. However, upon closer look, the values lag by one day, which is a sign that they are only viewing the previous day and mimicking those values. Furthermore, the model cannot go several days or years into the future because there is no data to run on, such as opening price or volume. The experiment is further confounded by the nature of stock prices: they follow random walk theory, which means that the nature in which they move follows a random walk: the changes in price do not necessarily happen as a result of previous changes. Thus, this nature of stocks contradicts the very architecture of this experiment because long short-term memory assumes that the values have an effect on one another.&lt;/p>
&lt;p>For future research, a program can scrape tweets from influencers' Twitter pages so that a model can guess whether public discussion of a cryptocurrency is favorable or unfavorable (and whether the price will increase as a result).&lt;/p>
&lt;h2 id="7-acknowledgments">7. Acknowledgments&lt;/h2>
&lt;p>Thank you to Gregor von Laszewski, Yohn Jairo, and Carlos Theran for their invaluable guidance. Furthermore, thank you to Florida A&amp;amp;M University for graciously funding this scientific excursion and Miami Dade College School of Science for this research opportunity.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Jacques Fleischer, README.md Install Documentation, [GitHub]
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/blob/main/project/code/README.md">https://github.com/cybertraining-dsc/su21-reu-361/blob/main/project/code/README.md&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Jacques Fleischer, yfinance-lstm.ipynb Jupyter Notebook, [GitHub]
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-361/blob/main/project/code/yfinance-lstm.ipynb">https://github.com/cybertraining-dsc/su21-reu-361/blob/main/project/code/yfinance-lstm.ipynb&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Marco Iansiti and Karim R. Lakhani, The Truth About Blockchain, [Online resource]
&lt;a href="https://hbr.org/2017/01/the-truth-about-blockchain">https://hbr.org/2017/01/the-truth-about-blockchain&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Lori Schock, Thinking About Buying the Latest New Cryptocurrency or Token?, [Online resource]
&lt;a href="https://www.investor.gov/additional-resources/spotlight/directors-take/thinking-about-buying-latest-new-cryptocurrency-or">https://www.investor.gov/additional-resources/spotlight/directors-take/thinking-about-buying-latest-new-cryptocurrency-or&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Jeremy Swinfen Green, Understanding cryptocurrency market fluctuations, [Online resource]
&lt;a href="https://www.telegraph.co.uk/business/business-reporter/cryptocurrency-market-fluctuations/">https://www.telegraph.co.uk/business/business-reporter/cryptocurrency-market-fluctuations/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Raj Shroff, When Blockchain Meets Artificial Intelligence. [Online resource]
&lt;a href="https://medium.com/swlh/when-blockchain-meets-artificial-intelligence-e448968d0482">https://medium.com/swlh/when-blockchain-meets-artificial-intelligence-e448968d0482&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Sepp Hochreiter and Jürgen Schmidhuber, Long Short-Term Memory, [Online resource]
&lt;a href="https://www.bioinf.jku.at/publications/older/2604.pdf">https://www.bioinf.jku.at/publications/older/2604.pdf&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Yahoo Finance, EOS USD (EOS-USD), [Online resource]
&lt;a href="https://finance.yahoo.com/quote/EOS-USD/history?p=EOS-USD">https://finance.yahoo.com/quote/EOS-USD/history?p=EOS-USD&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Serafeim Loukas, Time-Series Forecasting: Predicting Stock Prices Using An LSTM Model, [Online resource]
&lt;a href="https://towardsdatascience.com/lstm-time-series-forecasting-predicting-stock-prices-using-an-lstm-model-6223e9644a2f">https://towardsdatascience.com/lstm-time-series-forecasting-predicting-stock-prices-using-an-lstm-model-6223e9644a2f&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Viraf, How (NOT) To Predict Stock Prices With LSTMs, [Online resource]
&lt;a href="https://towardsdatascience.com/how-not-to-predict-stock-prices-with-lstms-a51f564ccbca">https://towardsdatascience.com/how-not-to-predict-stock-prices-with-lstms-a51f564ccbca&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Derk Zomer, Using machine learning to predict future bitcoin prices, [Online resource]
&lt;a href="https://towardsdatascience.com/using-machine-learning-to-predict-future-bitcoin-prices-6637e7bfa58f">https://towardsdatascience.com/using-machine-learning-to-predict-future-bitcoin-prices-6637e7bfa58f&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Christopher Olah, Understanding LSTM Networks, [Online resource]
&lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/&lt;/a>&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, [GitHub]
&lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analysis of Covid-19 Vaccination Rates in Different Races</title><link>/report/su21-reu-375/project/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>/report/su21-reu-375/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/su21-reu-375/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/su21-reu-375/actions">&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: draft, Type: Project&lt;/p>
&lt;p>Ololade Latinwo, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-375">su21-reu-375&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/su21-reu-375/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>With the ready availability of COVID-19 vaccinations, it is concerning that a suprising large portion of the U.S. population still refuses to recieve one. The goal of this report is to analyze the demographics of those who choose not to recieve the vaccine and possibly find the reasoning behind their decision in order dissuade any concerns of those who may be on the fence over recieveing the vaccine.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-using-images">3. Using Images&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgments">8. Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, example.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>It has been shown by several economic and health institutions that rates COVID-19 in the United States have been among the highest in the world. Estimates show that about 10 million people have been infected and over a quarter of a million have died in the U.S. by the end of November 2020 &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Fortunately, several pharmaceutical companies such as() have managed to create a vaccine by the end of 2020, with several million Americans being given the vaccine by early March. Interestingly, it appears that despite the ready availability of vaccines, a sizeable portion of the population have no intention of receiving either their second does or either dose at all. Voluntarily receiving the COVID-19 vaccine is integral to putting the pandemic to an end, so it is important to explore which demographics are hesitant to receive their vaccine and explore their reasons for doing so.&lt;/p>
&lt;h2 id="3-using-images">3. Using Images&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/e13597076f290e67ddc888ec8ac2a7f6fbf8a3ad/Pictures/USA%20Vaccine%20.jpg" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> An image of the United States illustrating the percentage of adults who have recieved at least one dose of the COVID-19 vaccine&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/b624e0213bad00132fe7ec9762730466aa4210b3/Pictures/Vaccine%20Rate%20by%20COVID%20Status.jpg" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> A bar graph of the vaccine rates in people who have or have not been previously been diagnosed with COVID-19&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/b624e0213bad00132fe7ec9762730466aa4210b3/Pictures/USA%20Vaccine%20Hesitancy.jpg" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> An image of the United States illustrating the percentage of adults who are hesitant to recieve a vaccine for COVID-19&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/su21-reu-375/blob/90b659ac3f76fb77dda3bb2d5f3dd33fadbda166/Pictures/Political%20Affiliation%20by%20Region.jpg" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> A graph of the political alignments seen in various regions of the United States&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Datasets can be huge and GitHub has limited space. Only very small datasets should be stored in GitHub.
However, if the data is publicly available you program must contain a download function instead that you customize.
Write it using pythons &lt;code>request&lt;/code>. You will get point deductions if you check-in data sets that are large and do not use
the download function.&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Your project must include a benchmark. The easiest is to use cloudmesh-common &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>A convincing but not fake conclusion should summarize what the conclusion of the project is.&lt;/p>
&lt;h2 id="8-acknowledgments">8. Acknowledgments&lt;/h2>
&lt;p>Special thanks to Yohn J Parra, Carlos Theran, and Gregor Lasweski for supporting this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Khubchandani, J., Sharma, S., Price, J.H. et al.
COVID-19 Vaccination Hesitancy in the United States: A Rapid National Assessment.
J Community Health 46, 270–277 (2021).
&lt;a href="https://doi.org/10.1007/s10900-020-00958-x">https://doi.org/10.1007/s10900-020-00958-x&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Household Pulse Survey COVID-19 Vaccination Tracker
United States Census Bureau
&lt;a href="https://www.census.gov/library/visualizations/interactive/household-pulse-survey-covid-19-vaccination-tracker.html">https://www.census.gov/library/visualizations/interactive/household-pulse-survey-covid-19-vaccination-tracker.html&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Vaccine Hesitancy for COVID-19
Centers for Disease Control and Prevention
&lt;a href="https://data.cdc.gov/stories/s/Vaccine-Hesitancy-for-COVID-19/cnd2-a6zw">https://data.cdc.gov/stories/s/Vaccine-Hesitancy-for-COVID-19/cnd2-a6zw&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/li>
&lt;/ol>
&lt;/section></description></item></channel></rss>